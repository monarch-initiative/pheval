{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Introduction PhEval - Phenotypic Inference Evaluation Framework PhEval: Tool-specific processing (VP pipeline) flowchart LR PC-->DP PC[(Phenopackets Corpus)] SSSOM[Semantic Similarity Profiles Mapping Commons]-->|OAK-SEMSIM|DP[Data Prepare] KG[Source data KG - Monarch KG]-->|KGX-BIOLINK|DP[Data Prepare] ONT[Ontologies - Phenio]-->|OAK-ONTO|DP[Data Prepare] DP-->RP[Run Prepare] RP-->PR[PhEval Runner] PR-->DP2[Data Process] ER[Exomiser Runner]-->PR EDP[Exomiser Data Prepare]-->DP ERP[Exomiser Run Prepare]-->RP PPP[Disease-profile similarity prediction Post-process]-->DP2 PV[Phenotype/Variant]-->DP2 GVP[Gene VP Post-process]-->DP2 EPP[Exomiser Post Process]-->GVP GVP-->VPR[VP Report] Quick links: GitHub page","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#introduction","text":"PhEval - Phenotypic Inference Evaluation Framework","title":"Introduction"},{"location":"#pheval-tool-specific-processing-vp-pipeline","text":"flowchart LR PC-->DP PC[(Phenopackets Corpus)] SSSOM[Semantic Similarity Profiles Mapping Commons]-->|OAK-SEMSIM|DP[Data Prepare] KG[Source data KG - Monarch KG]-->|KGX-BIOLINK|DP[Data Prepare] ONT[Ontologies - Phenio]-->|OAK-ONTO|DP[Data Prepare] DP-->RP[Run Prepare] RP-->PR[PhEval Runner] PR-->DP2[Data Process] ER[Exomiser Runner]-->PR EDP[Exomiser Data Prepare]-->DP ERP[Exomiser Run Prepare]-->RP PPP[Disease-profile similarity prediction Post-process]-->DP2 PV[Phenotype/Variant]-->DP2 GVP[Gene VP Post-process]-->DP2 EPP[Exomiser Post Process]-->GVP GVP-->VPR[VP Report] Quick links: GitHub page","title":"PhEval: Tool-specific processing (VP pipeline)"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This code of conduct has been derived from the excellent code of conduct of the ATOM project which in turn is adapted from the Contributor Covenant , version 1.4, available at https://contributor-covenant.org/version/1/4","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This code of conduct has been derived from the excellent code of conduct of the ATOM project which in turn is adapted from the Contributor Covenant , version 1.4, available at https://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"about/","text":"PhEval - Phenotypic Inference Evaluation Framework Many variant prioritization tools (such as Exomiser and other computational approaches) rely on ontologies and phenotype matching, sometimes involving complex processes such as cross-species inference. The performance of such tools is exceedingly hard to evaluate because of the many factors involved: changes to the structure of the ontology, cross-species mappings, and semantic similarity algorithms can have significant consequences. Furthermore, the lack of suitable real-world problems/corpora leads to the situation that many algorithms are evaluated using simulations, which may fail to capture real-world scenarios. The lack of an evaluation framework that enables studying effects on data and knowledge inputs on real-world problems makes it difficult to optimize algorithms. To this end, we are developing a modular Phenotypic Inference Evaluation Framework (PhEval), which is delivered as a community resource.","title":"About"},{"location":"about/#pheval-phenotypic-inference-evaluation-framework","text":"Many variant prioritization tools (such as Exomiser and other computational approaches) rely on ontologies and phenotype matching, sometimes involving complex processes such as cross-species inference. The performance of such tools is exceedingly hard to evaluate because of the many factors involved: changes to the structure of the ontology, cross-species mappings, and semantic similarity algorithms can have significant consequences. Furthermore, the lack of suitable real-world problems/corpora leads to the situation that many algorithms are evaluated using simulations, which may fail to capture real-world scenarios. The lack of an evaluation framework that enables studying effects on data and knowledge inputs on real-world problems makes it difficult to optimize algorithms. To this end, we are developing a modular Phenotypic Inference Evaluation Framework (PhEval), which is delivered as a community resource.","title":"PhEval - Phenotypic Inference Evaluation Framework"},{"location":"contact/","text":"Contact The preferred way to contact the PhEval team is through the issue tracker (for problems with PhEval) or the GitHub discussions (for general questions). You can find any of the members of the PhEval core team on GitHub: https://github.com/orgs/monarch-initiative/teams/pheval-team Their GitHub profiles usually also provide email addresses.","title":"Contact Us"},{"location":"contact/#contact","text":"The preferred way to contact the PhEval team is through the issue tracker (for problems with PhEval) or the GitHub discussions (for general questions). You can find any of the members of the PhEval core team on GitHub: https://github.com/orgs/monarch-initiative/teams/pheval-team Their GitHub profiles usually also provide email addresses.","title":"Contact"},{"location":"contributing/","text":"Contributions First of all: Thank you for taking the time to contribute! The following is a set of guidelines for contributing to the PhEval framework. These guidelines are not strict rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Table Of Contents Contributions Table Of Contents Code of Conduct Guidelines for Contributions and Requests Reporting problems with the data model Code of Conduct The monarch-technical-documentation team strives to create a welcoming environment for editors, users and other contributors. Please carefully read our Code of Conduct . Guidelines for Contributions and Requests Reporting problems with the data model Please use our Issue Tracker for reporting problems with the ontology.","title":"Contributions"},{"location":"contributing/#contributions","text":"First of all: Thank you for taking the time to contribute! The following is a set of guidelines for contributing to the PhEval framework. These guidelines are not strict rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Contributions"},{"location":"contributing/#table-of-contents","text":"Contributions Table Of Contents Code of Conduct Guidelines for Contributions and Requests Reporting problems with the data model","title":"Table Of Contents"},{"location":"contributing/#code-of-conduct","text":"The monarch-technical-documentation team strives to create a welcoming environment for editors, users and other contributors. Please carefully read our Code of Conduct .","title":"Code of Conduct"},{"location":"contributing/#guidelines-for-contributions-and-requests","text":"","title":"Guidelines for Contributions and Requests"},{"location":"contributing/#reporting-problems-with-the-data-model","text":"Please use our Issue Tracker for reporting problems with the ontology.","title":"Reporting problems with the data model"},{"location":"developing_a_pheval_plugin/","text":"Developing a PhEval Plugin Description Plugin development allows PhEval to be extensible, as we have designed it. The plugin goal is to be flexible through custom runner implementations. This plugin development enhances the PhEval functionality. You can build one quickly using this step-by-step process. All custom Runners implementations must implement all PhevalRunner methods Bases: ABC PhEvalRunner Class Source code in src/pheval/runners/runner.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @dataclass class PhEvalRunner ( ABC ): \"\"\"PhEvalRunner Class\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str directory_path = None input_dir_config = None _meta_data = None __raw_results_dir = \"raw_results/\" __pheval_gene_results_dir = \"pheval_gene_results/\" __pheval_variant_results_dir = \"pheval_variant_results/\" __pheval_disease_results_dir = \"pheval_disease_results/\" __tool_input_commands_dir = \"tool_input_commands/\" __run_meta_data_file = \"results.yml\" def __post_init__ ( self ): self . input_dir_config = parse_input_dir_config ( self . input_dir ) def _get_tool ( self ): return self . input_dir_config . tool def _get_variant_analysis ( self ): return self . input_dir_config . variant_analysis def _get_gene_analysis ( self ): return self . input_dir_config . gene_analysis def _get_disease_analysis ( self ): return self . input_dir_config . disease_analysis @property def tool_input_commands_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __tool_input_commands_dir ) @tool_input_commands_dir . setter def tool_input_commands_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def raw_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __raw_results_dir ) @raw_results_dir . setter def raw_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_gene_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_gene_results_dir ) @pheval_gene_results_dir . setter def pheval_gene_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_variant_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_variant_results_dir ) @pheval_variant_results_dir . setter def pheval_variant_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_disease_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_disease_results_dir ) @pheval_disease_results_dir . setter def pheval_disease_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True ) @property def meta_data ( self ): self . _meta_data = BasicOutputRunMetaData ( tool = self . input_dir_config . tool , tool_version = self . version , config = f \" { Path ( self . input_dir ) . parent . name } / { Path ( self . input_dir ) . name } \" , run_timestamp = datetime . now () . timestamp (), corpus = f \" { Path ( self . testdata_dir ) . parent . name } / { Path ( self . testdata_dir ) . name } \" , mondo_download_date = get_resource_timestamp ( \"mondo.sssom.tsv\" ), hgnc_download_date = get_resource_timestamp ( \"hgnc_complete_set.txt\" ), ) return self . _meta_data @meta_data . setter def meta_data ( self , meta_data ): self . _meta_data = meta_data @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\" @abstractmethod def run ( self ): \"\"\"run\"\"\" @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\" def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data Step-by-Step Plugin Development Process The plugin structure is derived from a cookiecutter template, cookiecutter , and it uses MkDocs , tox and poetry as core dependencies. This allows PhEval extensibility to be standardised in terms of documentation and dependency management. 1. Cookiecutter scaffold First, install the cruft package. Cruft enables keeping projects up-to-date with future updates made to this original template. Install the latest release of cruft from pip pip install cruft NOTE: You may encounter an error with the naming of the project layout if using an older release of cruft. To avoid this, make sure you have installed the latest release version. Next, create a project using the cookiecutter template. cruft create https://github.com/monarch-initiative/pheval-runner-template 2. Further setup Install poetry if you haven't already. pip install poetry Install dependencies poetry install Run tox to see if the setup works poetry run tox 3. Implement PhEval Custom Runner In the project structure generated by Cookiecutter, you'll find runner.py located in the src directory. This is where you'll define the methods required to develop the plugin. Specifically, you'll implement the prepare, run, and post-process methods, which are essential for executing the pheval run command. \"\"\"Runner.\"\"\" from dataclasses import dataclass from pathlib import Path from pheval.runners.runner import PhEvalRunner @dataclass class CustomRunner ( PhEvalRunner ): \"\"\"Runner class implementation.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"Prepare.\"\"\" print ( \"preparing\" ) def run ( self ): \"\"\"Run.\"\"\" print ( \"running\" ) def post_process ( self ): \"\"\"Post Process.\"\"\" print ( \"post processing\" ) The Cookiecutter will automatically populate the plugins section in the pyproject.toml file. If you decide to modify the path of runner.py or rename its class, be sure to update the corresponding entries in this section accordingly: [tool.poetry.plugins. \"pheval.plugins\" ] customrunner = \"pheval_plugin_example.runner:CustomRunner\" Please Note that the path here and naming of the class is case-sensitive. 4. Implementing PhEval helper methods Streamlining the creation of your custom PhEval runner can be facilitated by leveraging PhEval's versatile helper methods, where applicable. Within PhEval, numerous public methods have been designed to assist in your runner methods. The utilisation of these helper methods is optional, yet they are crafted to enhance the overall implementation process. Utility methods The PhenopacketUtil class is designed to aid in the collection of specific data from a Phenopacket. Class for retrieving data from a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 class PhenopacketUtil : \"\"\"Class for retrieving data from a Phenopacket or Family object\"\"\" def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases def _diagnosis_from_interpretations ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the interpretations object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] interpretation = self . interpretations () for i in interpretation : ( diagnoses . append ( ProbandDisease ( disease_name = i . diagnosis . disease . label , disease_identifier = i . diagnosis . disease . id , ) ) if i . diagnosis . disease . label != \"\" and i . diagnosis . disease . id != \"\" else None ) return diagnoses def _diagnosis_from_disease ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the diseases object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] for disease in self . diseases (): diagnoses . append ( ProbandDisease ( disease_name = disease . term . label , disease_identifier = disease . term . id ) ) return diagnoses def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ())) def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data @staticmethod def _extract_diagnosed_gene ( genomic_interpretation : GenomicInterpretation , ) -> ProbandCausativeGene : \"\"\" Retrieve the disease causing genes from the variant descriptor field if not empty, otherwise, retrieves from the gene descriptor from a phenopacket. Args: genomic_interpretation (GenomicInterpretation): A genomic interpretation from a Phenopacket Returns: ProbandCausativeGene: The disease causing gene \"\"\" if genomic_interpretation . variant_interpretation . ByteSize () != 0 : return ProbandCausativeGene ( genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . symbol , genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . value_id , ) else : return ProbandCausativeGene ( gene_symbol = genomic_interpretation . gene . symbol , gene_identifier = genomic_interpretation . gene . value_id , ) def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False PhenopacketUtil proves particularly beneficial in scenarios where the tool for which you're crafting a runner implementation does not directly accept Phenopackets as inputs. Instead, it might require elements\u2014such as HPO IDs\u2014 via the command-line interface (CLI). In this context, leveraging PhenopacketUtil within the runner's preparation phase enables the extraction of observed phenotypic features from the Phenopacket input, facilitating seamless processing. An example of how this could be implemented is outlined here: from pheval.utils.phenopacket_utils import phenopacket_reader from pheval.utils.phenopacket_utils import PhenopacketUtil phenopacket = phenopacket_reader ( \"/path/to/phenopacket.json\" ) phenopacket_util = PhenopacketUtil ( phenopacket ) # To return a list of all observed phenotypes for a phenopacket observed_phenotypes = phenopacket_util . observed_phenotypic_features () # To extract just the HPO ID as a list observed_phenotypes_hpo_ids = [ observed_phenotype . type . id for observed_phenotype in observed_phenotypes ] Additional tool-specific configurations For the pheval run command to execute successfully, a config.yaml should be found within the input directory supplied on the CLI. tool : tool_version : variant_analysis : gene_analysis : disease_analysis : tool_specific_configuration_options : The tool_specific_configuration_options is an optional field that can be populated with any variables specific to your runner implementation that is required for the running of your tool. All other fields are required to be filled in. The variant_analysis , gene_analysis , and disease_analysis are set as booleans and are for specifying what type of analysis/prioritisation the tool outputs. To populate the tool_specific_configurations_options with customised data, we suggest using the pydantic package as it can easily parse the data from the yaml structure. e.g., Define a BaseModel class with the fields that will populate the tool_specific_configuration_options from pydantic import BaseModel , Field class CustomisedConfigurations ( BaseModel ): \"\"\" Class for defining the customised configurations in tool_specific_configurations field, within the input_dir config.yaml Args: environment (str): Environment to run \"\"\" environment : str = Field ( ... ) Within your runner parse the field into an object. from dataclasses import dataclass from pheval.runners.runner import PhEvalRunner from pathlib import Path @dataclass class CustomPhevalRunner ( PhEvalRunner ): \"\"\"CustomPhevalRunner Class.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"prepare method.\"\"\" print ( \"preparing\" ) config = CustomisedConfigurations . parse_obj ( self . input_dir_config . tool_specific_configuration_options ) environment = config . environment def run ( self ): \"\"\"run method.\"\"\" print ( \"running with custom pheval runner\" ) def post_process ( self ): \"\"\"post_process method.\"\"\" print ( \"post processing\" ) Post-processing methods PhEval currently supports the benchmarking of gene, variant, and disease prioritisation results. To benchmark these result types, PhEval parquet result files need to be generated. PhEval can deal with the ranking and generation of these files to the correct location. However, the runner implementation must handle the extraction of essential data from the tool-specific raw results. This involves transforming them into a polars dataframe with the required columns for the benchmark type. The columns representing essential information extracted from tool-specific output for gene, variant, and disease prioritisation are defined as follows: Bases: Enum Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. Source code in src/pheval/post_processing/validate_result_format.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ResultSchema ( Enum ): \"\"\" Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. \"\"\" GENE_RESULT_SCHEMA = pl . Schema ( { \"gene_symbol\" : pl . String , \"gene_identifier\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) VARIANT_RESULT_SCHEMA = pl . Schema ( { \"chrom\" : pl . String , \"start\" : pl . Int64 , \"end\" : pl . Int64 , \"ref\" : pl . String , \"alt\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) DISEASE_RESULT_SCHEMA = pl . Schema ( { \"disease_identifier\" : pl . String , \"score\" : pl . Float64 , } ) def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True GENE_RESULT_SCHEMA = pl . Schema ({ 'gene_symbol' : pl . String , 'gene_identifier' : pl . String , 'score' : pl . Float64 , 'grouping_id' : pl . Utf8 }) class-attribute instance-attribute VARIANT_RESULT_SCHEMA = pl . Schema ({ 'chrom' : pl . String , 'start' : pl . Int64 , 'end' : pl . Int64 , 'ref' : pl . String , 'alt' : pl . String , 'score' : pl . Float64 , 'grouping_id' : pl . Utf8 }) class-attribute instance-attribute DISEASE_RESULT_SCHEMA = pl . Schema ({ 'disease_identifier' : pl . String , 'score' : pl . Float64 }) class-attribute instance-attribute The grouping_id column is optional and is designed to handle cases where entities should be jointly ranked without being penalised. For example, in the ranking of compound heterozygous variant which occurs when two or more variants, inherited together, contribute to a phenotype. For this purpose, variants that are part of the same compound heterozygous group (e.g., within the same gene) should be assigned the same grouping_id . This ensures they are ranked as a single entity, preserving their combined significance. Variants that are not part of any compound heterozygous group should each have a unique grouping_id . This approach prevents any unintended overlap in ranking and ensures that each group or individual variant is accurately represented. The use of the grouping_id would also be suitable for the ranking and prioritisation of polygenic diseases. Depending on whether you need to generate gene, variant, and or disease results depends on the final method called to generate the results from the polars dataframe. The methods are outlined below: \u26a0\ufe0f Breaking Change (v0.5.0): The helper method generate_pheval_result has been replaced with three separate methods for each result type: - generate_gene_result - generate_variant_result - generate_disease_result Update your runner implementation to call the appropriate method based on the type of result your tool produces. Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @validate_dataframe ( ResultSchema . GENE_RESULT_SCHEMA ) def generate_gene_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_gene_results/ { result_path . stem } -gene_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_gene_results\" ), ResultType . GENE ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_gene_results ( ranked_results , output_file ) _write_gene_result ( classified_results , output_file ) Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @validate_dataframe ( ResultSchema . VARIANT_RESULT_SCHEMA ) def generate_variant_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_variant_results/ { result_path . stem } -variant_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_variant_results\" ), ResultType . VARIANT , ) ranked_results = _rank_results ( results , sort_order ) . with_columns ( pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ) ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_variant_results ( ranked_results , output_file ) _write_variant_result ( classified_results , output_file ) Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @validate_dataframe ( ResultSchema . DISEASE_RESULT_SCHEMA ) def generate_disease_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_disease_results/ { result_path . stem } -disease_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_disease_results\" ), ResultType . DISEASE , ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_disease_results ( ranked_results , output_file , mondo_mapping_table ) _write_disease_result ( classified_results , output_file ) An example of how the method can be called is outlined here: from pheval.post_processing.post_processing import generate_gene_result , SortOrder generate_gene_result ( results = pheval_gene_result , # this is the polars dataframe containing extracted PhEval result requirements sort_order = SortOrder . DESCENDING , # or can be ASCENDING - this determines in which order the scores will be ranked output_dir = output_directory , # this can be accessed from the runner instance e.g., self.output_dir result_path = result_path # this is the path to the tool-specific raw results file phenopacket_dir = phenopacket_dir # this is the path to the directory containing the phenopackets ) Adding metadata to the results.yml By default, PhEval will write a results.yml to the output directory supplied on the CLI. The results.yml contains basic metadata regarding the run configuration, however, there is also the option to add customised run metadata to the results.yml in the tool_specific_configuration_options field. To achieve this, you'll need to create a construct_meta_data() method within your runner implementation. This method is responsible for appending customised metadata to the metadata object in the form of a defined dataclass. It should return the entire metadata object once the addition is completed. e.g., Defined customised metadata dataclass: from dataclasses import dataclass @dataclass class CustomisedMetaData : customised_field : str Example of implementation in the runner. from dataclasses import dataclass from pheval.runners.runner import PhEvalRunner from pathlib import Path @dataclass class CustomPhevalRunner ( PhEvalRunner ): \"\"\"CustomPhevalRunner Class.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"prepare method.\"\"\" print ( \"preparing\" ) def run ( self ): \"\"\"run method.\"\"\" print ( \"running with custom pheval runner\" ) def post_process ( self ): \"\"\"post_process method.\"\"\" print ( \"post processing\" ) def construct_meta_data ( self ): \"\"\"Add metadata.\"\"\" self . meta_data . tool_specific_configuration_options = CustomisedMetaData ( customised_field = \"customised_value\" ) return self . meta_data 6. Test it. To update your custom pheval runner implementation, you must first install the package poetry install Now you have to be able to run PhEval passing your custom runner as parameter. e.g., pheval run -i ./input_dir -t ./test_data_dir -r 'customphevalrunner' -o output_dir The -r parameter stands for your plugin runner class name, and it must be entirely lowercase. Output: preparing running post processing","title":"Developing a PhEval Plugin"},{"location":"developing_a_pheval_plugin/#developing-a-pheval-plugin","text":"","title":"Developing a PhEval Plugin"},{"location":"developing_a_pheval_plugin/#description","text":"Plugin development allows PhEval to be extensible, as we have designed it. The plugin goal is to be flexible through custom runner implementations. This plugin development enhances the PhEval functionality. You can build one quickly using this step-by-step process. All custom Runners implementations must implement all PhevalRunner methods Bases: ABC PhEvalRunner Class Source code in src/pheval/runners/runner.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @dataclass class PhEvalRunner ( ABC ): \"\"\"PhEvalRunner Class\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str directory_path = None input_dir_config = None _meta_data = None __raw_results_dir = \"raw_results/\" __pheval_gene_results_dir = \"pheval_gene_results/\" __pheval_variant_results_dir = \"pheval_variant_results/\" __pheval_disease_results_dir = \"pheval_disease_results/\" __tool_input_commands_dir = \"tool_input_commands/\" __run_meta_data_file = \"results.yml\" def __post_init__ ( self ): self . input_dir_config = parse_input_dir_config ( self . input_dir ) def _get_tool ( self ): return self . input_dir_config . tool def _get_variant_analysis ( self ): return self . input_dir_config . variant_analysis def _get_gene_analysis ( self ): return self . input_dir_config . gene_analysis def _get_disease_analysis ( self ): return self . input_dir_config . disease_analysis @property def tool_input_commands_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __tool_input_commands_dir ) @tool_input_commands_dir . setter def tool_input_commands_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def raw_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __raw_results_dir ) @raw_results_dir . setter def raw_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_gene_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_gene_results_dir ) @pheval_gene_results_dir . setter def pheval_gene_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_variant_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_variant_results_dir ) @pheval_variant_results_dir . setter def pheval_variant_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_disease_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_disease_results_dir ) @pheval_disease_results_dir . setter def pheval_disease_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True ) @property def meta_data ( self ): self . _meta_data = BasicOutputRunMetaData ( tool = self . input_dir_config . tool , tool_version = self . version , config = f \" { Path ( self . input_dir ) . parent . name } / { Path ( self . input_dir ) . name } \" , run_timestamp = datetime . now () . timestamp (), corpus = f \" { Path ( self . testdata_dir ) . parent . name } / { Path ( self . testdata_dir ) . name } \" , mondo_download_date = get_resource_timestamp ( \"mondo.sssom.tsv\" ), hgnc_download_date = get_resource_timestamp ( \"hgnc_complete_set.txt\" ), ) return self . _meta_data @meta_data . setter def meta_data ( self , meta_data ): self . _meta_data = meta_data @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\" @abstractmethod def run ( self ): \"\"\"run\"\"\" @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\" def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data","title":"Description"},{"location":"developing_a_pheval_plugin/#step-by-step-plugin-development-process","text":"The plugin structure is derived from a cookiecutter template, cookiecutter , and it uses MkDocs , tox and poetry as core dependencies. This allows PhEval extensibility to be standardised in terms of documentation and dependency management.","title":"Step-by-Step Plugin Development Process"},{"location":"developing_a_pheval_plugin/#1-cookiecutter-scaffold","text":"First, install the cruft package. Cruft enables keeping projects up-to-date with future updates made to this original template. Install the latest release of cruft from pip pip install cruft NOTE: You may encounter an error with the naming of the project layout if using an older release of cruft. To avoid this, make sure you have installed the latest release version. Next, create a project using the cookiecutter template. cruft create https://github.com/monarch-initiative/pheval-runner-template","title":"1. Cookiecutter scaffold"},{"location":"developing_a_pheval_plugin/#2-further-setup","text":"","title":"2. Further setup"},{"location":"developing_a_pheval_plugin/#install-poetry-if-you-havent-already","text":"pip install poetry","title":"Install poetry if you haven't already."},{"location":"developing_a_pheval_plugin/#install-dependencies","text":"poetry install","title":"Install dependencies"},{"location":"developing_a_pheval_plugin/#run-tox-to-see-if-the-setup-works","text":"poetry run tox","title":"Run tox to see if the setup works"},{"location":"developing_a_pheval_plugin/#3-implement-pheval-custom-runner","text":"In the project structure generated by Cookiecutter, you'll find runner.py located in the src directory. This is where you'll define the methods required to develop the plugin. Specifically, you'll implement the prepare, run, and post-process methods, which are essential for executing the pheval run command. \"\"\"Runner.\"\"\" from dataclasses import dataclass from pathlib import Path from pheval.runners.runner import PhEvalRunner @dataclass class CustomRunner ( PhEvalRunner ): \"\"\"Runner class implementation.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"Prepare.\"\"\" print ( \"preparing\" ) def run ( self ): \"\"\"Run.\"\"\" print ( \"running\" ) def post_process ( self ): \"\"\"Post Process.\"\"\" print ( \"post processing\" ) The Cookiecutter will automatically populate the plugins section in the pyproject.toml file. If you decide to modify the path of runner.py or rename its class, be sure to update the corresponding entries in this section accordingly: [tool.poetry.plugins. \"pheval.plugins\" ] customrunner = \"pheval_plugin_example.runner:CustomRunner\" Please Note that the path here and naming of the class is case-sensitive.","title":"3. Implement PhEval Custom Runner"},{"location":"developing_a_pheval_plugin/#4-implementing-pheval-helper-methods","text":"Streamlining the creation of your custom PhEval runner can be facilitated by leveraging PhEval's versatile helper methods, where applicable. Within PhEval, numerous public methods have been designed to assist in your runner methods. The utilisation of these helper methods is optional, yet they are crafted to enhance the overall implementation process.","title":"4. Implementing PhEval helper methods"},{"location":"developing_a_pheval_plugin/#utility-methods","text":"The PhenopacketUtil class is designed to aid in the collection of specific data from a Phenopacket. Class for retrieving data from a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 class PhenopacketUtil : \"\"\"Class for retrieving data from a Phenopacket or Family object\"\"\" def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases def _diagnosis_from_interpretations ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the interpretations object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] interpretation = self . interpretations () for i in interpretation : ( diagnoses . append ( ProbandDisease ( disease_name = i . diagnosis . disease . label , disease_identifier = i . diagnosis . disease . id , ) ) if i . diagnosis . disease . label != \"\" and i . diagnosis . disease . id != \"\" else None ) return diagnoses def _diagnosis_from_disease ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the diseases object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] for disease in self . diseases (): diagnoses . append ( ProbandDisease ( disease_name = disease . term . label , disease_identifier = disease . term . id ) ) return diagnoses def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ())) def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data @staticmethod def _extract_diagnosed_gene ( genomic_interpretation : GenomicInterpretation , ) -> ProbandCausativeGene : \"\"\" Retrieve the disease causing genes from the variant descriptor field if not empty, otherwise, retrieves from the gene descriptor from a phenopacket. Args: genomic_interpretation (GenomicInterpretation): A genomic interpretation from a Phenopacket Returns: ProbandCausativeGene: The disease causing gene \"\"\" if genomic_interpretation . variant_interpretation . ByteSize () != 0 : return ProbandCausativeGene ( genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . symbol , genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . value_id , ) else : return ProbandCausativeGene ( gene_symbol = genomic_interpretation . gene . symbol , gene_identifier = genomic_interpretation . gene . value_id , ) def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False PhenopacketUtil proves particularly beneficial in scenarios where the tool for which you're crafting a runner implementation does not directly accept Phenopackets as inputs. Instead, it might require elements\u2014such as HPO IDs\u2014 via the command-line interface (CLI). In this context, leveraging PhenopacketUtil within the runner's preparation phase enables the extraction of observed phenotypic features from the Phenopacket input, facilitating seamless processing. An example of how this could be implemented is outlined here: from pheval.utils.phenopacket_utils import phenopacket_reader from pheval.utils.phenopacket_utils import PhenopacketUtil phenopacket = phenopacket_reader ( \"/path/to/phenopacket.json\" ) phenopacket_util = PhenopacketUtil ( phenopacket ) # To return a list of all observed phenotypes for a phenopacket observed_phenotypes = phenopacket_util . observed_phenotypic_features () # To extract just the HPO ID as a list observed_phenotypes_hpo_ids = [ observed_phenotype . type . id for observed_phenotype in observed_phenotypes ]","title":"Utility methods"},{"location":"developing_a_pheval_plugin/#additional-tool-specific-configurations","text":"For the pheval run command to execute successfully, a config.yaml should be found within the input directory supplied on the CLI. tool : tool_version : variant_analysis : gene_analysis : disease_analysis : tool_specific_configuration_options : The tool_specific_configuration_options is an optional field that can be populated with any variables specific to your runner implementation that is required for the running of your tool. All other fields are required to be filled in. The variant_analysis , gene_analysis , and disease_analysis are set as booleans and are for specifying what type of analysis/prioritisation the tool outputs. To populate the tool_specific_configurations_options with customised data, we suggest using the pydantic package as it can easily parse the data from the yaml structure. e.g., Define a BaseModel class with the fields that will populate the tool_specific_configuration_options from pydantic import BaseModel , Field class CustomisedConfigurations ( BaseModel ): \"\"\" Class for defining the customised configurations in tool_specific_configurations field, within the input_dir config.yaml Args: environment (str): Environment to run \"\"\" environment : str = Field ( ... ) Within your runner parse the field into an object. from dataclasses import dataclass from pheval.runners.runner import PhEvalRunner from pathlib import Path @dataclass class CustomPhevalRunner ( PhEvalRunner ): \"\"\"CustomPhevalRunner Class.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"prepare method.\"\"\" print ( \"preparing\" ) config = CustomisedConfigurations . parse_obj ( self . input_dir_config . tool_specific_configuration_options ) environment = config . environment def run ( self ): \"\"\"run method.\"\"\" print ( \"running with custom pheval runner\" ) def post_process ( self ): \"\"\"post_process method.\"\"\" print ( \"post processing\" )","title":"Additional tool-specific configurations"},{"location":"developing_a_pheval_plugin/#post-processing-methods","text":"PhEval currently supports the benchmarking of gene, variant, and disease prioritisation results. To benchmark these result types, PhEval parquet result files need to be generated. PhEval can deal with the ranking and generation of these files to the correct location. However, the runner implementation must handle the extraction of essential data from the tool-specific raw results. This involves transforming them into a polars dataframe with the required columns for the benchmark type. The columns representing essential information extracted from tool-specific output for gene, variant, and disease prioritisation are defined as follows: Bases: Enum Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. Source code in src/pheval/post_processing/validate_result_format.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ResultSchema ( Enum ): \"\"\" Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. \"\"\" GENE_RESULT_SCHEMA = pl . Schema ( { \"gene_symbol\" : pl . String , \"gene_identifier\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) VARIANT_RESULT_SCHEMA = pl . Schema ( { \"chrom\" : pl . String , \"start\" : pl . Int64 , \"end\" : pl . Int64 , \"ref\" : pl . String , \"alt\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) DISEASE_RESULT_SCHEMA = pl . Schema ( { \"disease_identifier\" : pl . String , \"score\" : pl . Float64 , } ) def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True","title":"Post-processing methods"},{"location":"developing_a_pheval_plugin/#src.pheval.post_processing.validate_result_format.ResultSchema.GENE_RESULT_SCHEMA","text":"","title":"GENE_RESULT_SCHEMA"},{"location":"developing_a_pheval_plugin/#src.pheval.post_processing.validate_result_format.ResultSchema.VARIANT_RESULT_SCHEMA","text":"","title":"VARIANT_RESULT_SCHEMA"},{"location":"developing_a_pheval_plugin/#src.pheval.post_processing.validate_result_format.ResultSchema.DISEASE_RESULT_SCHEMA","text":"The grouping_id column is optional and is designed to handle cases where entities should be jointly ranked without being penalised. For example, in the ranking of compound heterozygous variant which occurs when two or more variants, inherited together, contribute to a phenotype. For this purpose, variants that are part of the same compound heterozygous group (e.g., within the same gene) should be assigned the same grouping_id . This ensures they are ranked as a single entity, preserving their combined significance. Variants that are not part of any compound heterozygous group should each have a unique grouping_id . This approach prevents any unintended overlap in ranking and ensures that each group or individual variant is accurately represented. The use of the grouping_id would also be suitable for the ranking and prioritisation of polygenic diseases. Depending on whether you need to generate gene, variant, and or disease results depends on the final method called to generate the results from the polars dataframe. The methods are outlined below: \u26a0\ufe0f Breaking Change (v0.5.0): The helper method generate_pheval_result has been replaced with three separate methods for each result type: - generate_gene_result - generate_variant_result - generate_disease_result Update your runner implementation to call the appropriate method based on the type of result your tool produces. Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @validate_dataframe ( ResultSchema . GENE_RESULT_SCHEMA ) def generate_gene_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_gene_results/ { result_path . stem } -gene_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_gene_results\" ), ResultType . GENE ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_gene_results ( ranked_results , output_file ) _write_gene_result ( classified_results , output_file ) Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @validate_dataframe ( ResultSchema . VARIANT_RESULT_SCHEMA ) def generate_variant_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_variant_results/ { result_path . stem } -variant_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_variant_results\" ), ResultType . VARIANT , ) ranked_results = _rank_results ( results , sort_order ) . with_columns ( pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ) ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_variant_results ( ranked_results , output_file ) _write_variant_result ( classified_results , output_file ) Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @validate_dataframe ( ResultSchema . DISEASE_RESULT_SCHEMA ) def generate_disease_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_disease_results/ { result_path . stem } -disease_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_disease_results\" ), ResultType . DISEASE , ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_disease_results ( ranked_results , output_file , mondo_mapping_table ) _write_disease_result ( classified_results , output_file ) An example of how the method can be called is outlined here: from pheval.post_processing.post_processing import generate_gene_result , SortOrder generate_gene_result ( results = pheval_gene_result , # this is the polars dataframe containing extracted PhEval result requirements sort_order = SortOrder . DESCENDING , # or can be ASCENDING - this determines in which order the scores will be ranked output_dir = output_directory , # this can be accessed from the runner instance e.g., self.output_dir result_path = result_path # this is the path to the tool-specific raw results file phenopacket_dir = phenopacket_dir # this is the path to the directory containing the phenopackets )","title":"DISEASE_RESULT_SCHEMA"},{"location":"developing_a_pheval_plugin/#adding-metadata-to-the-resultsyml","text":"By default, PhEval will write a results.yml to the output directory supplied on the CLI. The results.yml contains basic metadata regarding the run configuration, however, there is also the option to add customised run metadata to the results.yml in the tool_specific_configuration_options field. To achieve this, you'll need to create a construct_meta_data() method within your runner implementation. This method is responsible for appending customised metadata to the metadata object in the form of a defined dataclass. It should return the entire metadata object once the addition is completed. e.g., Defined customised metadata dataclass: from dataclasses import dataclass @dataclass class CustomisedMetaData : customised_field : str Example of implementation in the runner. from dataclasses import dataclass from pheval.runners.runner import PhEvalRunner from pathlib import Path @dataclass class CustomPhevalRunner ( PhEvalRunner ): \"\"\"CustomPhevalRunner Class.\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): \"\"\"prepare method.\"\"\" print ( \"preparing\" ) def run ( self ): \"\"\"run method.\"\"\" print ( \"running with custom pheval runner\" ) def post_process ( self ): \"\"\"post_process method.\"\"\" print ( \"post processing\" ) def construct_meta_data ( self ): \"\"\"Add metadata.\"\"\" self . meta_data . tool_specific_configuration_options = CustomisedMetaData ( customised_field = \"customised_value\" ) return self . meta_data","title":"Adding metadata to the results.yml"},{"location":"developing_a_pheval_plugin/#6-test-it","text":"To update your custom pheval runner implementation, you must first install the package poetry install Now you have to be able to run PhEval passing your custom runner as parameter. e.g., pheval run -i ./input_dir -t ./test_data_dir -r 'customphevalrunner' -o output_dir The -r parameter stands for your plugin runner class name, and it must be entirely lowercase. Output: preparing running post processing","title":"6. Test it."},{"location":"executing_a_benchmark/","text":"Executing a Benchmark PhEval is designed for benchmarking algorithms across various datasets. To execute a benchmark using PhEval, you need to: Execute your runner; generating the PhEval standardised parquet outputs for gene/variant/disease prioritisation. Configure the benchmarking parameters. Run the benchmark. PhEval will generate various performance reports, allowing you to easily compare the effectiveness of different algorithms. After the Runner Execution After executing a run, you may be left with an output directory structure like so: . \u251c\u2500\u2500 pheval_disease_results \u2502 \u251c\u2500\u2500 patient_1-disease_result.parquet \u251c\u2500\u2500 pheval_gene_results \u2502 \u251c\u2500\u2500 patient_1-gene_result.parquet \u251c\u2500\u2500 pheval_variant_results \u2502 \u251c\u2500\u2500 patient_1-variant_result.parquet \u251c\u2500\u2500 raw_results \u2502 \u251c\u2500\u2500 patient_1.json \u251c\u2500\u2500 results.yml \u2514\u2500\u2500 tool_input_commands \u2514\u2500\u2500 tool_input_commands.txt Whether you have populated pheval_disease_results , pheval_gene_results , and pheval_variant_results directories will depend on what is specified in the config.yaml for the runner execution. It is the results in these directories that are consumed in the benchmarking to produce the statistical comparison reports. Benchmarking Configuration File To configure the benchmarking parameters, a YAML configuration file should be created and supplied to the CLI command. An outline of the configuration file structure follows below: benchmark_name : exomiser_14_benchmark runs : - run_identifier : run_identifier_1 results_dir : /path/to/results_dir_1 phenopacket_dir : /path/to/phenopacket_dir gene_analysis : True variant_analysis : False disease_analysis : True threshold : score_order : descending - run_identifier : run_identifier_2 results_dir : /path/to/results_dir_2 phenopacket_dir : /path/to/phenopacket_dir gene_analysis : True variant_analysis : True disease_analysis : True threshold : score_order : descending plot_customisation : gene_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : disease_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : variant_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : The benchmark_name is what will be used to name the duckdb database that will contain all the ranking and binary statistics as well as comparisons between runs. The name provided should not have any whitespace or special characters. Runs section The runs section specifies which run configurations should be included in the benchmarking. For each run configuration you will need to populate the following parameters: run_identifier : The identifier associated with the run - this should be meaningful as it will be used in the naming in tables and plots. results_dir : The full path to the root directory where the directories pheval_gene_results / pheval_variant_results / pheval_disease_results can be found. phenopacket_dir : The full path to the phenopacket directory used during the runner execution. gene_analysis : Boolean specifying whether to perform benchmarking for gene prioritisation analysis. variant_analysis : Boolean specifying whether to perform benchmarking for variant prioritisation analysis disease_analysis : Boolean specifying whether to perform benchmarking for disease prioritisation analysis threshold : OPTIONAL score threshold to consider for inclusion of results. score_order : Ordering of results for ranking. Either ascending or descending. Plot customisation section The plot_customisation section specifies any additional customisation to the plots output from the benchmarking. Here you can specify title names for all the plots output, as well as the plot type for displaying the summary ranking stats. This section is split by the plots output from the gene, variant and disease prioritisation benchmarking. The parameters in this section do not need to be populated - however, if left blank it will default to generic titles. The parameters as follows are: plot_type : The plot type output for the summary rank stats plot. This can be either, bar_cumulative, bar_non_cumulative or bar_stacked. rank_plot_title : The customised title for the summary rank stats plot. roc_curve_title : The customised title for the ROC curve plot. precision_recall_title The customised title for the precision-recall curve plot. Executing the benchmark After configuring the benchmarking YAML, executing the benchmark is relatively simple. pheval-utils benchmark --run-yaml benchmarking_config.yaml Note: As of pheval-utils version 0.5.0 onwards, the command is benchmark . In earlier versions, the equivalent command was generate-benchmark-stats . See the v0.5.1 release notes for more details.","title":"Executing a Benchmark"},{"location":"executing_a_benchmark/#executing-a-benchmark","text":"PhEval is designed for benchmarking algorithms across various datasets. To execute a benchmark using PhEval, you need to: Execute your runner; generating the PhEval standardised parquet outputs for gene/variant/disease prioritisation. Configure the benchmarking parameters. Run the benchmark. PhEval will generate various performance reports, allowing you to easily compare the effectiveness of different algorithms.","title":"Executing a Benchmark"},{"location":"executing_a_benchmark/#after-the-runner-execution","text":"After executing a run, you may be left with an output directory structure like so: . \u251c\u2500\u2500 pheval_disease_results \u2502 \u251c\u2500\u2500 patient_1-disease_result.parquet \u251c\u2500\u2500 pheval_gene_results \u2502 \u251c\u2500\u2500 patient_1-gene_result.parquet \u251c\u2500\u2500 pheval_variant_results \u2502 \u251c\u2500\u2500 patient_1-variant_result.parquet \u251c\u2500\u2500 raw_results \u2502 \u251c\u2500\u2500 patient_1.json \u251c\u2500\u2500 results.yml \u2514\u2500\u2500 tool_input_commands \u2514\u2500\u2500 tool_input_commands.txt Whether you have populated pheval_disease_results , pheval_gene_results , and pheval_variant_results directories will depend on what is specified in the config.yaml for the runner execution. It is the results in these directories that are consumed in the benchmarking to produce the statistical comparison reports.","title":"After the Runner Execution"},{"location":"executing_a_benchmark/#benchmarking-configuration-file","text":"To configure the benchmarking parameters, a YAML configuration file should be created and supplied to the CLI command. An outline of the configuration file structure follows below: benchmark_name : exomiser_14_benchmark runs : - run_identifier : run_identifier_1 results_dir : /path/to/results_dir_1 phenopacket_dir : /path/to/phenopacket_dir gene_analysis : True variant_analysis : False disease_analysis : True threshold : score_order : descending - run_identifier : run_identifier_2 results_dir : /path/to/results_dir_2 phenopacket_dir : /path/to/phenopacket_dir gene_analysis : True variant_analysis : True disease_analysis : True threshold : score_order : descending plot_customisation : gene_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : disease_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : variant_plots : plot_type : bar_cumulative rank_plot_title : roc_curve_title : precision_recall_title : The benchmark_name is what will be used to name the duckdb database that will contain all the ranking and binary statistics as well as comparisons between runs. The name provided should not have any whitespace or special characters.","title":"Benchmarking Configuration File"},{"location":"executing_a_benchmark/#runs-section","text":"The runs section specifies which run configurations should be included in the benchmarking. For each run configuration you will need to populate the following parameters: run_identifier : The identifier associated with the run - this should be meaningful as it will be used in the naming in tables and plots. results_dir : The full path to the root directory where the directories pheval_gene_results / pheval_variant_results / pheval_disease_results can be found. phenopacket_dir : The full path to the phenopacket directory used during the runner execution. gene_analysis : Boolean specifying whether to perform benchmarking for gene prioritisation analysis. variant_analysis : Boolean specifying whether to perform benchmarking for variant prioritisation analysis disease_analysis : Boolean specifying whether to perform benchmarking for disease prioritisation analysis threshold : OPTIONAL score threshold to consider for inclusion of results. score_order : Ordering of results for ranking. Either ascending or descending.","title":"Runs section"},{"location":"executing_a_benchmark/#plot-customisation-section","text":"The plot_customisation section specifies any additional customisation to the plots output from the benchmarking. Here you can specify title names for all the plots output, as well as the plot type for displaying the summary ranking stats. This section is split by the plots output from the gene, variant and disease prioritisation benchmarking. The parameters in this section do not need to be populated - however, if left blank it will default to generic titles. The parameters as follows are: plot_type : The plot type output for the summary rank stats plot. This can be either, bar_cumulative, bar_non_cumulative or bar_stacked. rank_plot_title : The customised title for the summary rank stats plot. roc_curve_title : The customised title for the ROC curve plot. precision_recall_title The customised title for the precision-recall curve plot.","title":"Plot customisation section"},{"location":"executing_a_benchmark/#executing-the-benchmark","text":"After configuring the benchmarking YAML, executing the benchmark is relatively simple. pheval-utils benchmark --run-yaml benchmarking_config.yaml Note: As of pheval-utils version 0.5.0 onwards, the command is benchmark . In earlier versions, the equivalent command was generate-benchmark-stats . See the v0.5.1 release notes for more details.","title":"Executing the benchmark"},{"location":"plugins/","text":"A full list of implemented PhEval runners are listed below along with links to the original tool: Tool PhEval plugin Comment Exomiser Exomiser runner The link to the original tool can be found here Phen2Gene Phen2Gene runner The link to the original tool can be found here LIRICAL LIRICAL runner The link to the original tool can be found here SvAnna SvAnna runner The link to the original tool can be found here GADO GADO runner The link to the original tool can be found here Template Template runner OntoGPT OntoGPT runner ELDER ELDER runner MALCO MALCO runner AI MARRVEL AI MARRVEL runner The link to the original tool can be found here OAK OAK runner PhenoGenius PhenoGenius runner The link to the original tool can be found here","title":"Plugins"},{"location":"roadmap/","text":"Roadmap The Roadmap is a rough plan, changes are expected throughout the year. 2023 Q1 Finalising the PhEval architecture (draft is done) End-to-end pipeline for testing PhEval with Exomiser and two versions of HPO Submitting a poster to Biocuration which outlines the full vision Q2 Focus on an analytic framework around PhEval, focusing on studying how changes to ontologies affect changes in variant prioritisation Extend phenotype pipeline to enable base releases and alternative patterns Q3 Improving the analytic framework of PhEval, especially phenotype analysis All intermediate files of pipeline have a corresponding LinkML model Focus on studying the effect of KG snippets (p2ds) on VP performance Q4 Drafting a PhEval paper Building standalone pipeline that reports changes in algorithm behaviours to ontology developers.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"The Roadmap is a rough plan, changes are expected throughout the year.","title":"Roadmap"},{"location":"roadmap/#2023","text":"","title":"2023"},{"location":"roadmap/#q1","text":"Finalising the PhEval architecture (draft is done) End-to-end pipeline for testing PhEval with Exomiser and two versions of HPO Submitting a poster to Biocuration which outlines the full vision","title":"Q1"},{"location":"roadmap/#q2","text":"Focus on an analytic framework around PhEval, focusing on studying how changes to ontologies affect changes in variant prioritisation Extend phenotype pipeline to enable base releases and alternative patterns","title":"Q2"},{"location":"roadmap/#q3","text":"Improving the analytic framework of PhEval, especially phenotype analysis All intermediate files of pipeline have a corresponding LinkML model Focus on studying the effect of KG snippets (p2ds) on VP performance","title":"Q3"},{"location":"roadmap/#q4","text":"Drafting a PhEval paper Building standalone pipeline that reports changes in algorithm behaviours to ontology developers.","title":"Q4"},{"location":"styleguide/","text":"Monarch Style Guide for PhEval No code in CLI methods","title":"Monarch Style Guide for PhEval"},{"location":"styleguide/#monarch-style-guide-for-pheval","text":"No code in CLI methods","title":"Monarch Style Guide for PhEval"},{"location":"api/pheval/cli/","text":"main Main CLI method for PhEval. Usage: main [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default -v , --verbose integer range ( 0 and above) N/A 0 -q , --quiet boolean N/A False --help boolean Show this message and exit. False Subcommands pheval : pheval pheval-utils : pheval_utils pheval pheval Usage: main pheval [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands run : PhEval Runner Command Line Interface update : # run PhEval Runner Command Line Interface Args: input_dir (Path): The input directory (relative path: e.g exomiser-13.11) testdata_dir (Path): The input directory (relative path: e.g ./data runner (str): Runner implementation (e.g exomiser-13.11) tmp_dir (Path): The path of the temporary directory (optional) output_dir (Path): The path of the output directory config (Path): The path of the configuration file (optional e.g., config.yaml) version (str): The version of the tool implementation Usage: main pheval run [OPTIONS] Options: Name Type Description Default --input-dir , -i Path The input directory (relative path: e.g exomiser-13.11) _required --testdata-dir , -t Path The input directory (relative path: e.g ./data) _required --runner , -r text Runner implementation (e.g exomiser-13.11) _required --tmp-dir , -m Path The path of the temporary directory (optional) None --output-dir , -o Path The path of the output directory _required --config , -c Path The path of the configuration file (optional e.g config.yaml) None --version , -v text Version of the tool implementation. None --help boolean Show this message and exit. False # update Download the latest MONDO and HGNC mapping files. This command fetches the most recent versions of: The MONDO SSSOM mapping file from the Monarch Initiative The HGNC complete gene set from the HGNC download site These files are saved to the resources/ directory and will overwrite any existing versions. This ensures that PhEval has access to the most up-to-date identifier mappings for disease and gene normalisation. Usage: main pheval update [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False pheval-utils pheval_utils Usage: main pheval-utils [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands benchmark : Benchmark the gene/variant/disease prioritisation performance for runs. create-spiked-vcfs : generate-plots : Generate bar plot from benchmark db. prepare-corpus : scramble-phenopackets : Generate noisy phenopackets from existing ones. semsim-scramble : Scrambles semsim profile multiplying score value by scramble factor semsim-to-exomiserdb : ingests semsim file into exomiser phenotypic database update-phenopackets : Update gene symbols and identifiers for phenopackets. # benchmark Benchmark the gene/variant/disease prioritisation performance for runs. Usage: main pheval-utils benchmark [OPTIONS] Options: Name Type Description Default --run-yaml , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False # create-spiked-vcfs Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). output_dir (Path): The directory to store the generated spiked VCF file(s). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). Usage: main pheval-utils create-spiked-vcfs [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -O Path Path for creation of output directory vcf --help boolean Show this message and exit. False # generate-plots Generate bar plot from benchmark db. Usage: main pheval-utils generate-plots [OPTIONS] Options: Name Type Description Default --benchmark-db , -b Path Path to benchmark db output by PhEval benchmark commands. _required --run-data , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False # prepare-corpus Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Usage: main pheval-utils prepare-corpus [OPTIONS] Options: Name Type Description Default --phenopacket-dir , -p Path Path to phenopacket corpus directory.. _required --variant-analysis / --no-variant-analysis boolean Specify whether to check for complete variant records in the phenopackets. False --gene-analysis / --no-gene-analysis boolean Specify whether to check for complete gene records in the phenopackets. False --disease-analysis / --no-disease-analysis boolean Specify whether to check for complete disease records in the phenopackets. False --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to update in phenopacket None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -o Path Path to output prepared corpus. prepared_corpus --help boolean Show this message and exit. False # scramble-phenopackets Generate noisy phenopackets from existing ones. Usage: main pheval-utils scramble-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopackets directory. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --scramble-factor , -s float Scramble factor for randomising phenopacket phenotypic profiles. 0.5 --output-dir , -O Path Path for creation of output directory noisy_phenopackets --local-ontology-cache , -l Path Path to the local ontology cache, e.g., path to the hp.obo. None --help boolean Show this message and exit. False # semsim-scramble Scrambles semsim profile multiplying score value by scramble factor Args: input (Path): Path file that points out to the semsim profile output (Path): Path file that points out to the output file score_column (List[str]): Score column(s) that will be scrambled scramble_factor (float): Scramble Magnitude Usage: main pheval-utils semsim-scramble [OPTIONS] Options: Name Type Description Default --input , -i Path Path to the semantic similarity profile to be scrambled. _required --output , -o Path Path where the scrambled semsim file will be written. _required --score-column , -c choice ( jaccard_similarity | dice_similarity | phenodigm_score ) Score column that will be scrambled _required --scramble-factor , -s float Scramble Magnitude (noise) that will be applied to semantic similarity score column (e.g. jaccard similarity). 0.5 --help boolean Show this message and exit. False # semsim-to-exomiserdb ingests semsim file into exomiser phenotypic database Args: input_file (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) Usage: main pheval-utils semsim-to-exomiserdb [OPTIONS] Options: Name Type Description Default --input-file , -i Path Semsim input file. _required --object-prefix text Object Prefix. e.g. MP _required --subject-prefix text Subject Prefix. e.g. HP _required --db-path , -d Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/). This is the path where the phenotypic database folder will be written out. _required --help boolean Show this message and exit. False # update-phenopackets Update gene symbols and identifiers for phenopackets. Usage: main pheval-utils update-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --output-dir , -o Path Path to write phenopacket. _required --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to add to phenopacket ensembl_id --help boolean Show this message and exit. False pheval pheval Usage: pheval [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands run : PhEval Runner Command Line Interface update : run PhEval Runner Command Line Interface Args: input_dir (Path): The input directory (relative path: e.g exomiser-13.11) testdata_dir (Path): The input directory (relative path: e.g ./data runner (str): Runner implementation (e.g exomiser-13.11) tmp_dir (Path): The path of the temporary directory (optional) output_dir (Path): The path of the output directory config (Path): The path of the configuration file (optional e.g., config.yaml) version (str): The version of the tool implementation Usage: pheval run [OPTIONS] Options: Name Type Description Default --input-dir , -i Path The input directory (relative path: e.g exomiser-13.11) _required --testdata-dir , -t Path The input directory (relative path: e.g ./data) _required --runner , -r text Runner implementation (e.g exomiser-13.11) _required --tmp-dir , -m Path The path of the temporary directory (optional) None --output-dir , -o Path The path of the output directory _required --config , -c Path The path of the configuration file (optional e.g config.yaml) None --version , -v text Version of the tool implementation. None --help boolean Show this message and exit. False update Download the latest MONDO and HGNC mapping files. This command fetches the most recent versions of: The MONDO SSSOM mapping file from the Monarch Initiative The HGNC complete gene set from the HGNC download site These files are saved to the resources/ directory and will overwrite any existing versions. This ensures that PhEval has access to the most up-to-date identifier mappings for disease and gene normalisation. Usage: pheval update [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False pheval-utils pheval_utils Usage: pheval-utils [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands benchmark : Benchmark the gene/variant/disease prioritisation performance for runs. create-spiked-vcfs : generate-plots : Generate bar plot from benchmark db. prepare-corpus : scramble-phenopackets : Generate noisy phenopackets from existing ones. semsim-scramble : Scrambles semsim profile multiplying score value by scramble factor semsim-to-exomiserdb : ingests semsim file into exomiser phenotypic database update-phenopackets : Update gene symbols and identifiers for phenopackets. benchmark Benchmark the gene/variant/disease prioritisation performance for runs. Usage: pheval-utils benchmark [OPTIONS] Options: Name Type Description Default --run-yaml , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False create-spiked-vcfs Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). output_dir (Path): The directory to store the generated spiked VCF file(s). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). Usage: pheval-utils create-spiked-vcfs [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -O Path Path for creation of output directory vcf --help boolean Show this message and exit. False generate-plots Generate bar plot from benchmark db. Usage: pheval-utils generate-plots [OPTIONS] Options: Name Type Description Default --benchmark-db , -b Path Path to benchmark db output by PhEval benchmark commands. _required --run-data , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False prepare-corpus Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Usage: pheval-utils prepare-corpus [OPTIONS] Options: Name Type Description Default --phenopacket-dir , -p Path Path to phenopacket corpus directory.. _required --variant-analysis / --no-variant-analysis boolean Specify whether to check for complete variant records in the phenopackets. False --gene-analysis / --no-gene-analysis boolean Specify whether to check for complete gene records in the phenopackets. False --disease-analysis / --no-disease-analysis boolean Specify whether to check for complete disease records in the phenopackets. False --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to update in phenopacket None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -o Path Path to output prepared corpus. prepared_corpus --help boolean Show this message and exit. False scramble-phenopackets Generate noisy phenopackets from existing ones. Usage: pheval-utils scramble-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopackets directory. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --scramble-factor , -s float Scramble factor for randomising phenopacket phenotypic profiles. 0.5 --output-dir , -O Path Path for creation of output directory noisy_phenopackets --local-ontology-cache , -l Path Path to the local ontology cache, e.g., path to the hp.obo. None --help boolean Show this message and exit. False semsim-scramble Scrambles semsim profile multiplying score value by scramble factor Args: input (Path): Path file that points out to the semsim profile output (Path): Path file that points out to the output file score_column (List[str]): Score column(s) that will be scrambled scramble_factor (float): Scramble Magnitude Usage: pheval-utils semsim-scramble [OPTIONS] Options: Name Type Description Default --input , -i Path Path to the semantic similarity profile to be scrambled. _required --output , -o Path Path where the scrambled semsim file will be written. _required --score-column , -c choice ( jaccard_similarity | dice_similarity | phenodigm_score ) Score column that will be scrambled _required --scramble-factor , -s float Scramble Magnitude (noise) that will be applied to semantic similarity score column (e.g. jaccard similarity). 0.5 --help boolean Show this message and exit. False semsim-to-exomiserdb ingests semsim file into exomiser phenotypic database Args: input_file (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) Usage: pheval-utils semsim-to-exomiserdb [OPTIONS] Options: Name Type Description Default --input-file , -i Path Semsim input file. _required --object-prefix text Object Prefix. e.g. MP _required --subject-prefix text Subject Prefix. e.g. HP _required --db-path , -d Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/). This is the path where the phenotypic database folder will be written out. _required --help boolean Show this message and exit. False update-phenopackets Update gene symbols and identifiers for phenopackets. Usage: pheval-utils update-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --output-dir , -o Path Path to write phenopacket. _required --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to add to phenopacket ensembl_id --help boolean Show this message and exit. False","title":"Cli"},{"location":"api/pheval/cli/#main","text":"Main CLI method for PhEval. Usage: main [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default -v , --verbose integer range ( 0 and above) N/A 0 -q , --quiet boolean N/A False --help boolean Show this message and exit. False Subcommands pheval : pheval pheval-utils : pheval_utils","title":"main"},{"location":"api/pheval/cli/#pheval","text":"pheval Usage: main pheval [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands run : PhEval Runner Command Line Interface update :","title":"pheval"},{"location":"api/pheval/cli/#run","text":"PhEval Runner Command Line Interface Args: input_dir (Path): The input directory (relative path: e.g exomiser-13.11) testdata_dir (Path): The input directory (relative path: e.g ./data runner (str): Runner implementation (e.g exomiser-13.11) tmp_dir (Path): The path of the temporary directory (optional) output_dir (Path): The path of the output directory config (Path): The path of the configuration file (optional e.g., config.yaml) version (str): The version of the tool implementation Usage: main pheval run [OPTIONS] Options: Name Type Description Default --input-dir , -i Path The input directory (relative path: e.g exomiser-13.11) _required --testdata-dir , -t Path The input directory (relative path: e.g ./data) _required --runner , -r text Runner implementation (e.g exomiser-13.11) _required --tmp-dir , -m Path The path of the temporary directory (optional) None --output-dir , -o Path The path of the output directory _required --config , -c Path The path of the configuration file (optional e.g config.yaml) None --version , -v text Version of the tool implementation. None --help boolean Show this message and exit. False","title":"# run"},{"location":"api/pheval/cli/#update","text":"Download the latest MONDO and HGNC mapping files. This command fetches the most recent versions of: The MONDO SSSOM mapping file from the Monarch Initiative The HGNC complete gene set from the HGNC download site These files are saved to the resources/ directory and will overwrite any existing versions. This ensures that PhEval has access to the most up-to-date identifier mappings for disease and gene normalisation. Usage: main pheval update [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"# update"},{"location":"api/pheval/cli/#pheval-utils","text":"pheval_utils Usage: main pheval-utils [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands benchmark : Benchmark the gene/variant/disease prioritisation performance for runs. create-spiked-vcfs : generate-plots : Generate bar plot from benchmark db. prepare-corpus : scramble-phenopackets : Generate noisy phenopackets from existing ones. semsim-scramble : Scrambles semsim profile multiplying score value by scramble factor semsim-to-exomiserdb : ingests semsim file into exomiser phenotypic database update-phenopackets : Update gene symbols and identifiers for phenopackets.","title":"pheval-utils"},{"location":"api/pheval/cli/#benchmark","text":"Benchmark the gene/variant/disease prioritisation performance for runs. Usage: main pheval-utils benchmark [OPTIONS] Options: Name Type Description Default --run-yaml , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False","title":"# benchmark"},{"location":"api/pheval/cli/#create-spiked-vcfs","text":"Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). output_dir (Path): The directory to store the generated spiked VCF file(s). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). Usage: main pheval-utils create-spiked-vcfs [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -O Path Path for creation of output directory vcf --help boolean Show this message and exit. False","title":"# create-spiked-vcfs"},{"location":"api/pheval/cli/#generate-plots","text":"Generate bar plot from benchmark db. Usage: main pheval-utils generate-plots [OPTIONS] Options: Name Type Description Default --benchmark-db , -b Path Path to benchmark db output by PhEval benchmark commands. _required --run-data , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False","title":"# generate-plots"},{"location":"api/pheval/cli/#prepare-corpus","text":"Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Usage: main pheval-utils prepare-corpus [OPTIONS] Options: Name Type Description Default --phenopacket-dir , -p Path Path to phenopacket corpus directory.. _required --variant-analysis / --no-variant-analysis boolean Specify whether to check for complete variant records in the phenopackets. False --gene-analysis / --no-gene-analysis boolean Specify whether to check for complete gene records in the phenopackets. False --disease-analysis / --no-disease-analysis boolean Specify whether to check for complete disease records in the phenopackets. False --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to update in phenopacket None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -o Path Path to output prepared corpus. prepared_corpus --help boolean Show this message and exit. False","title":"# prepare-corpus"},{"location":"api/pheval/cli/#scramble-phenopackets","text":"Generate noisy phenopackets from existing ones. Usage: main pheval-utils scramble-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopackets directory. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --scramble-factor , -s float Scramble factor for randomising phenopacket phenotypic profiles. 0.5 --output-dir , -O Path Path for creation of output directory noisy_phenopackets --local-ontology-cache , -l Path Path to the local ontology cache, e.g., path to the hp.obo. None --help boolean Show this message and exit. False","title":"# scramble-phenopackets"},{"location":"api/pheval/cli/#semsim-scramble","text":"Scrambles semsim profile multiplying score value by scramble factor Args: input (Path): Path file that points out to the semsim profile output (Path): Path file that points out to the output file score_column (List[str]): Score column(s) that will be scrambled scramble_factor (float): Scramble Magnitude Usage: main pheval-utils semsim-scramble [OPTIONS] Options: Name Type Description Default --input , -i Path Path to the semantic similarity profile to be scrambled. _required --output , -o Path Path where the scrambled semsim file will be written. _required --score-column , -c choice ( jaccard_similarity | dice_similarity | phenodigm_score ) Score column that will be scrambled _required --scramble-factor , -s float Scramble Magnitude (noise) that will be applied to semantic similarity score column (e.g. jaccard similarity). 0.5 --help boolean Show this message and exit. False","title":"# semsim-scramble"},{"location":"api/pheval/cli/#semsim-to-exomiserdb","text":"ingests semsim file into exomiser phenotypic database Args: input_file (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) Usage: main pheval-utils semsim-to-exomiserdb [OPTIONS] Options: Name Type Description Default --input-file , -i Path Semsim input file. _required --object-prefix text Object Prefix. e.g. MP _required --subject-prefix text Subject Prefix. e.g. HP _required --db-path , -d Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/). This is the path where the phenotypic database folder will be written out. _required --help boolean Show this message and exit. False","title":"# semsim-to-exomiserdb"},{"location":"api/pheval/cli/#update-phenopackets","text":"Update gene symbols and identifiers for phenopackets. Usage: main pheval-utils update-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --output-dir , -o Path Path to write phenopacket. _required --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to add to phenopacket ensembl_id --help boolean Show this message and exit. False","title":"# update-phenopackets"},{"location":"api/pheval/cli/#pheval_1","text":"pheval Usage: pheval [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands run : PhEval Runner Command Line Interface update :","title":"pheval"},{"location":"api/pheval/cli/#run_1","text":"PhEval Runner Command Line Interface Args: input_dir (Path): The input directory (relative path: e.g exomiser-13.11) testdata_dir (Path): The input directory (relative path: e.g ./data runner (str): Runner implementation (e.g exomiser-13.11) tmp_dir (Path): The path of the temporary directory (optional) output_dir (Path): The path of the output directory config (Path): The path of the configuration file (optional e.g., config.yaml) version (str): The version of the tool implementation Usage: pheval run [OPTIONS] Options: Name Type Description Default --input-dir , -i Path The input directory (relative path: e.g exomiser-13.11) _required --testdata-dir , -t Path The input directory (relative path: e.g ./data) _required --runner , -r text Runner implementation (e.g exomiser-13.11) _required --tmp-dir , -m Path The path of the temporary directory (optional) None --output-dir , -o Path The path of the output directory _required --config , -c Path The path of the configuration file (optional e.g config.yaml) None --version , -v text Version of the tool implementation. None --help boolean Show this message and exit. False","title":"run"},{"location":"api/pheval/cli/#update_1","text":"Download the latest MONDO and HGNC mapping files. This command fetches the most recent versions of: The MONDO SSSOM mapping file from the Monarch Initiative The HGNC complete gene set from the HGNC download site These files are saved to the resources/ directory and will overwrite any existing versions. This ensures that PhEval has access to the most up-to-date identifier mappings for disease and gene normalisation. Usage: pheval update [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"update"},{"location":"api/pheval/cli/#pheval-utils_1","text":"pheval_utils Usage: pheval-utils [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False Subcommands benchmark : Benchmark the gene/variant/disease prioritisation performance for runs. create-spiked-vcfs : generate-plots : Generate bar plot from benchmark db. prepare-corpus : scramble-phenopackets : Generate noisy phenopackets from existing ones. semsim-scramble : Scrambles semsim profile multiplying score value by scramble factor semsim-to-exomiserdb : ingests semsim file into exomiser phenotypic database update-phenopackets : Update gene symbols and identifiers for phenopackets.","title":"pheval-utils"},{"location":"api/pheval/cli/#benchmark_1","text":"Benchmark the gene/variant/disease prioritisation performance for runs. Usage: pheval-utils benchmark [OPTIONS] Options: Name Type Description Default --run-yaml , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False","title":"benchmark"},{"location":"api/pheval/cli/#create-spiked-vcfs_1","text":"Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). output_dir (Path): The directory to store the generated spiked VCF file(s). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). Usage: pheval-utils create-spiked-vcfs [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -O Path Path for creation of output directory vcf --help boolean Show this message and exit. False","title":"create-spiked-vcfs"},{"location":"api/pheval/cli/#generate-plots_1","text":"Generate bar plot from benchmark db. Usage: pheval-utils generate-plots [OPTIONS] Options: Name Type Description Default --benchmark-db , -b Path Path to benchmark db output by PhEval benchmark commands. _required --run-data , -r Path Path to yaml configuration file for benchmarking. _required --help boolean Show this message and exit. False","title":"generate-plots"},{"location":"api/pheval/cli/#prepare-corpus_1","text":"Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): Path to the directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing the hg38 VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Usage: pheval-utils prepare-corpus [OPTIONS] Options: Name Type Description Default --phenopacket-dir , -p Path Path to phenopacket corpus directory.. _required --variant-analysis / --no-variant-analysis boolean Specify whether to check for complete variant records in the phenopackets. False --gene-analysis / --no-gene-analysis boolean Specify whether to check for complete gene records in the phenopackets. False --disease-analysis / --no-disease-analysis boolean Specify whether to check for complete disease records in the phenopackets. False --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to update in phenopacket None --hg19-template-vcf , -hg19 Path Template hg19 VCF file NOTE: This argument is mutually exclusive with arguments: [hg19_vcf_dir]. None --hg38-template-vcf , -hg38 Path Template hg38 VCF file NOTE: This argument is mutually exclusive with arguments: [hg38_vcf_dir]. None --hg19-vcf-dir , -hg19-dir Path Path to directory containing hg19 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg19_template_vcf]. None --hg38-vcf-dir , -hg38-dir Path Path to directory containing hg38 VCF templates. NOTE: This argument is mutually exclusive with arguments: [hg38_template_vcf]. None --output-dir , -o Path Path to output prepared corpus. prepared_corpus --help boolean Show this message and exit. False","title":"prepare-corpus"},{"location":"api/pheval/cli/#scramble-phenopackets_1","text":"Generate noisy phenopackets from existing ones. Usage: pheval-utils scramble-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopackets directory. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --scramble-factor , -s float Scramble factor for randomising phenopacket phenotypic profiles. 0.5 --output-dir , -O Path Path for creation of output directory noisy_phenopackets --local-ontology-cache , -l Path Path to the local ontology cache, e.g., path to the hp.obo. None --help boolean Show this message and exit. False","title":"scramble-phenopackets"},{"location":"api/pheval/cli/#semsim-scramble_1","text":"Scrambles semsim profile multiplying score value by scramble factor Args: input (Path): Path file that points out to the semsim profile output (Path): Path file that points out to the output file score_column (List[str]): Score column(s) that will be scrambled scramble_factor (float): Scramble Magnitude Usage: pheval-utils semsim-scramble [OPTIONS] Options: Name Type Description Default --input , -i Path Path to the semantic similarity profile to be scrambled. _required --output , -o Path Path where the scrambled semsim file will be written. _required --score-column , -c choice ( jaccard_similarity | dice_similarity | phenodigm_score ) Score column that will be scrambled _required --scramble-factor , -s float Scramble Magnitude (noise) that will be applied to semantic similarity score column (e.g. jaccard similarity). 0.5 --help boolean Show this message and exit. False","title":"semsim-scramble"},{"location":"api/pheval/cli/#semsim-to-exomiserdb_1","text":"ingests semsim file into exomiser phenotypic database Args: input_file (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) Usage: pheval-utils semsim-to-exomiserdb [OPTIONS] Options: Name Type Description Default --input-file , -i Path Semsim input file. _required --object-prefix text Object Prefix. e.g. MP _required --subject-prefix text Subject Prefix. e.g. HP _required --db-path , -d Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/). This is the path where the phenotypic database folder will be written out. _required --help boolean Show this message and exit. False","title":"semsim-to-exomiserdb"},{"location":"api/pheval/cli/#update-phenopackets_1","text":"Update gene symbols and identifiers for phenopackets. Usage: pheval-utils update-phenopackets [OPTIONS] Options: Name Type Description Default --phenopacket-path , -p Path Path to phenopacket. NOTE: This argument is mutually exclusive with arguments: [phenopacket_dir]. None --phenopacket-dir , -P Path Path to phenopacket directory for updating. NOTE: This argument is mutually exclusive with arguments: [phenopacket_path]. None --output-dir , -o Path Path to write phenopacket. _required --gene-identifier , -g choice ( ensembl_id | entrez_id | hgnc_id ) Gene identifier to add to phenopacket ensembl_id --help boolean Show this message and exit. False","title":"update-phenopackets"},{"location":"api/pheval/config_parser/","text":"InputDirConfig dataclass Class for defining the fields within the input directory config. Parameters: Name Type Description Default tool str Name of the tool implementation (e.g. exomiser/phen2gene) required tool_version str Version of the tool implementation required variant_analysis bool Whether to extract prioritised variants from results. required gene_analysis bool Whether to extract prioritised genes from results. required disease_analysis bool Whether to extract prioritised diseases from results. required tool_specific_configuration_options Any Tool specific configurations required Source code in src/pheval/config_parser.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @serde @dataclass class InputDirConfig : \"\"\" Class for defining the fields within the input directory config. Args: tool (str): Name of the tool implementation (e.g. exomiser/phen2gene) tool_version (str): Version of the tool implementation variant_analysis (bool): Whether to extract prioritised variants from results. gene_analysis (bool): Whether to extract prioritised genes from results. disease_analysis (bool): Whether to extract prioritised diseases from results. tool_specific_configuration_options (Any): Tool specific configurations \"\"\" tool : str tool_version : str variant_analysis : bool gene_analysis : bool disease_analysis : bool tool_specific_configuration_options : Any parse_input_dir_config ( input_dir ) Reads the config file. Source code in src/pheval/config_parser.py 39 40 41 42 43 44 45 def parse_input_dir_config ( input_dir : Path ) -> InputDirConfig : \"\"\"Reads the config file.\"\"\" logger . info ( f \"Parsing config.yaml located in { input_dir } .\" ) with open ( Path ( input_dir ) . joinpath ( \"config.yaml\" ), \"r\" ) as config_file : config = yaml . safe_load ( config_file ) config_file . close () return from_yaml ( InputDirConfig , yaml . dump ( config ))","title":"Config parser"},{"location":"api/pheval/config_parser/#src.pheval.config_parser.InputDirConfig","text":"Class for defining the fields within the input directory config. Parameters: Name Type Description Default tool str Name of the tool implementation (e.g. exomiser/phen2gene) required tool_version str Version of the tool implementation required variant_analysis bool Whether to extract prioritised variants from results. required gene_analysis bool Whether to extract prioritised genes from results. required disease_analysis bool Whether to extract prioritised diseases from results. required tool_specific_configuration_options Any Tool specific configurations required Source code in src/pheval/config_parser.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @serde @dataclass class InputDirConfig : \"\"\" Class for defining the fields within the input directory config. Args: tool (str): Name of the tool implementation (e.g. exomiser/phen2gene) tool_version (str): Version of the tool implementation variant_analysis (bool): Whether to extract prioritised variants from results. gene_analysis (bool): Whether to extract prioritised genes from results. disease_analysis (bool): Whether to extract prioritised diseases from results. tool_specific_configuration_options (Any): Tool specific configurations \"\"\" tool : str tool_version : str variant_analysis : bool gene_analysis : bool disease_analysis : bool tool_specific_configuration_options : Any","title":"InputDirConfig"},{"location":"api/pheval/config_parser/#src.pheval.config_parser.parse_input_dir_config","text":"Reads the config file. Source code in src/pheval/config_parser.py 39 40 41 42 43 44 45 def parse_input_dir_config ( input_dir : Path ) -> InputDirConfig : \"\"\"Reads the config file.\"\"\" logger . info ( f \"Parsing config.yaml located in { input_dir } .\" ) with open ( Path ( input_dir ) . joinpath ( \"config.yaml\" ), \"r\" ) as config_file : config = yaml . safe_load ( config_file ) config_file . close () return from_yaml ( InputDirConfig , yaml . dump ( config ))","title":"parse_input_dir_config"},{"location":"api/pheval/run_metadata/","text":"BasicOutputRunMetaData dataclass Class for defining variables for the run metadata. Args: tool (str): Name of the tool implementation tool_version (str): Version of the tool implementation config (Path): Path to the config file located in the input directory run_timestamp (int): Time taken for run to complete corpus (Path): Path to corpus used in pheval run mondo_download_date (Optional[str]): ISO timestamp for MONDO file hgnc_download_date (Optional[str]): ISO timestamp for HGNC file tool_specific_configuration_options (Any): Special field that can be overwritten by tool implementations to contain any extra tool specific configurations used in the run Source code in src/pheval/run_metadata.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @serde @dataclass class BasicOutputRunMetaData : \"\"\"Class for defining variables for the run metadata. Args: tool (str): Name of the tool implementation tool_version (str): Version of the tool implementation config (Path): Path to the config file located in the input directory run_timestamp (int): Time taken for run to complete corpus (Path): Path to corpus used in pheval run mondo_download_date (Optional[str]): ISO timestamp for MONDO file hgnc_download_date (Optional[str]): ISO timestamp for HGNC file tool_specific_configuration_options (Any): Special field that can be overwritten by tool implementations to contain any extra tool specific configurations used in the run \"\"\" tool : str tool_version : str config : Path run_timestamp : int corpus : Path mondo_download_date : str hgnc_download_date : str tool_specific_configuration_options : Any = None","title":"Run metadata"},{"location":"api/pheval/run_metadata/#src.pheval.run_metadata.BasicOutputRunMetaData","text":"Class for defining variables for the run metadata. Args: tool (str): Name of the tool implementation tool_version (str): Version of the tool implementation config (Path): Path to the config file located in the input directory run_timestamp (int): Time taken for run to complete corpus (Path): Path to corpus used in pheval run mondo_download_date (Optional[str]): ISO timestamp for MONDO file hgnc_download_date (Optional[str]): ISO timestamp for HGNC file tool_specific_configuration_options (Any): Special field that can be overwritten by tool implementations to contain any extra tool specific configurations used in the run Source code in src/pheval/run_metadata.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @serde @dataclass class BasicOutputRunMetaData : \"\"\"Class for defining variables for the run metadata. Args: tool (str): Name of the tool implementation tool_version (str): Version of the tool implementation config (Path): Path to the config file located in the input directory run_timestamp (int): Time taken for run to complete corpus (Path): Path to corpus used in pheval run mondo_download_date (Optional[str]): ISO timestamp for MONDO file hgnc_download_date (Optional[str]): ISO timestamp for HGNC file tool_specific_configuration_options (Any): Special field that can be overwritten by tool implementations to contain any extra tool specific configurations used in the run \"\"\" tool : str tool_version : str config : Path run_timestamp : int corpus : Path mondo_download_date : str hgnc_download_date : str tool_specific_configuration_options : Any = None","title":"BasicOutputRunMetaData"},{"location":"api/pheval/analyse/benchmark/","text":"benchmark ( config , benchmark_type ) Benchmark results for specified runs for a specified prioritisation type for comparison. Args: config (Config): Configuration for benchmarking. benchmark_type (BenchmarkOutputType): Benchmark output type. Source code in src/pheval/analyse/benchmark.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def benchmark ( config : Config , benchmark_type : BenchmarkOutputType ) -> None : \"\"\" Benchmark results for specified runs for a specified prioritisation type for comparison. Args: config (Config): Configuration for benchmarking. benchmark_type (BenchmarkOutputType): Benchmark output type. \"\"\" conn = duckdb . connect ( f \" { config . benchmark_name } .duckdb\" ) stats , curve_results , true_positive_cases = process_stats ( config . runs , benchmark_type ) write_table ( conn , stats , f \" { config . benchmark_name } _ { benchmark_type . prioritisation_type_string } _summary\" ) write_table ( conn , curve_results , f \" { config . benchmark_name } _ { benchmark_type . prioritisation_type_string } _binary_classification_curves\" , ) calculate_rank_changes ( conn , [ run . run_identifier for run in config . runs ], true_positive_cases , benchmark_type ) generate_plots ( config . benchmark_name , stats , curve_results , benchmark_type , config . plot_customisation ) conn . close () benchmark_runs ( benchmark_config_file ) Benchmark results for specified runs for comparison. Args: benchmark_config_file (Path): Path to benchmark config file. Source code in src/pheval/analyse/benchmark.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def benchmark_runs ( benchmark_config_file : Path ) -> None : \"\"\" Benchmark results for specified runs for comparison. Args: benchmark_config_file (Path): Path to benchmark config file. \"\"\" logger = get_logger () start_time = time . perf_counter () logger . info ( \"Initiated benchmarking process.\" ) config = parse_run_config ( benchmark_config_file ) if Path ( f \" { config . benchmark_name } .duckdb\" ) . exists (): logger . error ( f \" { config . benchmark_name } .duckdb already exists! Exiting.\" ) sys . exit ( 1 ) gene_analysis_runs = [ run for run in config . runs if run . gene_analysis ] variant_analysis_runs = [ run for run in config . runs if run . variant_analysis ] disease_analysis_runs = [ run for run in config . runs if run . disease_analysis ] if gene_analysis_runs : logger . info ( \"Initiating benchmarking for gene results.\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = gene_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . GENE . value , ) logger . info ( \"Finished benchmarking for gene results.\" ) if variant_analysis_runs : logger . info ( \"Initiating benchmarking for variant results\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = variant_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . VARIANT . value , ) logger . info ( \"Finished benchmarking for variant results.\" ) if disease_analysis_runs : logger . info ( \"Initiating benchmarking for disease results\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = disease_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . DISEASE . value , ) logger . info ( \"Finished benchmarking for disease results.\" ) logger . info ( f \"Finished benchmarking! Total time: { time . perf_counter () - start_time : .2f } seconds.\" ) process_stats ( runs , benchmark_type ) Processes stats outputs for specified runs to compare. Args: runs (List[RunConfig]): List of runs to benchmark. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, pl.DataFrame]: The stats for all runs. Source code in src/pheval/analyse/benchmark.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def process_stats ( runs : List [ RunConfig ], benchmark_type : BenchmarkOutputType ) -> Tuple [ pl . DataFrame , pl . DataFrame , pl . DataFrame ]: \"\"\" Processes stats outputs for specified runs to compare. Args: runs (List[RunConfig]): List of runs to benchmark. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, pl.DataFrame]: The stats for all runs. \"\"\" stats , curve_results , true_positive_cases = [], [], [] for run in runs : result_scan = scan_directory ( run , benchmark_type ) stats . append ( compute_rank_stats ( run . run_identifier , result_scan ) . join ( compute_confusion_matrix ( run . run_identifier , result_scan ), on = \"run_identifier\" ) ) curve_results . append ( compute_curves ( run . run_identifier , result_scan )) true_positive_cases . append ( result_scan . filter ( pl . col ( \"true_positive\" )) . select ( [ \"result_file\" , * benchmark_type . columns , pl . col ( \"rank\" ) . alias ( run . run_identifier )] ) . sort ([ \"result_file\" , * benchmark_type . columns ]) ) return ( pl . concat ( stats , how = \"vertical\" ) . collect (), pl . concat ( curve_results , how = \"vertical\" ) . collect (), pl . concat ( [ true_positive_cases [ 0 ]] + [ df . select ( [ col for col in df . collect_schema () . keys () if col not in [ \"result_file\" , * benchmark_type . columns ] ] ) for df in true_positive_cases [ 1 :] ], how = \"horizontal\" , ) . collect (), ) scan_directory ( run , benchmark_type ) Scan a results directory containing pheval parquet standardised results and return a LazyFrame object. Args: run (RunConfig): RunConfig object. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: pl.LazyFrame: LazyFrame object containing all the results in the directory. Source code in src/pheval/analyse/benchmark.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def scan_directory ( run : RunConfig , benchmark_type : BenchmarkOutputType ) -> pl . LazyFrame : \"\"\" Scan a results directory containing pheval parquet standardised results and return a LazyFrame object. Args: run (RunConfig): RunConfig object. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: pl.LazyFrame: LazyFrame object containing all the results in the directory. \"\"\" logger = get_logger () logger . info ( f \"Analysing results in { run . results_dir . joinpath ( benchmark_type . result_directory ) } \" ) return ( ( pl . scan_parquet ( run . results_dir . joinpath ( benchmark_type . result_directory ), include_file_paths = \"file_path\" , ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 ), pl . col ( \"file_path\" ) . str . extract ( r \"([^/ \\\\ ]+)$\" ) . alias ( \"result_file\" ), pl . col ( \"true_positive\" ) . fill_null ( False ), ) ) . filter ( ( pl . col ( \"score\" ) >= run . threshold if run . score_order . lower () == \"descending\" else pl . col ( \"score\" ) <= run . threshold ) if run . threshold is not None else True ) . sort ( \"rank\" ) . unique ( subset = [ \"file_path\" , * benchmark_type . columns ], keep = \"first\" ) )","title":"Benchmark"},{"location":"api/pheval/analyse/benchmark/#src.pheval.analyse.benchmark.benchmark","text":"Benchmark results for specified runs for a specified prioritisation type for comparison. Args: config (Config): Configuration for benchmarking. benchmark_type (BenchmarkOutputType): Benchmark output type. Source code in src/pheval/analyse/benchmark.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def benchmark ( config : Config , benchmark_type : BenchmarkOutputType ) -> None : \"\"\" Benchmark results for specified runs for a specified prioritisation type for comparison. Args: config (Config): Configuration for benchmarking. benchmark_type (BenchmarkOutputType): Benchmark output type. \"\"\" conn = duckdb . connect ( f \" { config . benchmark_name } .duckdb\" ) stats , curve_results , true_positive_cases = process_stats ( config . runs , benchmark_type ) write_table ( conn , stats , f \" { config . benchmark_name } _ { benchmark_type . prioritisation_type_string } _summary\" ) write_table ( conn , curve_results , f \" { config . benchmark_name } _ { benchmark_type . prioritisation_type_string } _binary_classification_curves\" , ) calculate_rank_changes ( conn , [ run . run_identifier for run in config . runs ], true_positive_cases , benchmark_type ) generate_plots ( config . benchmark_name , stats , curve_results , benchmark_type , config . plot_customisation ) conn . close ()","title":"benchmark"},{"location":"api/pheval/analyse/benchmark/#src.pheval.analyse.benchmark.benchmark_runs","text":"Benchmark results for specified runs for comparison. Args: benchmark_config_file (Path): Path to benchmark config file. Source code in src/pheval/analyse/benchmark.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def benchmark_runs ( benchmark_config_file : Path ) -> None : \"\"\" Benchmark results for specified runs for comparison. Args: benchmark_config_file (Path): Path to benchmark config file. \"\"\" logger = get_logger () start_time = time . perf_counter () logger . info ( \"Initiated benchmarking process.\" ) config = parse_run_config ( benchmark_config_file ) if Path ( f \" { config . benchmark_name } .duckdb\" ) . exists (): logger . error ( f \" { config . benchmark_name } .duckdb already exists! Exiting.\" ) sys . exit ( 1 ) gene_analysis_runs = [ run for run in config . runs if run . gene_analysis ] variant_analysis_runs = [ run for run in config . runs if run . variant_analysis ] disease_analysis_runs = [ run for run in config . runs if run . disease_analysis ] if gene_analysis_runs : logger . info ( \"Initiating benchmarking for gene results.\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = gene_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . GENE . value , ) logger . info ( \"Finished benchmarking for gene results.\" ) if variant_analysis_runs : logger . info ( \"Initiating benchmarking for variant results\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = variant_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . VARIANT . value , ) logger . info ( \"Finished benchmarking for variant results.\" ) if disease_analysis_runs : logger . info ( \"Initiating benchmarking for disease results\" ) benchmark ( Config ( benchmark_name = config . benchmark_name , runs = disease_analysis_runs , plot_customisation = config . plot_customisation , ), BenchmarkOutputTypeEnum . DISEASE . value , ) logger . info ( \"Finished benchmarking for disease results.\" ) logger . info ( f \"Finished benchmarking! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"benchmark_runs"},{"location":"api/pheval/analyse/benchmark/#src.pheval.analyse.benchmark.process_stats","text":"Processes stats outputs for specified runs to compare. Args: runs (List[RunConfig]): List of runs to benchmark. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, pl.DataFrame]: The stats for all runs. Source code in src/pheval/analyse/benchmark.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def process_stats ( runs : List [ RunConfig ], benchmark_type : BenchmarkOutputType ) -> Tuple [ pl . DataFrame , pl . DataFrame , pl . DataFrame ]: \"\"\" Processes stats outputs for specified runs to compare. Args: runs (List[RunConfig]): List of runs to benchmark. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, pl.DataFrame]: The stats for all runs. \"\"\" stats , curve_results , true_positive_cases = [], [], [] for run in runs : result_scan = scan_directory ( run , benchmark_type ) stats . append ( compute_rank_stats ( run . run_identifier , result_scan ) . join ( compute_confusion_matrix ( run . run_identifier , result_scan ), on = \"run_identifier\" ) ) curve_results . append ( compute_curves ( run . run_identifier , result_scan )) true_positive_cases . append ( result_scan . filter ( pl . col ( \"true_positive\" )) . select ( [ \"result_file\" , * benchmark_type . columns , pl . col ( \"rank\" ) . alias ( run . run_identifier )] ) . sort ([ \"result_file\" , * benchmark_type . columns ]) ) return ( pl . concat ( stats , how = \"vertical\" ) . collect (), pl . concat ( curve_results , how = \"vertical\" ) . collect (), pl . concat ( [ true_positive_cases [ 0 ]] + [ df . select ( [ col for col in df . collect_schema () . keys () if col not in [ \"result_file\" , * benchmark_type . columns ] ] ) for df in true_positive_cases [ 1 :] ], how = \"horizontal\" , ) . collect (), )","title":"process_stats"},{"location":"api/pheval/analyse/benchmark/#src.pheval.analyse.benchmark.scan_directory","text":"Scan a results directory containing pheval parquet standardised results and return a LazyFrame object. Args: run (RunConfig): RunConfig object. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: pl.LazyFrame: LazyFrame object containing all the results in the directory. Source code in src/pheval/analyse/benchmark.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def scan_directory ( run : RunConfig , benchmark_type : BenchmarkOutputType ) -> pl . LazyFrame : \"\"\" Scan a results directory containing pheval parquet standardised results and return a LazyFrame object. Args: run (RunConfig): RunConfig object. benchmark_type (BenchmarkOutputTypeEnum): Benchmark output type. Returns: pl.LazyFrame: LazyFrame object containing all the results in the directory. \"\"\" logger = get_logger () logger . info ( f \"Analysing results in { run . results_dir . joinpath ( benchmark_type . result_directory ) } \" ) return ( ( pl . scan_parquet ( run . results_dir . joinpath ( benchmark_type . result_directory ), include_file_paths = \"file_path\" , ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 ), pl . col ( \"file_path\" ) . str . extract ( r \"([^/ \\\\ ]+)$\" ) . alias ( \"result_file\" ), pl . col ( \"true_positive\" ) . fill_null ( False ), ) ) . filter ( ( pl . col ( \"score\" ) >= run . threshold if run . score_order . lower () == \"descending\" else pl . col ( \"score\" ) <= run . threshold ) if run . threshold is not None else True ) . sort ( \"rank\" ) . unique ( subset = [ \"file_path\" , * benchmark_type . columns ], keep = \"first\" ) )","title":"scan_directory"},{"location":"api/pheval/analyse/benchmark_db_manager/","text":"write_table ( conn , df , table_name ) Write table to DuckDB database. Args: conn (DuckDBPyConnection): DuckDB connection. df (pl.DataFrame): DuckDB dataframe. table_name (str): Table name. Source code in src/pheval/analyse/benchmark_db_manager.py 14 15 16 17 18 19 20 21 22 23 def write_table ( conn : DuckDBPyConnection , df : pl . DataFrame , table_name : str ) -> None : \"\"\" Write table to DuckDB database. Args: conn (DuckDBPyConnection): DuckDB connection. df (pl.DataFrame): DuckDB dataframe. table_name (str): Table name. \"\"\" logger . info ( f \"Storing results in { table_name } .\" ) conn . execute ( f \"\"\"CREATE TABLE \" { table_name } \" AS SELECT * FROM df\"\"\" )","title":"Benchmark db manager"},{"location":"api/pheval/analyse/benchmark_db_manager/#src.pheval.analyse.benchmark_db_manager.write_table","text":"Write table to DuckDB database. Args: conn (DuckDBPyConnection): DuckDB connection. df (pl.DataFrame): DuckDB dataframe. table_name (str): Table name. Source code in src/pheval/analyse/benchmark_db_manager.py 14 15 16 17 18 19 20 21 22 23 def write_table ( conn : DuckDBPyConnection , df : pl . DataFrame , table_name : str ) -> None : \"\"\" Write table to DuckDB database. Args: conn (DuckDBPyConnection): DuckDB connection. df (pl.DataFrame): DuckDB dataframe. table_name (str): Table name. \"\"\" logger . info ( f \"Storing results in { table_name } .\" ) conn . execute ( f \"\"\"CREATE TABLE \" { table_name } \" AS SELECT * FROM df\"\"\" )","title":"write_table"},{"location":"api/pheval/analyse/benchmark_output_type/","text":"BenchmarkOutputType Bases: NamedTuple Represents the structure of benchmark output types. Attributes: Name Type Description prioritisation_type_string str The type of prioritisation being performed. y_label str The label for the y-axis in performance evaluation plots. columns List [ str ] The list of column names relevant to the benchmark output. result_directory str The directory where benchmark results are stored. Source code in src/pheval/analyse/benchmark_output_type.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class BenchmarkOutputType ( NamedTuple ): \"\"\" Represents the structure of benchmark output types. Attributes: prioritisation_type_string (str): The type of prioritisation being performed. y_label (str): The label for the y-axis in performance evaluation plots. columns (List[str]): The list of column names relevant to the benchmark output. result_directory (str): The directory where benchmark results are stored. \"\"\" prioritisation_type_string : str y_label : str columns : List [ str ] result_directory : str BenchmarkOutputTypeEnum Bases: Enum Enumeration of benchmark output types, representing different entities. Attributes: Name Type Description GENE BenchmarkOutputType Benchmark output type for gene prioritisation. VARIANT BenchmarkOutputType Benchmark output type for variant prioritisation. DISEASE BenchmarkOutputType Benchmark output type for disease prioritisation. Source code in src/pheval/analyse/benchmark_output_type.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BenchmarkOutputTypeEnum ( Enum ): \"\"\" Enumeration of benchmark output types, representing different entities. Attributes: GENE (BenchmarkOutputType): Benchmark output type for gene prioritisation. VARIANT (BenchmarkOutputType): Benchmark output type for variant prioritisation. DISEASE (BenchmarkOutputType): Benchmark output type for disease prioritisation. \"\"\" GENE = BenchmarkOutputType ( \"gene\" , \"Disease-causing genes (%)\" , [ \"gene_identifier\" , \"gene_symbol\" ], \"pheval_gene_results\" , ) VARIANT = BenchmarkOutputType ( \"variant\" , \"Disease-causing variants (%)\" , [ \"variant_id\" ], \"pheval_variant_results\" ) DISEASE = BenchmarkOutputType ( \"disease\" , \"Known diseases (%)\" , [ \"disease_identifier\" , \"mondo_identifier\" ], \"pheval_disease_results\" , )","title":"Benchmark output type"},{"location":"api/pheval/analyse/benchmark_output_type/#src.pheval.analyse.benchmark_output_type.BenchmarkOutputType","text":"Bases: NamedTuple Represents the structure of benchmark output types. Attributes: Name Type Description prioritisation_type_string str The type of prioritisation being performed. y_label str The label for the y-axis in performance evaluation plots. columns List [ str ] The list of column names relevant to the benchmark output. result_directory str The directory where benchmark results are stored. Source code in src/pheval/analyse/benchmark_output_type.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class BenchmarkOutputType ( NamedTuple ): \"\"\" Represents the structure of benchmark output types. Attributes: prioritisation_type_string (str): The type of prioritisation being performed. y_label (str): The label for the y-axis in performance evaluation plots. columns (List[str]): The list of column names relevant to the benchmark output. result_directory (str): The directory where benchmark results are stored. \"\"\" prioritisation_type_string : str y_label : str columns : List [ str ] result_directory : str","title":"BenchmarkOutputType"},{"location":"api/pheval/analyse/benchmark_output_type/#src.pheval.analyse.benchmark_output_type.BenchmarkOutputTypeEnum","text":"Bases: Enum Enumeration of benchmark output types, representing different entities. Attributes: Name Type Description GENE BenchmarkOutputType Benchmark output type for gene prioritisation. VARIANT BenchmarkOutputType Benchmark output type for variant prioritisation. DISEASE BenchmarkOutputType Benchmark output type for disease prioritisation. Source code in src/pheval/analyse/benchmark_output_type.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BenchmarkOutputTypeEnum ( Enum ): \"\"\" Enumeration of benchmark output types, representing different entities. Attributes: GENE (BenchmarkOutputType): Benchmark output type for gene prioritisation. VARIANT (BenchmarkOutputType): Benchmark output type for variant prioritisation. DISEASE (BenchmarkOutputType): Benchmark output type for disease prioritisation. \"\"\" GENE = BenchmarkOutputType ( \"gene\" , \"Disease-causing genes (%)\" , [ \"gene_identifier\" , \"gene_symbol\" ], \"pheval_gene_results\" , ) VARIANT = BenchmarkOutputType ( \"variant\" , \"Disease-causing variants (%)\" , [ \"variant_id\" ], \"pheval_variant_results\" ) DISEASE = BenchmarkOutputType ( \"disease\" , \"Known diseases (%)\" , [ \"disease_identifier\" , \"mondo_identifier\" ], \"pheval_disease_results\" , )","title":"BenchmarkOutputTypeEnum"},{"location":"api/pheval/analyse/binary_classification_curves/","text":"BinaryClassificationCurves Class for computing and storing ROC & Precision-Recall curves in Polars. Source code in src/pheval/analyse/binary_classification_curves.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class BinaryClassificationCurves : \"\"\"Class for computing and storing ROC & Precision-Recall curves in Polars.\"\"\" @staticmethod def _compute_finite_bounds ( result_scan : pl . LazyFrame ) -> Tuple [ float , float ]: \"\"\" Compute min and max finite values in the 'score' column to handle NaN and Inf values. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. Returns: Tuple[float, float]: The (max_finite, min_finite) values for normalising scores. \"\"\" return ( result_scan . select ( [ pl . col ( \"score\" ) . filter ( pl . col ( \"score\" ) . is_finite ()) . max () . alias ( \"max_finite\" ), pl . col ( \"score\" ) . filter ( pl . col ( \"score\" ) . is_finite ()) . min () . alias ( \"min_finite\" ), ] ) . collect () . row ( 0 ) ) @staticmethod def _clean_and_extract_data ( result_scan : pl . LazyFrame , max_finite : float , min_finite : float ) -> pl . LazyFrame : \"\"\" Normalise the 'score' column (handling NaNs and Inf values) and extract 'true_positive' labels. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. max_finite (float): The maximum finite score value. min_finite (float): The minimum finite score value. Returns: pl.LazyFrame: A LazyFrame with cleaned 'score' and binary 'true_positive' columns. \"\"\" return result_scan . with_columns ( [ pl . when ( pl . col ( \"score\" ) . is_nan ()) . then ( 0.0 ) . when ( pl . col ( \"score\" ) . is_infinite () & ( pl . col ( \"score\" ) > 0 )) . then ( max_finite ) . when ( pl . col ( \"score\" ) . is_infinite () & ( pl . col ( \"score\" ) < 0 )) . then ( min_finite ) . otherwise ( pl . col ( \"score\" )) . alias ( \"score\" ), pl . when ( pl . col ( \"true_positive\" ) . is_null ()) . then ( 0 ) . otherwise ( pl . col ( \"true_positive\" ) . cast ( pl . Int8 )) . alias ( \"true_positive\" ), ] ) @staticmethod def _compute_roc_pr_curves ( run_identifier : str , labels : np . ndarray , scores : np . ndarray ) -> pl . LazyFrame : \"\"\" Compute ROC and Precision-Recall curves. Args: labels (np.ndarray): Binary ground truth labels (0 or 1). scores (np.ndarray): Prediction scores. Returns: pl.LazyFrame: A LazyFrame containing the computed FPR, TPR, Precision, Recall, and Thresholds. \"\"\" fpr , tpr , roc_thresholds = roc_curve ( labels , scores , pos_label = 1 ) precision , recall , pr_thresholds = precision_recall_curve ( labels , scores , pos_label = 1 ) return pl . LazyFrame ( { \"run_identifier\" : [ run_identifier ], \"fpr\" : [ fpr . tolist ()], \"tpr\" : [ tpr . tolist ()], \"threshold_roc\" : [ roc_thresholds . tolist ()], \"precision\" : [ precision . tolist ()], \"recall\" : [ recall . tolist ()], \"threshold_pr\" : [ pr_thresholds . tolist ()], } ) @classmethod def process ( cls , result_scan : pl . LazyFrame , run_identifier : str ) -> pl . LazyFrame : \"\"\" Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. \"\"\" max_finite , min_finite = cls . _compute_finite_bounds ( result_scan ) cleaned_data = ( cls . _clean_and_extract_data ( result_scan , max_finite , min_finite ) . select ([ \"true_positive\" , \"score\" ]) . collect () ) return cls . _compute_roc_pr_curves ( run_identifier , cleaned_data [ \"true_positive\" ] . to_numpy () . flatten (), cleaned_data [ \"score\" ] . to_numpy () . flatten (), ) process ( result_scan , run_identifier ) classmethod Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Parameters: Name Type Description Default result_scan LazyFrame The LazyFrame containing the results for the directory. required run_identifier str Identifier for this run. required Returns: Type Description LazyFrame pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. Source code in src/pheval/analyse/binary_classification_curves.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @classmethod def process ( cls , result_scan : pl . LazyFrame , run_identifier : str ) -> pl . LazyFrame : \"\"\" Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. \"\"\" max_finite , min_finite = cls . _compute_finite_bounds ( result_scan ) cleaned_data = ( cls . _clean_and_extract_data ( result_scan , max_finite , min_finite ) . select ([ \"true_positive\" , \"score\" ]) . collect () ) return cls . _compute_roc_pr_curves ( run_identifier , cleaned_data [ \"true_positive\" ] . to_numpy () . flatten (), cleaned_data [ \"score\" ] . to_numpy () . flatten (), ) compute_curves ( run_identifier , result_scan ) Compute ROC and Precision-Recall curves. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: LazyFrame containing the ROC & Precision-Recall curve data with NumPy arrays. Source code in src/pheval/analyse/binary_classification_curves.py 121 122 123 124 125 126 127 128 129 130 131 132 def compute_curves ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Compute ROC and Precision-Recall curves. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: LazyFrame containing the ROC & Precision-Recall curve data with NumPy arrays. \"\"\" logger = get_logger () logger . info ( \"Calculating ROC and Precision-Recall metrics\" ) return BinaryClassificationCurves . process ( result_scan , run_identifier )","title":"Binary classification curves"},{"location":"api/pheval/analyse/binary_classification_curves/#src.pheval.analyse.binary_classification_curves.BinaryClassificationCurves","text":"Class for computing and storing ROC & Precision-Recall curves in Polars. Source code in src/pheval/analyse/binary_classification_curves.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class BinaryClassificationCurves : \"\"\"Class for computing and storing ROC & Precision-Recall curves in Polars.\"\"\" @staticmethod def _compute_finite_bounds ( result_scan : pl . LazyFrame ) -> Tuple [ float , float ]: \"\"\" Compute min and max finite values in the 'score' column to handle NaN and Inf values. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. Returns: Tuple[float, float]: The (max_finite, min_finite) values for normalising scores. \"\"\" return ( result_scan . select ( [ pl . col ( \"score\" ) . filter ( pl . col ( \"score\" ) . is_finite ()) . max () . alias ( \"max_finite\" ), pl . col ( \"score\" ) . filter ( pl . col ( \"score\" ) . is_finite ()) . min () . alias ( \"min_finite\" ), ] ) . collect () . row ( 0 ) ) @staticmethod def _clean_and_extract_data ( result_scan : pl . LazyFrame , max_finite : float , min_finite : float ) -> pl . LazyFrame : \"\"\" Normalise the 'score' column (handling NaNs and Inf values) and extract 'true_positive' labels. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. max_finite (float): The maximum finite score value. min_finite (float): The minimum finite score value. Returns: pl.LazyFrame: A LazyFrame with cleaned 'score' and binary 'true_positive' columns. \"\"\" return result_scan . with_columns ( [ pl . when ( pl . col ( \"score\" ) . is_nan ()) . then ( 0.0 ) . when ( pl . col ( \"score\" ) . is_infinite () & ( pl . col ( \"score\" ) > 0 )) . then ( max_finite ) . when ( pl . col ( \"score\" ) . is_infinite () & ( pl . col ( \"score\" ) < 0 )) . then ( min_finite ) . otherwise ( pl . col ( \"score\" )) . alias ( \"score\" ), pl . when ( pl . col ( \"true_positive\" ) . is_null ()) . then ( 0 ) . otherwise ( pl . col ( \"true_positive\" ) . cast ( pl . Int8 )) . alias ( \"true_positive\" ), ] ) @staticmethod def _compute_roc_pr_curves ( run_identifier : str , labels : np . ndarray , scores : np . ndarray ) -> pl . LazyFrame : \"\"\" Compute ROC and Precision-Recall curves. Args: labels (np.ndarray): Binary ground truth labels (0 or 1). scores (np.ndarray): Prediction scores. Returns: pl.LazyFrame: A LazyFrame containing the computed FPR, TPR, Precision, Recall, and Thresholds. \"\"\" fpr , tpr , roc_thresholds = roc_curve ( labels , scores , pos_label = 1 ) precision , recall , pr_thresholds = precision_recall_curve ( labels , scores , pos_label = 1 ) return pl . LazyFrame ( { \"run_identifier\" : [ run_identifier ], \"fpr\" : [ fpr . tolist ()], \"tpr\" : [ tpr . tolist ()], \"threshold_roc\" : [ roc_thresholds . tolist ()], \"precision\" : [ precision . tolist ()], \"recall\" : [ recall . tolist ()], \"threshold_pr\" : [ pr_thresholds . tolist ()], } ) @classmethod def process ( cls , result_scan : pl . LazyFrame , run_identifier : str ) -> pl . LazyFrame : \"\"\" Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. \"\"\" max_finite , min_finite = cls . _compute_finite_bounds ( result_scan ) cleaned_data = ( cls . _clean_and_extract_data ( result_scan , max_finite , min_finite ) . select ([ \"true_positive\" , \"score\" ]) . collect () ) return cls . _compute_roc_pr_curves ( run_identifier , cleaned_data [ \"true_positive\" ] . to_numpy () . flatten (), cleaned_data [ \"score\" ] . to_numpy () . flatten (), )","title":"BinaryClassificationCurves"},{"location":"api/pheval/analyse/binary_classification_curves/#src.pheval.analyse.binary_classification_curves.BinaryClassificationCurves.process","text":"Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Parameters: Name Type Description Default result_scan LazyFrame The LazyFrame containing the results for the directory. required run_identifier str Identifier for this run. required Returns: Type Description LazyFrame pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. Source code in src/pheval/analyse/binary_classification_curves.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @classmethod def process ( cls , result_scan : pl . LazyFrame , run_identifier : str ) -> pl . LazyFrame : \"\"\" Process scores, extract true labels, compute ROC and Precision-Recall curves, and store results in a Polars LazyFrame with NumPy arrays. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: A LazyFrame containing ROC & PR curve data with NumPy arrays. \"\"\" max_finite , min_finite = cls . _compute_finite_bounds ( result_scan ) cleaned_data = ( cls . _clean_and_extract_data ( result_scan , max_finite , min_finite ) . select ([ \"true_positive\" , \"score\" ]) . collect () ) return cls . _compute_roc_pr_curves ( run_identifier , cleaned_data [ \"true_positive\" ] . to_numpy () . flatten (), cleaned_data [ \"score\" ] . to_numpy () . flatten (), )","title":"process"},{"location":"api/pheval/analyse/binary_classification_curves/#src.pheval.analyse.binary_classification_curves.compute_curves","text":"Compute ROC and Precision-Recall curves. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: LazyFrame containing the ROC & Precision-Recall curve data with NumPy arrays. Source code in src/pheval/analyse/binary_classification_curves.py 121 122 123 124 125 126 127 128 129 130 131 132 def compute_curves ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Compute ROC and Precision-Recall curves. Args: result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. run_identifier (str): Identifier for this run. Returns: pl.LazyFrame: LazyFrame containing the ROC & Precision-Recall curve data with NumPy arrays. \"\"\" logger = get_logger () logger . info ( \"Calculating ROC and Precision-Recall metrics\" ) return BinaryClassificationCurves . process ( result_scan , run_identifier )","title":"compute_curves"},{"location":"api/pheval/analyse/binary_classification_stats/","text":"BinaryClassificationStats dataclass Binary classification statistic expressions. Source code in src/pheval/analyse/binary_classification_stats.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @dataclass ( frozen = True ) class BinaryClassificationStats : \"\"\"Binary classification statistic expressions.\"\"\" SENSITIVITY = ( pl . when (( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( pl . col ( \"true_positives\" ) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"sensitivity\" ) ) SPECIFICITY = ( pl . when (( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) != 0 ) . then ( pl . col ( \"true_negatives\" ) / ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" ))) . otherwise ( 0.0 ) . alias ( \"specificity\" ) ) PRECISION = ( pl . when (( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) != 0 ) . then ( pl . col ( \"true_positives\" ) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ))) . otherwise ( 0.0 ) . alias ( \"precision\" ) ) NEGATIVE_PREDICTIVE_VALUE = ( pl . when (( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( pl . col ( \"true_negatives\" ) / ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"negative_predictive_value\" ) ) FALSE_POSITIVE_RATE = ( pl . when (( pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" )) != 0 ) . then ( pl . col ( \"false_positives\" ) / ( pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"false_positive_rate\" ) ) FALSE_DISCOVERY_RATE = ( pl . when (( pl . col ( \"false_positives\" ) + pl . col ( \"true_positives\" )) != 0 ) . then ( pl . col ( \"false_positives\" ) / ( pl . col ( \"false_positives\" ) + pl . col ( \"true_positives\" ))) . otherwise ( 0.0 ) . alias ( \"false_discovery_rate\" ) ) FALSE_NEGATIVE_RATE = ( pl . when (( pl . col ( \"false_negatives\" ) + pl . col ( \"true_positives\" )) != 0 ) . then ( pl . col ( \"false_negatives\" ) / ( pl . col ( \"false_negatives\" ) + pl . col ( \"true_positives\" ))) . otherwise ( 0.0 ) . alias ( \"false_negative_rate\" ) ) ACCURACY = ( pl . when ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ) ) != 0 ) . then ( ( pl . col ( \"true_positives\" ) + pl . col ( \"true_negatives\" )) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ) ) ) . otherwise ( 0.0 ) . alias ( \"accuracy\" ) ) F1_SCORE = ( pl . when ( 2 * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( 2 * pl . col ( \"true_positives\" ) / ( 2 * pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"false_negatives\" )) ) . otherwise ( 0.0 ) . alias ( \"f1_score\" ) ) MATTHEWS_CORRELATION_COEFFICIENT = ( pl . when ( ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) ) > 0 ) . then ( ( ( pl . col ( \"true_positives\" ) * pl . col ( \"true_negatives\" )) - ( pl . col ( \"false_positives\" ) * pl . col ( \"false_negatives\" )) ) / ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) ) . sqrt () ) . otherwise ( 0.0 ) . alias ( \"matthews_correlation_coefficient\" ) ) ConfusionMatrix dataclass Define logical conditions for computing a confusion matrix using Polars expressions. Attributes: Name Type Description TRUE_POSITIVES Expr Condition identifying true positive cases, where rank == 1 and true_positive is True . FALSE_POSITIVES Expr Condition identifying false positive cases, where rank == 1 and true_positive is False . TRUE_NEGATIVES Expr Condition identifying true negative cases, where rank != 1 and true_positive is False . FALSE_NEGATIVES Expr Condition identifying false negative cases, where rank != 1 and true_positive is True . Source code in src/pheval/analyse/binary_classification_stats.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @dataclass ( frozen = True ) class ConfusionMatrix : \"\"\" Define logical conditions for computing a confusion matrix using Polars expressions. Attributes: TRUE_POSITIVES (pl.Expr): Condition identifying true positive cases, where `rank == 1` and `true_positive` is `True`. FALSE_POSITIVES (pl.Expr): Condition identifying false positive cases, where `rank == 1` and `true_positive` is `False`. TRUE_NEGATIVES (pl.Expr): Condition identifying true negative cases, where `rank != 1` and `true_positive` is `False`. FALSE_NEGATIVES (pl.Expr): Condition identifying false negative cases, where `rank != 1` and `true_positive` is `True`. \"\"\" TRUE_POSITIVES = ( pl . col ( \"rank\" ) == 1 ) & ( pl . col ( \"true_positive\" )) FALSE_POSITIVES = ( pl . col ( \"rank\" ) == 1 ) & ( ~ pl . col ( \"true_positive\" )) TRUE_NEGATIVES = ( pl . col ( \"rank\" ) != 1 ) & ( ~ pl . col ( \"true_positive\" )) FALSE_NEGATIVES = ( pl . col ( \"rank\" ) != 1 ) & ( pl . col ( \"true_positive\" )) compute_confusion_matrix ( run_identifier , result_scan ) Computes binary classification statistics. Parameters: Name Type Description Default run_identifier str The identifier for the run. required result_scan LazyFrame The LazyFrame containing the results for the directory. required Returns: Type Description LazyFrame pl.LazyFrame: The LazyFrame containing the binary classification statistics. Source code in src/pheval/analyse/binary_classification_stats.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def compute_confusion_matrix ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Computes binary classification statistics. Args: run_identifier (str): The identifier for the run. result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. Returns: pl.LazyFrame: The LazyFrame containing the binary classification statistics. \"\"\" logger = get_logger () logger . info ( f \"Computing binary classification statistics for { run_identifier } \" ) confusion_matrix = result_scan . select ( [ pl . lit ( run_identifier ) . alias ( \"run_identifier\" ), ConfusionMatrix . TRUE_POSITIVES . sum () . alias ( \"true_positives\" ) . cast ( pl . Int64 ), ConfusionMatrix . FALSE_POSITIVES . sum () . alias ( \"false_positives\" ) . cast ( pl . Int64 ), ConfusionMatrix . TRUE_NEGATIVES . sum () . alias ( \"true_negatives\" ) . cast ( pl . Int64 ), ConfusionMatrix . FALSE_NEGATIVES . sum () . alias ( \"false_negatives\" ) . cast ( pl . Int64 ), ] ) return confusion_matrix . select ( [ pl . col ( \"run_identifier\" ), pl . col ( \"true_positives\" ), pl . col ( \"false_positives\" ), pl . col ( \"true_negatives\" ), pl . col ( \"false_negatives\" ), BinaryClassificationStats . SENSITIVITY , BinaryClassificationStats . SPECIFICITY , BinaryClassificationStats . PRECISION , BinaryClassificationStats . NEGATIVE_PREDICTIVE_VALUE , BinaryClassificationStats . FALSE_POSITIVE_RATE , BinaryClassificationStats . FALSE_DISCOVERY_RATE , BinaryClassificationStats . FALSE_NEGATIVE_RATE , BinaryClassificationStats . ACCURACY , BinaryClassificationStats . F1_SCORE , BinaryClassificationStats . MATTHEWS_CORRELATION_COEFFICIENT , ] )","title":"Binary classification stats"},{"location":"api/pheval/analyse/binary_classification_stats/#src.pheval.analyse.binary_classification_stats.BinaryClassificationStats","text":"Binary classification statistic expressions. Source code in src/pheval/analyse/binary_classification_stats.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @dataclass ( frozen = True ) class BinaryClassificationStats : \"\"\"Binary classification statistic expressions.\"\"\" SENSITIVITY = ( pl . when (( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( pl . col ( \"true_positives\" ) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"sensitivity\" ) ) SPECIFICITY = ( pl . when (( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) != 0 ) . then ( pl . col ( \"true_negatives\" ) / ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" ))) . otherwise ( 0.0 ) . alias ( \"specificity\" ) ) PRECISION = ( pl . when (( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) != 0 ) . then ( pl . col ( \"true_positives\" ) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ))) . otherwise ( 0.0 ) . alias ( \"precision\" ) ) NEGATIVE_PREDICTIVE_VALUE = ( pl . when (( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( pl . col ( \"true_negatives\" ) / ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"negative_predictive_value\" ) ) FALSE_POSITIVE_RATE = ( pl . when (( pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" )) != 0 ) . then ( pl . col ( \"false_positives\" ) / ( pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ))) . otherwise ( 0.0 ) . alias ( \"false_positive_rate\" ) ) FALSE_DISCOVERY_RATE = ( pl . when (( pl . col ( \"false_positives\" ) + pl . col ( \"true_positives\" )) != 0 ) . then ( pl . col ( \"false_positives\" ) / ( pl . col ( \"false_positives\" ) + pl . col ( \"true_positives\" ))) . otherwise ( 0.0 ) . alias ( \"false_discovery_rate\" ) ) FALSE_NEGATIVE_RATE = ( pl . when (( pl . col ( \"false_negatives\" ) + pl . col ( \"true_positives\" )) != 0 ) . then ( pl . col ( \"false_negatives\" ) / ( pl . col ( \"false_negatives\" ) + pl . col ( \"true_positives\" ))) . otherwise ( 0.0 ) . alias ( \"false_negative_rate\" ) ) ACCURACY = ( pl . when ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ) ) != 0 ) . then ( ( pl . col ( \"true_positives\" ) + pl . col ( \"true_negatives\" )) / ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" ) ) ) . otherwise ( 0.0 ) . alias ( \"accuracy\" ) ) F1_SCORE = ( pl . when ( 2 * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"false_negatives\" )) != 0 ) . then ( 2 * pl . col ( \"true_positives\" ) / ( 2 * pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" ) + pl . col ( \"false_negatives\" )) ) . otherwise ( 0.0 ) . alias ( \"f1_score\" ) ) MATTHEWS_CORRELATION_COEFFICIENT = ( pl . when ( ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) ) > 0 ) . then ( ( ( pl . col ( \"true_positives\" ) * pl . col ( \"true_negatives\" )) - ( pl . col ( \"false_positives\" ) * pl . col ( \"false_negatives\" )) ) / ( ( pl . col ( \"true_positives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_positives\" ) + pl . col ( \"false_negatives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_positives\" )) * ( pl . col ( \"true_negatives\" ) + pl . col ( \"false_negatives\" )) ) . sqrt () ) . otherwise ( 0.0 ) . alias ( \"matthews_correlation_coefficient\" ) )","title":"BinaryClassificationStats"},{"location":"api/pheval/analyse/binary_classification_stats/#src.pheval.analyse.binary_classification_stats.ConfusionMatrix","text":"Define logical conditions for computing a confusion matrix using Polars expressions. Attributes: Name Type Description TRUE_POSITIVES Expr Condition identifying true positive cases, where rank == 1 and true_positive is True . FALSE_POSITIVES Expr Condition identifying false positive cases, where rank == 1 and true_positive is False . TRUE_NEGATIVES Expr Condition identifying true negative cases, where rank != 1 and true_positive is False . FALSE_NEGATIVES Expr Condition identifying false negative cases, where rank != 1 and true_positive is True . Source code in src/pheval/analyse/binary_classification_stats.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @dataclass ( frozen = True ) class ConfusionMatrix : \"\"\" Define logical conditions for computing a confusion matrix using Polars expressions. Attributes: TRUE_POSITIVES (pl.Expr): Condition identifying true positive cases, where `rank == 1` and `true_positive` is `True`. FALSE_POSITIVES (pl.Expr): Condition identifying false positive cases, where `rank == 1` and `true_positive` is `False`. TRUE_NEGATIVES (pl.Expr): Condition identifying true negative cases, where `rank != 1` and `true_positive` is `False`. FALSE_NEGATIVES (pl.Expr): Condition identifying false negative cases, where `rank != 1` and `true_positive` is `True`. \"\"\" TRUE_POSITIVES = ( pl . col ( \"rank\" ) == 1 ) & ( pl . col ( \"true_positive\" )) FALSE_POSITIVES = ( pl . col ( \"rank\" ) == 1 ) & ( ~ pl . col ( \"true_positive\" )) TRUE_NEGATIVES = ( pl . col ( \"rank\" ) != 1 ) & ( ~ pl . col ( \"true_positive\" )) FALSE_NEGATIVES = ( pl . col ( \"rank\" ) != 1 ) & ( pl . col ( \"true_positive\" ))","title":"ConfusionMatrix"},{"location":"api/pheval/analyse/binary_classification_stats/#src.pheval.analyse.binary_classification_stats.compute_confusion_matrix","text":"Computes binary classification statistics. Parameters: Name Type Description Default run_identifier str The identifier for the run. required result_scan LazyFrame The LazyFrame containing the results for the directory. required Returns: Type Description LazyFrame pl.LazyFrame: The LazyFrame containing the binary classification statistics. Source code in src/pheval/analyse/binary_classification_stats.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def compute_confusion_matrix ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Computes binary classification statistics. Args: run_identifier (str): The identifier for the run. result_scan (pl.LazyFrame): The LazyFrame containing the results for the directory. Returns: pl.LazyFrame: The LazyFrame containing the binary classification statistics. \"\"\" logger = get_logger () logger . info ( f \"Computing binary classification statistics for { run_identifier } \" ) confusion_matrix = result_scan . select ( [ pl . lit ( run_identifier ) . alias ( \"run_identifier\" ), ConfusionMatrix . TRUE_POSITIVES . sum () . alias ( \"true_positives\" ) . cast ( pl . Int64 ), ConfusionMatrix . FALSE_POSITIVES . sum () . alias ( \"false_positives\" ) . cast ( pl . Int64 ), ConfusionMatrix . TRUE_NEGATIVES . sum () . alias ( \"true_negatives\" ) . cast ( pl . Int64 ), ConfusionMatrix . FALSE_NEGATIVES . sum () . alias ( \"false_negatives\" ) . cast ( pl . Int64 ), ] ) return confusion_matrix . select ( [ pl . col ( \"run_identifier\" ), pl . col ( \"true_positives\" ), pl . col ( \"false_positives\" ), pl . col ( \"true_negatives\" ), pl . col ( \"false_negatives\" ), BinaryClassificationStats . SENSITIVITY , BinaryClassificationStats . SPECIFICITY , BinaryClassificationStats . PRECISION , BinaryClassificationStats . NEGATIVE_PREDICTIVE_VALUE , BinaryClassificationStats . FALSE_POSITIVE_RATE , BinaryClassificationStats . FALSE_DISCOVERY_RATE , BinaryClassificationStats . FALSE_NEGATIVE_RATE , BinaryClassificationStats . ACCURACY , BinaryClassificationStats . F1_SCORE , BinaryClassificationStats . MATTHEWS_CORRELATION_COEFFICIENT , ] )","title":"compute_confusion_matrix"},{"location":"api/pheval/analyse/generate_plots/","text":"PlotGenerator Class to generate plots. Source code in src/pheval/analyse/generate_plots.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class PlotGenerator : \"\"\"Class to generate plots.\"\"\" palette_hex_codes = [ \"#f4ae3d\" , \"#ee5825\" , \"#2b7288\" , \"#9a84b2\" , \"#0c604c\" , \"#c94c4c\" , \"#3d8e83\" , \"#725ac1\" , \"#e7ba52\" , \"#1b9e77\" , ] def __init__ ( self , benchmark_name : str ): \"\"\" Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. \"\"\" self . benchmark_name = benchmark_name matplotlib . rcParams [ \"axes.spines.right\" ] = False matplotlib . rcParams [ \"axes.spines.top\" ] = False @staticmethod def _generate_stacked_data ( benchmarking_stats_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate stacked data. Args: benchmarking_stats_df (pl.DataFrame): benchmarking stats dataframe. Returns: pl.DataFrame: Data formatted for plotting stacked data. \"\"\" return benchmarking_stats_df . with_columns ( [ pl . col ( \"run_identifier\" ) . alias ( \"Run\" ), pl . col ( \"percentage@1\" ) . alias ( \"Top\" ), ( pl . col ( \"percentage@3\" ) - pl . col ( \"percentage@1\" )) . alias ( \"2-3\" ), ( pl . col ( \"percentage@5\" ) - pl . col ( \"percentage@3\" )) . alias ( \"4-5\" ), ( pl . col ( \"percentage@10\" ) - pl . col ( \"percentage@5\" )) . alias ( \"6-10\" ), ( pl . col ( \"percentage_found\" ) - pl . col ( \"percentage@10\" )) . alias ( \">10\" ), ( 100 - pl . col ( \"percentage_found\" )) . alias ( \"Missed\" ), ] ) . select ([ \"Run\" , \"Top\" , \"2-3\" , \"4-5\" , \"6-10\" , \">10\" , \"Missed\" ]) @staticmethod def _extract_mrr_data ( benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for MRR (Mean Reciprocal Rank) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. Returns: pl.DataFrame: Data formatted for plotting MRR bar plot. \"\"\" return benchmarking_results_df . select ([ \"run_identifier\" , \"mrr\" ]) . rename ( { \"run_identifier\" : \"Run\" , \"mrr\" : \"Percentage\" } ) def _save_fig ( self , benchmark_output_type : BenchmarkOutputType , y_lower_limit : int , y_upper_limit : int ) -> None : \"\"\" Save the generated figure. Args: benchmark_output_type (BenchmarkOutputType): Benchmark output type. y_lower_limit (int): Lower limit for the y-axis. y_upper_limit (int): Upper limit for the y-axis. \"\"\" plt . ylim ( y_lower_limit , y_upper_limit ) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_output_type . prioritisation_type_string } _rank_stats.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) def generate_stacked_bar_plot ( self , benchmarking_results_df : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. \"\"\" plt . clf () stats_df = self . _generate_stacked_data ( benchmarking_results_df ) stats_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , stacked = True , color = self . palette_hex_codes , ylabel = benchmark_output_type . y_label , edgecolor = \"white\" , ) . legend ( loc = \"center left\" , bbox_to_anchor = ( 1.0 , 0.5 )) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 100 ) mrr_df = self . _extract_mrr_data ( benchmarking_results_df ) mrr_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , color = self . palette_hex_codes , ylabel = f \" { benchmark_output_type . prioritisation_type_string . capitalize () } mean reciprocal rank\" , legend = False , edgecolor = \"white\" , ) plt . title ( f \" { benchmark_output_type . prioritisation_type_string . capitalize () } results - mean reciprocal rank\" ) self . _save_fig ( benchmark_output_type , 0 , 1 ) @staticmethod def _generate_cumulative_bar_plot_data ( benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for a cumulative bar plot, appending to the self.stats attribute of the class. \"\"\" return benchmarking_results_df . select ( [ pl . col ( \"run_identifier\" ) . alias ( \"Run\" ), pl . col ( \"percentage@1\" ) . alias ( \"Top\" ) / 100 , pl . col ( \"percentage@3\" ) . alias ( \"Top3\" ) / 100 , pl . col ( \"percentage@5\" ) . alias ( \"Top5\" ) / 100 , pl . col ( \"percentage@10\" ) . alias ( \"Top10\" ) / 100 , pl . col ( \"percentage_found\" ) . alias ( \"Found\" ) / 100 , pl . col ( \"mrr\" ) . alias ( \"MRR\" ), ] ) def _plot_bar_plot ( self , benchmark_output_type : BenchmarkOutputType , stats_df : pl . DataFrame , plot_customisation : SinglePlotCustomisation , ) -> None : stats_df = stats_df . to_pandas () . melt ( id_vars = [ \"Run\" ], value_vars = [ \"Top\" , \"Top3\" , \"Top5\" , \"Top10\" , \"Found\" , \"MRR\" ], var_name = \"Rank\" , value_name = \"Percentage\" , ) sns . catplot ( data = stats_df , kind = \"bar\" , x = \"Rank\" , y = \"Percentage\" , hue = \"Run\" , palette = self . palette_hex_codes , edgecolor = \"white\" , legend = False , ) . set ( xlabel = \"Rank\" , ylabel = benchmark_output_type . y_label ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 ), ncol = 3 , title = \"Run\" ) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 1 ) def _generate_non_cumulative_bar_plot_data ( self , benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for a non-cumulative bar plot, appending to the self.stats attribute of the class. \"\"\" return self . _generate_stacked_data ( benchmarking_results_df ) . hstack ( self . _extract_mrr_data ( benchmarking_results_df ) . select ( pl . col ( \"Percentage\" ) . alias ( \"MRR\" ) ) ) def generate_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) def generate_non_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a non-cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_non_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) def generate_roc_curve ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: \"\"\" plt . clf () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] fpr = row [ \"fpr\" ] tpr = row [ \"tpr\" ] roc_auc = auc ( fpr , tpr ) plt . plot ( fpr , tpr , label = f \" { run_identifier } ROC Curve (AUC = { roc_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate\" ) plt . title ( plot_customisation . roc_curve_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _roc_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) def generate_precision_recall ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Precision-Recall curves for binary classification benchmark results. \"\"\" plt . clf () plt . figure () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] precision = row [ \"precision\" ] recall = row [ \"recall\" ] pr_auc = auc ( recall [:: - 1 ], precision [:: - 1 ]) plt . plot ( recall , precision , label = f \" { run_identifier } Precision-Recall Curve (AUC = { pr_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"Recall\" ) plt . ylabel ( \"Precision\" ) plt . title ( plot_customisation . precision_recall_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _pr_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) __init__ ( benchmark_name ) Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. Source code in src/pheval/analyse/generate_plots.py 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , benchmark_name : str ): \"\"\" Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. \"\"\" self . benchmark_name = benchmark_name matplotlib . rcParams [ \"axes.spines.right\" ] = False matplotlib . rcParams [ \"axes.spines.top\" ] = False generate_cumulative_bar ( benchmarking_results_df , benchmark_generator , plot_customisation ) Generate a cumulative bar plot. Source code in src/pheval/analyse/generate_plots.py 205 206 207 208 209 210 211 212 213 214 215 216 def generate_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) generate_non_cumulative_bar ( benchmarking_results_df , benchmark_generator , plot_customisation ) Generate a non-cumulative bar plot. Source code in src/pheval/analyse/generate_plots.py 218 219 220 221 222 223 224 225 226 227 228 229 def generate_non_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a non-cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_non_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) generate_precision_recall ( curves , benchmark_generator , plot_customisation ) Generate and plot Precision-Recall curves for binary classification benchmark results. Source code in src/pheval/analyse/generate_plots.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def generate_precision_recall ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Precision-Recall curves for binary classification benchmark results. \"\"\" plt . clf () plt . figure () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] precision = row [ \"precision\" ] recall = row [ \"recall\" ] pr_auc = auc ( recall [:: - 1 ], precision [:: - 1 ]) plt . plot ( recall , precision , label = f \" { run_identifier } Precision-Recall Curve (AUC = { pr_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"Recall\" ) plt . ylabel ( \"Precision\" ) plt . title ( plot_customisation . precision_recall_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _pr_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) generate_roc_curve ( curves , benchmark_generator , plot_customisation ) Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: Source code in src/pheval/analyse/generate_plots.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def generate_roc_curve ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: \"\"\" plt . clf () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] fpr = row [ \"fpr\" ] tpr = row [ \"tpr\" ] roc_auc = auc ( fpr , tpr ) plt . plot ( fpr , tpr , label = f \" { run_identifier } ROC Curve (AUC = { roc_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate\" ) plt . title ( plot_customisation . roc_curve_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _roc_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) generate_stacked_bar_plot ( benchmarking_results_df , benchmark_output_type , plot_customisation ) Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. Source code in src/pheval/analyse/generate_plots.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def generate_stacked_bar_plot ( self , benchmarking_results_df : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. \"\"\" plt . clf () stats_df = self . _generate_stacked_data ( benchmarking_results_df ) stats_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , stacked = True , color = self . palette_hex_codes , ylabel = benchmark_output_type . y_label , edgecolor = \"white\" , ) . legend ( loc = \"center left\" , bbox_to_anchor = ( 1.0 , 0.5 )) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 100 ) mrr_df = self . _extract_mrr_data ( benchmarking_results_df ) mrr_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , color = self . palette_hex_codes , ylabel = f \" { benchmark_output_type . prioritisation_type_string . capitalize () } mean reciprocal rank\" , legend = False , edgecolor = \"white\" , ) plt . title ( f \" { benchmark_output_type . prioritisation_type_string . capitalize () } results - mean reciprocal rank\" ) self . _save_fig ( benchmark_output_type , 0 , 1 ) generate_plots ( benchmark_name , benchmarking_results_df , curves , benchmark_output_type , plot_customisation ) Generate summary statistics bar plots for prioritisation. This method generates summary statistics bar plots based on the provided benchmarking results and plot type. Source code in src/pheval/analyse/generate_plots.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def generate_plots ( benchmark_name : str , benchmarking_results_df : pl . DataFrame , curves : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : PlotCustomisation , ) -> None : \"\"\" Generate summary statistics bar plots for prioritisation. This method generates summary statistics bar plots based on the provided benchmarking results and plot type. \"\"\" plot_generator = PlotGenerator ( benchmark_name ) plot_customisation_type = getattr ( plot_customisation , f \" { benchmark_output_type . prioritisation_type_string } _plots\" ) logger . info ( \"Generating ROC curve visualisations.\" ) plot_generator . generate_roc_curve ( curves , benchmark_output_type , plot_customisation_type ) logger . info ( \"Generating Precision-Recall curves visualisations.\" ) plot_generator . generate_precision_recall ( curves , benchmark_output_type , plot_customisation_type ) plot_type = PlotTypes ( plot_customisation_type . plot_type ) match plot_type : case PlotTypes . BAR_STACKED : logger . info ( \"Generating stacked bar plot.\" ) plot_generator . generate_stacked_bar_plot ( benchmarking_results_df , benchmark_output_type , plot_customisation_type ) case PlotTypes . BAR_CUMULATIVE : logger . info ( \"Generating cumulative bar plot.\" ) plot_generator . generate_cumulative_bar ( benchmarking_results_df , benchmark_output_type , plot_customisation_type ) case PlotTypes . BAR_NON_CUMULATIVE : logger . info ( \"Generating non cumulative bar plot.\" ) plot_generator . generate_non_cumulative_bar ( benchmarking_results_df , benchmark_output_type , plot_customisation_type ) generate_plots_from_db ( db_path , config ) Generate plots from database file. Args: db_path (Path): Path to the database file. config (Path): Path to the benchmarking config file. Source code in src/pheval/analyse/generate_plots.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def generate_plots_from_db ( db_path : Path , config : Path ) -> None : \"\"\" Generate plots from database file. Args: db_path (Path): Path to the database file. config (Path): Path to the benchmarking config file. \"\"\" logger . info ( f \"Generating plots from { db_path } \" ) conn = duckdb . connect ( db_path ) logger . info ( f \"Parsing configurations from { config } \" ) benchmark_config_file = parse_run_config ( config ) tables = { row [ 0 ] for row in conn . execute ( \"\"\"SELECT table_name FROM duckdb_tables WHERE table_name \"\"\" \"\"\"LIKE '%_summary%' OR table_name LIKE '%_binary_classification_curves'\"\"\" ) . fetchall () } for benchmark_output_type in BenchmarkOutputTypeEnum : summary_table = ( f \" { benchmark_config_file . benchmark_name } _\" f \" { benchmark_output_type . value . prioritisation_type_string } _summary\" ) curve_table = ( f \" { benchmark_config_file . benchmark_name } _\" f \" { benchmark_output_type . value . prioritisation_type_string } _binary_classification_curves\" ) if summary_table in tables and curve_table in tables : logger . info ( f \"Generating plots for { benchmark_output_type . value . prioritisation_type_string } prioritisation.\" ) benchmarking_results_df = load_table_lazy ( summary_table , conn ) . collect () curves_df = load_table_lazy ( curve_table , conn ) . collect () generate_plots ( benchmark_name = benchmark_config_file . benchmark_name , benchmarking_results_df = benchmarking_results_df , curves = curves_df , benchmark_output_type = benchmark_output_type . value , plot_customisation = benchmark_config_file . plot_customisation , ) logger . info ( \"Finished generating plots.\" ) conn . close ()","title":"Generate plots"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator","text":"Class to generate plots. Source code in src/pheval/analyse/generate_plots.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 class PlotGenerator : \"\"\"Class to generate plots.\"\"\" palette_hex_codes = [ \"#f4ae3d\" , \"#ee5825\" , \"#2b7288\" , \"#9a84b2\" , \"#0c604c\" , \"#c94c4c\" , \"#3d8e83\" , \"#725ac1\" , \"#e7ba52\" , \"#1b9e77\" , ] def __init__ ( self , benchmark_name : str ): \"\"\" Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. \"\"\" self . benchmark_name = benchmark_name matplotlib . rcParams [ \"axes.spines.right\" ] = False matplotlib . rcParams [ \"axes.spines.top\" ] = False @staticmethod def _generate_stacked_data ( benchmarking_stats_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate stacked data. Args: benchmarking_stats_df (pl.DataFrame): benchmarking stats dataframe. Returns: pl.DataFrame: Data formatted for plotting stacked data. \"\"\" return benchmarking_stats_df . with_columns ( [ pl . col ( \"run_identifier\" ) . alias ( \"Run\" ), pl . col ( \"percentage@1\" ) . alias ( \"Top\" ), ( pl . col ( \"percentage@3\" ) - pl . col ( \"percentage@1\" )) . alias ( \"2-3\" ), ( pl . col ( \"percentage@5\" ) - pl . col ( \"percentage@3\" )) . alias ( \"4-5\" ), ( pl . col ( \"percentage@10\" ) - pl . col ( \"percentage@5\" )) . alias ( \"6-10\" ), ( pl . col ( \"percentage_found\" ) - pl . col ( \"percentage@10\" )) . alias ( \">10\" ), ( 100 - pl . col ( \"percentage_found\" )) . alias ( \"Missed\" ), ] ) . select ([ \"Run\" , \"Top\" , \"2-3\" , \"4-5\" , \"6-10\" , \">10\" , \"Missed\" ]) @staticmethod def _extract_mrr_data ( benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for MRR (Mean Reciprocal Rank) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. Returns: pl.DataFrame: Data formatted for plotting MRR bar plot. \"\"\" return benchmarking_results_df . select ([ \"run_identifier\" , \"mrr\" ]) . rename ( { \"run_identifier\" : \"Run\" , \"mrr\" : \"Percentage\" } ) def _save_fig ( self , benchmark_output_type : BenchmarkOutputType , y_lower_limit : int , y_upper_limit : int ) -> None : \"\"\" Save the generated figure. Args: benchmark_output_type (BenchmarkOutputType): Benchmark output type. y_lower_limit (int): Lower limit for the y-axis. y_upper_limit (int): Upper limit for the y-axis. \"\"\" plt . ylim ( y_lower_limit , y_upper_limit ) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_output_type . prioritisation_type_string } _rank_stats.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) def generate_stacked_bar_plot ( self , benchmarking_results_df : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. \"\"\" plt . clf () stats_df = self . _generate_stacked_data ( benchmarking_results_df ) stats_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , stacked = True , color = self . palette_hex_codes , ylabel = benchmark_output_type . y_label , edgecolor = \"white\" , ) . legend ( loc = \"center left\" , bbox_to_anchor = ( 1.0 , 0.5 )) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 100 ) mrr_df = self . _extract_mrr_data ( benchmarking_results_df ) mrr_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , color = self . palette_hex_codes , ylabel = f \" { benchmark_output_type . prioritisation_type_string . capitalize () } mean reciprocal rank\" , legend = False , edgecolor = \"white\" , ) plt . title ( f \" { benchmark_output_type . prioritisation_type_string . capitalize () } results - mean reciprocal rank\" ) self . _save_fig ( benchmark_output_type , 0 , 1 ) @staticmethod def _generate_cumulative_bar_plot_data ( benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for a cumulative bar plot, appending to the self.stats attribute of the class. \"\"\" return benchmarking_results_df . select ( [ pl . col ( \"run_identifier\" ) . alias ( \"Run\" ), pl . col ( \"percentage@1\" ) . alias ( \"Top\" ) / 100 , pl . col ( \"percentage@3\" ) . alias ( \"Top3\" ) / 100 , pl . col ( \"percentage@5\" ) . alias ( \"Top5\" ) / 100 , pl . col ( \"percentage@10\" ) . alias ( \"Top10\" ) / 100 , pl . col ( \"percentage_found\" ) . alias ( \"Found\" ) / 100 , pl . col ( \"mrr\" ) . alias ( \"MRR\" ), ] ) def _plot_bar_plot ( self , benchmark_output_type : BenchmarkOutputType , stats_df : pl . DataFrame , plot_customisation : SinglePlotCustomisation , ) -> None : stats_df = stats_df . to_pandas () . melt ( id_vars = [ \"Run\" ], value_vars = [ \"Top\" , \"Top3\" , \"Top5\" , \"Top10\" , \"Found\" , \"MRR\" ], var_name = \"Rank\" , value_name = \"Percentage\" , ) sns . catplot ( data = stats_df , kind = \"bar\" , x = \"Rank\" , y = \"Percentage\" , hue = \"Run\" , palette = self . palette_hex_codes , edgecolor = \"white\" , legend = False , ) . set ( xlabel = \"Rank\" , ylabel = benchmark_output_type . y_label ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 ), ncol = 3 , title = \"Run\" ) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 1 ) def _generate_non_cumulative_bar_plot_data ( self , benchmarking_results_df : pl . DataFrame ) -> pl . DataFrame : \"\"\" Generate data in the correct format for dataframe creation for a non-cumulative bar plot, appending to the self.stats attribute of the class. \"\"\" return self . _generate_stacked_data ( benchmarking_results_df ) . hstack ( self . _extract_mrr_data ( benchmarking_results_df ) . select ( pl . col ( \"Percentage\" ) . alias ( \"MRR\" ) ) ) def generate_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) def generate_non_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a non-cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_non_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation ) def generate_roc_curve ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: \"\"\" plt . clf () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] fpr = row [ \"fpr\" ] tpr = row [ \"tpr\" ] roc_auc = auc ( fpr , tpr ) plt . plot ( fpr , tpr , label = f \" { run_identifier } ROC Curve (AUC = { roc_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate\" ) plt . title ( plot_customisation . roc_curve_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _roc_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , ) def generate_precision_recall ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Precision-Recall curves for binary classification benchmark results. \"\"\" plt . clf () plt . figure () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] precision = row [ \"precision\" ] recall = row [ \"recall\" ] pr_auc = auc ( recall [:: - 1 ], precision [:: - 1 ]) plt . plot ( recall , precision , label = f \" { run_identifier } Precision-Recall Curve (AUC = { pr_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"Recall\" ) plt . ylabel ( \"Precision\" ) plt . title ( plot_customisation . precision_recall_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _pr_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , )","title":"PlotGenerator"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.__init__","text":"Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. Source code in src/pheval/analyse/generate_plots.py 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , benchmark_name : str ): \"\"\" Initialise the PlotGenerator class. Note: Matplotlib settings are configured to remove the right and top axes spines for generated plots. \"\"\" self . benchmark_name = benchmark_name matplotlib . rcParams [ \"axes.spines.right\" ] = False matplotlib . rcParams [ \"axes.spines.top\" ] = False","title":"__init__"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.generate_cumulative_bar","text":"Generate a cumulative bar plot. Source code in src/pheval/analyse/generate_plots.py 205 206 207 208 209 210 211 212 213 214 215 216 def generate_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation )","title":"generate_cumulative_bar"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.generate_non_cumulative_bar","text":"Generate a non-cumulative bar plot. Source code in src/pheval/analyse/generate_plots.py 218 219 220 221 222 223 224 225 226 227 228 229 def generate_non_cumulative_bar ( self , benchmarking_results_df : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a non-cumulative bar plot. \"\"\" plt . clf () stats_df = self . _generate_non_cumulative_bar_plot_data ( benchmarking_results_df ) self . _plot_bar_plot ( benchmark_generator , stats_df , plot_customisation )","title":"generate_non_cumulative_bar"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.generate_precision_recall","text":"Generate and plot Precision-Recall curves for binary classification benchmark results. Source code in src/pheval/analyse/generate_plots.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def generate_precision_recall ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Precision-Recall curves for binary classification benchmark results. \"\"\" plt . clf () plt . figure () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] precision = row [ \"precision\" ] recall = row [ \"recall\" ] pr_auc = auc ( recall [:: - 1 ], precision [:: - 1 ]) plt . plot ( recall , precision , label = f \" { run_identifier } Precision-Recall Curve (AUC = { pr_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"Recall\" ) plt . ylabel ( \"Precision\" ) plt . title ( plot_customisation . precision_recall_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _pr_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , )","title":"generate_precision_recall"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.generate_roc_curve","text":"Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: Source code in src/pheval/analyse/generate_plots.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def generate_roc_curve ( self , curves : pl . DataFrame , benchmark_generator : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ): \"\"\" Generate and plot Receiver Operating Characteristic (ROC) curves for binary classification benchmark results. Args: \"\"\" plt . clf () for i , row in enumerate ( curves . iter_rows ( named = True )): run_identifier = row [ \"run_identifier\" ] fpr = row [ \"fpr\" ] tpr = row [ \"tpr\" ] roc_auc = auc ( fpr , tpr ) plt . plot ( fpr , tpr , label = f \" { run_identifier } ROC Curve (AUC = { roc_auc : .2f } )\" , color = self . palette_hex_codes [ i ], ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], linestyle = \"--\" , color = \"gray\" ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate\" ) plt . title ( plot_customisation . roc_curve_title ) plt . legend ( loc = \"upper center\" , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . savefig ( f \" { self . benchmark_name } _ { benchmark_generator . prioritisation_type_string } _roc_curve.svg\" , format = \"svg\" , bbox_inches = \"tight\" , )","title":"generate_roc_curve"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.PlotGenerator.generate_stacked_bar_plot","text":"Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. Source code in src/pheval/analyse/generate_plots.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def generate_stacked_bar_plot ( self , benchmarking_results_df : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : SinglePlotCustomisation , ) -> None : \"\"\" Generate a stacked bar plot and Mean Reciprocal Rank (MRR) bar plot. Args: benchmarking_results_df (pl.DataFrame): benchmarking stats dataframe. benchmark_output_type (BenchmarkOutputType): Benchmark output type. plot_customisation (SinglePlotCustomisation): Plotting customisation. \"\"\" plt . clf () stats_df = self . _generate_stacked_data ( benchmarking_results_df ) stats_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , stacked = True , color = self . palette_hex_codes , ylabel = benchmark_output_type . y_label , edgecolor = \"white\" , ) . legend ( loc = \"center left\" , bbox_to_anchor = ( 1.0 , 0.5 )) plt . title ( plot_customisation . rank_plot_title , loc = \"center\" , fontsize = 15 ) self . _save_fig ( benchmark_output_type , 0 , 100 ) mrr_df = self . _extract_mrr_data ( benchmarking_results_df ) mrr_df . to_pandas () . set_index ( \"Run\" ) . plot ( kind = \"bar\" , color = self . palette_hex_codes , ylabel = f \" { benchmark_output_type . prioritisation_type_string . capitalize () } mean reciprocal rank\" , legend = False , edgecolor = \"white\" , ) plt . title ( f \" { benchmark_output_type . prioritisation_type_string . capitalize () } results - mean reciprocal rank\" ) self . _save_fig ( benchmark_output_type , 0 , 1 )","title":"generate_stacked_bar_plot"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.generate_plots","text":"Generate summary statistics bar plots for prioritisation. This method generates summary statistics bar plots based on the provided benchmarking results and plot type. Source code in src/pheval/analyse/generate_plots.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def generate_plots ( benchmark_name : str , benchmarking_results_df : pl . DataFrame , curves : pl . DataFrame , benchmark_output_type : BenchmarkOutputType , plot_customisation : PlotCustomisation , ) -> None : \"\"\" Generate summary statistics bar plots for prioritisation. This method generates summary statistics bar plots based on the provided benchmarking results and plot type. \"\"\" plot_generator = PlotGenerator ( benchmark_name ) plot_customisation_type = getattr ( plot_customisation , f \" { benchmark_output_type . prioritisation_type_string } _plots\" ) logger . info ( \"Generating ROC curve visualisations.\" ) plot_generator . generate_roc_curve ( curves , benchmark_output_type , plot_customisation_type ) logger . info ( \"Generating Precision-Recall curves visualisations.\" ) plot_generator . generate_precision_recall ( curves , benchmark_output_type , plot_customisation_type ) plot_type = PlotTypes ( plot_customisation_type . plot_type ) match plot_type : case PlotTypes . BAR_STACKED : logger . info ( \"Generating stacked bar plot.\" ) plot_generator . generate_stacked_bar_plot ( benchmarking_results_df , benchmark_output_type , plot_customisation_type ) case PlotTypes . BAR_CUMULATIVE : logger . info ( \"Generating cumulative bar plot.\" ) plot_generator . generate_cumulative_bar ( benchmarking_results_df , benchmark_output_type , plot_customisation_type ) case PlotTypes . BAR_NON_CUMULATIVE : logger . info ( \"Generating non cumulative bar plot.\" ) plot_generator . generate_non_cumulative_bar ( benchmarking_results_df , benchmark_output_type , plot_customisation_type )","title":"generate_plots"},{"location":"api/pheval/analyse/generate_plots/#src.pheval.analyse.generate_plots.generate_plots_from_db","text":"Generate plots from database file. Args: db_path (Path): Path to the database file. config (Path): Path to the benchmarking config file. Source code in src/pheval/analyse/generate_plots.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def generate_plots_from_db ( db_path : Path , config : Path ) -> None : \"\"\" Generate plots from database file. Args: db_path (Path): Path to the database file. config (Path): Path to the benchmarking config file. \"\"\" logger . info ( f \"Generating plots from { db_path } \" ) conn = duckdb . connect ( db_path ) logger . info ( f \"Parsing configurations from { config } \" ) benchmark_config_file = parse_run_config ( config ) tables = { row [ 0 ] for row in conn . execute ( \"\"\"SELECT table_name FROM duckdb_tables WHERE table_name \"\"\" \"\"\"LIKE '%_summary%' OR table_name LIKE '%_binary_classification_curves'\"\"\" ) . fetchall () } for benchmark_output_type in BenchmarkOutputTypeEnum : summary_table = ( f \" { benchmark_config_file . benchmark_name } _\" f \" { benchmark_output_type . value . prioritisation_type_string } _summary\" ) curve_table = ( f \" { benchmark_config_file . benchmark_name } _\" f \" { benchmark_output_type . value . prioritisation_type_string } _binary_classification_curves\" ) if summary_table in tables and curve_table in tables : logger . info ( f \"Generating plots for { benchmark_output_type . value . prioritisation_type_string } prioritisation.\" ) benchmarking_results_df = load_table_lazy ( summary_table , conn ) . collect () curves_df = load_table_lazy ( curve_table , conn ) . collect () generate_plots ( benchmark_name = benchmark_config_file . benchmark_name , benchmarking_results_df = benchmarking_results_df , curves = curves_df , benchmark_output_type = benchmark_output_type . value , plot_customisation = benchmark_config_file . plot_customisation , ) logger . info ( \"Finished generating plots.\" ) conn . close ()","title":"generate_plots_from_db"},{"location":"api/pheval/analyse/generate_rank_comparisons/","text":"calculate_rank_changes ( conn , run_identifiers , true_positive_cases , benchmark_type ) Calculate rank changes between runs. Args: conn (DuckDBPyConnection): DuckDB connection. run_identifiers (List[str]): List of run identifiers. true_positive_cases (pl.LazyFrame): All true positive cases for a benchmark. benchmark_type (BenchmarkOutputType): Type of benchmark output. Source code in src/pheval/analyse/generate_rank_comparisons.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def calculate_rank_changes ( conn : DuckDBPyConnection , run_identifiers : List [ str ], true_positive_cases : pl . DataFrame , benchmark_type : BenchmarkOutputType , ) -> None : \"\"\" Calculate rank changes between runs. Args: conn (DuckDBPyConnection): DuckDB connection. run_identifiers (List[str]): List of run identifiers. true_positive_cases (pl.LazyFrame): All true positive cases for a benchmark. benchmark_type (BenchmarkOutputType): Type of benchmark output. \"\"\" logger = get_logger () pairwise_comparisons = list ( combinations ( run_identifiers , 2 )) for col1 , col2 in pairwise_comparisons : logger . info ( f \"Comparing rank changes: { col1 } vs. { col2 } \" ) rank_change_lf = true_positive_cases . with_columns ( [ pl . when (( pl . col ( col1 ) == 0 ) & ( pl . col ( col2 ) != 0 )) . then ( pl . lit ( \"GAINED\" )) . when (( pl . col ( col1 ) != 0 ) & ( pl . col ( col2 ) == 0 )) . then ( pl . lit ( \"LOST\" )) . otherwise (( pl . col ( col1 ) - pl . col ( col2 )) . cast ( pl . Int64 )) . alias ( \"rank_change\" ) ] ) . select ([ \"result_file\" , * benchmark_type . columns , col1 , col2 , \"rank_change\" ]) write_table ( conn , rank_change_lf , f \" { col1 } _vs_ { col2 } _ { benchmark_type . prioritisation_type_string } _rank_changes\" , )","title":"Generate rank comparisons"},{"location":"api/pheval/analyse/generate_rank_comparisons/#src.pheval.analyse.generate_rank_comparisons.calculate_rank_changes","text":"Calculate rank changes between runs. Args: conn (DuckDBPyConnection): DuckDB connection. run_identifiers (List[str]): List of run identifiers. true_positive_cases (pl.LazyFrame): All true positive cases for a benchmark. benchmark_type (BenchmarkOutputType): Type of benchmark output. Source code in src/pheval/analyse/generate_rank_comparisons.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def calculate_rank_changes ( conn : DuckDBPyConnection , run_identifiers : List [ str ], true_positive_cases : pl . DataFrame , benchmark_type : BenchmarkOutputType , ) -> None : \"\"\" Calculate rank changes between runs. Args: conn (DuckDBPyConnection): DuckDB connection. run_identifiers (List[str]): List of run identifiers. true_positive_cases (pl.LazyFrame): All true positive cases for a benchmark. benchmark_type (BenchmarkOutputType): Type of benchmark output. \"\"\" logger = get_logger () pairwise_comparisons = list ( combinations ( run_identifiers , 2 )) for col1 , col2 in pairwise_comparisons : logger . info ( f \"Comparing rank changes: { col1 } vs. { col2 } \" ) rank_change_lf = true_positive_cases . with_columns ( [ pl . when (( pl . col ( col1 ) == 0 ) & ( pl . col ( col2 ) != 0 )) . then ( pl . lit ( \"GAINED\" )) . when (( pl . col ( col1 ) != 0 ) & ( pl . col ( col2 ) == 0 )) . then ( pl . lit ( \"LOST\" )) . otherwise (( pl . col ( col1 ) - pl . col ( col2 )) . cast ( pl . Int64 )) . alias ( \"rank_change\" ) ] ) . select ([ \"result_file\" , * benchmark_type . columns , col1 , col2 , \"rank_change\" ]) write_table ( conn , rank_change_lf , f \" { col1 } _vs_ { col2 } _ { benchmark_type . prioritisation_type_string } _rank_changes\" , )","title":"calculate_rank_changes"},{"location":"api/pheval/analyse/rank_stats/","text":"Ranks dataclass Class for calculating ranking statistics. Source code in src/pheval/analyse/rank_stats.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @dataclass ( frozen = True ) class Ranks : \"\"\" Class for calculating ranking statistics. \"\"\" TOP_1 = pl . col ( \"rank\" ) . eq ( 1 ) . sum () . alias ( \"top1\" ) TOP_3 = pl . col ( \"rank\" ) . is_between ( 1 , 3 , closed = \"both\" ) . sum () . alias ( \"top3\" ) TOP_5 = pl . col ( \"rank\" ) . is_between ( 1 , 5 , closed = \"both\" ) . sum () . alias ( \"top5\" ) TOP_10 = pl . col ( \"rank\" ) . is_between ( 1 , 10 , closed = \"both\" ) . sum () . alias ( \"top10\" ) FOUND = pl . col ( \"rank\" ) . gt ( 0 ) . sum () . alias ( \"found\" ) TOTAL = pl . len () . alias ( \"total\" ) NUMBER_OF_SAMPLES = pl . col ( \"file_path\" ) . n_unique () . alias ( \"number_of_samples\" ) MRR = (( 1 / pl . col ( \"rank\" ) . filter ( pl . col ( \"rank\" ) > 0 )) . sum () / pl . len ()) . alias ( \"mrr\" ) @classmethod def _filter_results ( cls , df : pl . LazyFrame , k : int ) -> pl . LazyFrame : \"\"\" Filter for ranks within k. Args: df (pl.LazyFrame): The dataframe to filter. k (int): The number upper rank limit. Returns: pl.LazyFrame: The filtered dataframe. \"\"\" df = df . filter ( pl . col ( \"rank\" ) . is_between ( 1 , k , closed = \"both\" )) return df . group_by ( \"file_path\" ) . agg ( pl . col ( \"rank\" ) . sort () . alias ( \"ranks\" ), ) @classmethod def percentage_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. \"\"\" return ( 100 * pl . col ( f \"top { k } \" ) / pl . col ( \"total\" )) . alias ( f \"percentage@ { k } \" ) @classmethod def percentage_found ( cls ) -> pl . Expr : \"\"\" Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. \"\"\" return ( 100 * pl . col ( \"found\" ) / pl . col ( \"total\" )) . alias ( \"percentage_found\" ) @classmethod def precision_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. \"\"\" return ( pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k )) . alias ( f \"precision@ { k } \" ) @classmethod def f_beta_score_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. \"\"\" precision_expr = pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k ) recall_expr = pl . col ( f \"top { k } \" ) / pl . col ( \"total\" ) return ( (( 2 * precision_expr * recall_expr ) / ( precision_expr + recall_expr )) . fill_nan ( 0 ) . alias ( f \"f_beta@ { k } \" ) ) @classmethod def _average_precision_at_k ( cls , df : pl . LazyFrame , k : int ) -> pl . LazyFrame : \"\"\" Compute Average Precision at K (AP@K) for each query. AP@K = (1 / min(k, R)) * sum(P(i) * rel(i)) for i \u2264 k Args: df (pl.LazyFrame): The dataframe calculate AP@K for each query. k (int): The upper rank limit. Returns: pl.LazyFrame: The dataframe with AP@K for each query. \"\"\" filtered_df = cls . _filter_results ( df , k ) df_grouped = filtered_df . with_columns ( pl . struct ( \"ranks\" ) . map_elements ( lambda row : cls . _compute_ap_k ( np . array ( row [ \"ranks\" ])), return_dtype = pl . Float64 ) . alias ( f \"ap@ { k } \" ) ) return df_grouped . select ([ \"file_path\" , f \"ap@ { k } \" ]) @staticmethod def _compute_ap_k ( ranks : np . array ) -> np . floating : \"\"\" Helper function to compute AP@K for a single query. Args: ranks (np.array): The ranks to compute AP@K. Returns: float: The AP@K. \"\"\" num_relevant = np . arange ( 1 , len ( ranks ) + 1 ) precision_at_k = num_relevant / ranks return np . mean ( precision_at_k ) @classmethod def mean_average_precision_at_k ( cls , df : pl . LazyFrame , k : int ) -> float : ap_at_k_df = cls . _average_precision_at_k ( df , k ) ap_sum = ap_at_k_df . select ( pl . col ( f \"ap@ { k } \" ) . sum ()) . collect () . item () num_samples = df . select ( Ranks . NUMBER_OF_SAMPLES ) . collect () . item () return ap_sum / num_samples @classmethod def _calculate_ndcg_at_k ( cls , ranks : List [ int ], k : int ) -> float : \"\"\" Compute NDCG@K for a single query. Args: ranks (List[int]): The ranks to compute NDCG@K. k (int): The upper rank limit. Returns: float: The NDCG@K. \"\"\" result_ranks = np . zeros ( k , dtype = int ) indices = np . array ( ranks ) - 1 valid_indices = indices [( indices >= 0 ) & ( indices < k )] result_ranks [ valid_indices ] = 3 ideal_ranking = np . sort ( result_ranks )[:: - 1 ] return ( ndcg_score ( result_ranks . reshape ( 1 , - 1 ), ideal_ranking . reshape ( 1 , - 1 )) if np . sum ( result_ranks ) > 0 else 0.0 ) @classmethod def mean_normalised_discounted_cumulative_gain ( cls , df : pl . LazyFrame , k : int ) -> float : filtered_df = cls . _filter_results ( df , k ) ndcg_df = filtered_df . with_columns ( pl . struct ( \"ranks\" ) . map_elements ( lambda row : cls . _calculate_ndcg_at_k ( row [ \"ranks\" ], k ), return_dtype = pl . Float64 ) . alias ( f \"NDCG@ { k } \" ) ) ndcg_sum = ndcg_df . select ( pl . col ( f \"NDCG@ { k } \" ) . sum ()) . collect () . item () num_samples = df . select ( Ranks . NUMBER_OF_SAMPLES ) . collect () . item () return ndcg_sum / num_samples f_beta_score_at_k ( k ) classmethod Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. Source code in src/pheval/analyse/rank_stats.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @classmethod def f_beta_score_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. \"\"\" precision_expr = pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k ) recall_expr = pl . col ( f \"top { k } \" ) / pl . col ( \"total\" ) return ( (( 2 * precision_expr * recall_expr ) / ( precision_expr + recall_expr )) . fill_nan ( 0 ) . alias ( f \"f_beta@ { k } \" ) ) percentage_at_k ( k ) classmethod Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. Source code in src/pheval/analyse/rank_stats.py 42 43 44 45 46 47 48 49 50 51 @classmethod def percentage_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. \"\"\" return ( 100 * pl . col ( f \"top { k } \" ) / pl . col ( \"total\" )) . alias ( f \"percentage@ { k } \" ) percentage_found () classmethod Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. Source code in src/pheval/analyse/rank_stats.py 53 54 55 56 57 58 59 60 @classmethod def percentage_found ( cls ) -> pl . Expr : \"\"\" Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. \"\"\" return ( 100 * pl . col ( \"found\" ) / pl . col ( \"total\" )) . alias ( \"percentage_found\" ) precision_at_k ( k ) classmethod Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. Source code in src/pheval/analyse/rank_stats.py 62 63 64 65 66 67 68 69 70 71 @classmethod def precision_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. \"\"\" return ( pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k )) . alias ( f \"precision@ { k } \" ) compute_rank_stats ( run_identifier , result_scan ) Computes ranking statistics for a given benchmarking run. Args: run_identifier (str): The identifier of the benchmarking run. result_scan (pl.LazyFrame): The scan of the directory to compute ranking statistics for. Source code in src/pheval/analyse/rank_stats.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def compute_rank_stats ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Computes ranking statistics for a given benchmarking run. Args: run_identifier (str): The identifier of the benchmarking run. result_scan (pl.LazyFrame): The scan of the directory to compute ranking statistics for. \"\"\" logger = get_logger () logger . info ( f \"Generating ranking statistics for { run_identifier } ...\" ) true_positive_scan = result_scan . filter ( pl . col ( \"true_positive\" )) rankings = true_positive_scan . select ( [ pl . lit ( run_identifier ) . alias ( \"run_identifier\" ), Ranks . TOP_1 . alias ( \"top1\" ), Ranks . TOP_3 . alias ( \"top3\" ), Ranks . TOP_5 . alias ( \"top5\" ), Ranks . TOP_10 . alias ( \"top10\" ), Ranks . FOUND . alias ( \"found\" ), Ranks . TOTAL . alias ( \"total\" ), Ranks . NUMBER_OF_SAMPLES . alias ( \"number_of_samples\" ), Ranks . MRR . alias ( \"mrr\" ), ] ) return rankings . select ( [ pl . col ( \"run_identifier\" ), pl . col ( \"top1\" ), pl . col ( \"top3\" ), pl . col ( \"top5\" ), pl . col ( \"top10\" ), pl . col ( \"found\" ), pl . col ( \"total\" ), pl . col ( \"number_of_samples\" ), pl . col ( \"mrr\" ), Ranks . percentage_at_k ( 1 ), Ranks . percentage_at_k ( 3 ), Ranks . percentage_at_k ( 5 ), Ranks . percentage_at_k ( 10 ), Ranks . percentage_found (), Ranks . precision_at_k ( 1 ), Ranks . precision_at_k ( 3 ), Ranks . precision_at_k ( 5 ), Ranks . precision_at_k ( 10 ), Ranks . f_beta_score_at_k ( 1 ), Ranks . f_beta_score_at_k ( 3 ), Ranks . f_beta_score_at_k ( 5 ), Ranks . f_beta_score_at_k ( 10 ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 1 )) . alias ( \"MAP@1\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 3 )) . alias ( \"MAP@3\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 5 )) . alias ( \"MAP@5\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 10 )) . alias ( \"MAP@10\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 3 )) . alias ( \"NDCG@3\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 5 )) . alias ( \"NDCG@5\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 10 )) . alias ( \"NDCG@10\" ), ] )","title":"Rank stats"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.Ranks","text":"Class for calculating ranking statistics. Source code in src/pheval/analyse/rank_stats.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @dataclass ( frozen = True ) class Ranks : \"\"\" Class for calculating ranking statistics. \"\"\" TOP_1 = pl . col ( \"rank\" ) . eq ( 1 ) . sum () . alias ( \"top1\" ) TOP_3 = pl . col ( \"rank\" ) . is_between ( 1 , 3 , closed = \"both\" ) . sum () . alias ( \"top3\" ) TOP_5 = pl . col ( \"rank\" ) . is_between ( 1 , 5 , closed = \"both\" ) . sum () . alias ( \"top5\" ) TOP_10 = pl . col ( \"rank\" ) . is_between ( 1 , 10 , closed = \"both\" ) . sum () . alias ( \"top10\" ) FOUND = pl . col ( \"rank\" ) . gt ( 0 ) . sum () . alias ( \"found\" ) TOTAL = pl . len () . alias ( \"total\" ) NUMBER_OF_SAMPLES = pl . col ( \"file_path\" ) . n_unique () . alias ( \"number_of_samples\" ) MRR = (( 1 / pl . col ( \"rank\" ) . filter ( pl . col ( \"rank\" ) > 0 )) . sum () / pl . len ()) . alias ( \"mrr\" ) @classmethod def _filter_results ( cls , df : pl . LazyFrame , k : int ) -> pl . LazyFrame : \"\"\" Filter for ranks within k. Args: df (pl.LazyFrame): The dataframe to filter. k (int): The number upper rank limit. Returns: pl.LazyFrame: The filtered dataframe. \"\"\" df = df . filter ( pl . col ( \"rank\" ) . is_between ( 1 , k , closed = \"both\" )) return df . group_by ( \"file_path\" ) . agg ( pl . col ( \"rank\" ) . sort () . alias ( \"ranks\" ), ) @classmethod def percentage_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. \"\"\" return ( 100 * pl . col ( f \"top { k } \" ) / pl . col ( \"total\" )) . alias ( f \"percentage@ { k } \" ) @classmethod def percentage_found ( cls ) -> pl . Expr : \"\"\" Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. \"\"\" return ( 100 * pl . col ( \"found\" ) / pl . col ( \"total\" )) . alias ( \"percentage_found\" ) @classmethod def precision_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. \"\"\" return ( pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k )) . alias ( f \"precision@ { k } \" ) @classmethod def f_beta_score_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. \"\"\" precision_expr = pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k ) recall_expr = pl . col ( f \"top { k } \" ) / pl . col ( \"total\" ) return ( (( 2 * precision_expr * recall_expr ) / ( precision_expr + recall_expr )) . fill_nan ( 0 ) . alias ( f \"f_beta@ { k } \" ) ) @classmethod def _average_precision_at_k ( cls , df : pl . LazyFrame , k : int ) -> pl . LazyFrame : \"\"\" Compute Average Precision at K (AP@K) for each query. AP@K = (1 / min(k, R)) * sum(P(i) * rel(i)) for i \u2264 k Args: df (pl.LazyFrame): The dataframe calculate AP@K for each query. k (int): The upper rank limit. Returns: pl.LazyFrame: The dataframe with AP@K for each query. \"\"\" filtered_df = cls . _filter_results ( df , k ) df_grouped = filtered_df . with_columns ( pl . struct ( \"ranks\" ) . map_elements ( lambda row : cls . _compute_ap_k ( np . array ( row [ \"ranks\" ])), return_dtype = pl . Float64 ) . alias ( f \"ap@ { k } \" ) ) return df_grouped . select ([ \"file_path\" , f \"ap@ { k } \" ]) @staticmethod def _compute_ap_k ( ranks : np . array ) -> np . floating : \"\"\" Helper function to compute AP@K for a single query. Args: ranks (np.array): The ranks to compute AP@K. Returns: float: The AP@K. \"\"\" num_relevant = np . arange ( 1 , len ( ranks ) + 1 ) precision_at_k = num_relevant / ranks return np . mean ( precision_at_k ) @classmethod def mean_average_precision_at_k ( cls , df : pl . LazyFrame , k : int ) -> float : ap_at_k_df = cls . _average_precision_at_k ( df , k ) ap_sum = ap_at_k_df . select ( pl . col ( f \"ap@ { k } \" ) . sum ()) . collect () . item () num_samples = df . select ( Ranks . NUMBER_OF_SAMPLES ) . collect () . item () return ap_sum / num_samples @classmethod def _calculate_ndcg_at_k ( cls , ranks : List [ int ], k : int ) -> float : \"\"\" Compute NDCG@K for a single query. Args: ranks (List[int]): The ranks to compute NDCG@K. k (int): The upper rank limit. Returns: float: The NDCG@K. \"\"\" result_ranks = np . zeros ( k , dtype = int ) indices = np . array ( ranks ) - 1 valid_indices = indices [( indices >= 0 ) & ( indices < k )] result_ranks [ valid_indices ] = 3 ideal_ranking = np . sort ( result_ranks )[:: - 1 ] return ( ndcg_score ( result_ranks . reshape ( 1 , - 1 ), ideal_ranking . reshape ( 1 , - 1 )) if np . sum ( result_ranks ) > 0 else 0.0 ) @classmethod def mean_normalised_discounted_cumulative_gain ( cls , df : pl . LazyFrame , k : int ) -> float : filtered_df = cls . _filter_results ( df , k ) ndcg_df = filtered_df . with_columns ( pl . struct ( \"ranks\" ) . map_elements ( lambda row : cls . _calculate_ndcg_at_k ( row [ \"ranks\" ], k ), return_dtype = pl . Float64 ) . alias ( f \"NDCG@ { k } \" ) ) ndcg_sum = ndcg_df . select ( pl . col ( f \"NDCG@ { k } \" ) . sum ()) . collect () . item () num_samples = df . select ( Ranks . NUMBER_OF_SAMPLES ) . collect () . item () return ndcg_sum / num_samples","title":"Ranks"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.Ranks.f_beta_score_at_k","text":"Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. Source code in src/pheval/analyse/rank_stats.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @classmethod def f_beta_score_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute f_beta_score at k. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating f_beta_score at k. \"\"\" precision_expr = pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k ) recall_expr = pl . col ( f \"top { k } \" ) / pl . col ( \"total\" ) return ( (( 2 * precision_expr * recall_expr ) / ( precision_expr + recall_expr )) . fill_nan ( 0 ) . alias ( f \"f_beta@ { k } \" ) )","title":"f_beta_score_at_k"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.Ranks.percentage_at_k","text":"Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. Source code in src/pheval/analyse/rank_stats.py 42 43 44 45 46 47 48 49 50 51 @classmethod def percentage_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute percentage at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating percentage at k. \"\"\" return ( 100 * pl . col ( f \"top { k } \" ) / pl . col ( \"total\" )) . alias ( f \"percentage@ { k } \" )","title":"percentage_at_k"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.Ranks.percentage_found","text":"Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. Source code in src/pheval/analyse/rank_stats.py 53 54 55 56 57 58 59 60 @classmethod def percentage_found ( cls ) -> pl . Expr : \"\"\" Compute the percentage of found items. Returns: pl.Expr: The expression for calculating percentage of found items. \"\"\" return ( 100 * pl . col ( \"found\" ) / pl . col ( \"total\" )) . alias ( \"percentage_found\" )","title":"percentage_found"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.Ranks.precision_at_k","text":"Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. Source code in src/pheval/analyse/rank_stats.py 62 63 64 65 66 67 68 69 70 71 @classmethod def precision_at_k ( cls , k : int ) -> pl . Expr : \"\"\" Compute precision at k dynamically. Args: k (int): The upper rank limit. Returns: pl.Expr: The expression for calculating precision at k. \"\"\" return ( pl . col ( f \"top { k } \" ) / ( pl . col ( \"number_of_samples\" ) * k )) . alias ( f \"precision@ { k } \" )","title":"precision_at_k"},{"location":"api/pheval/analyse/rank_stats/#src.pheval.analyse.rank_stats.compute_rank_stats","text":"Computes ranking statistics for a given benchmarking run. Args: run_identifier (str): The identifier of the benchmarking run. result_scan (pl.LazyFrame): The scan of the directory to compute ranking statistics for. Source code in src/pheval/analyse/rank_stats.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def compute_rank_stats ( run_identifier : str , result_scan : pl . LazyFrame ) -> pl . LazyFrame : \"\"\" Computes ranking statistics for a given benchmarking run. Args: run_identifier (str): The identifier of the benchmarking run. result_scan (pl.LazyFrame): The scan of the directory to compute ranking statistics for. \"\"\" logger = get_logger () logger . info ( f \"Generating ranking statistics for { run_identifier } ...\" ) true_positive_scan = result_scan . filter ( pl . col ( \"true_positive\" )) rankings = true_positive_scan . select ( [ pl . lit ( run_identifier ) . alias ( \"run_identifier\" ), Ranks . TOP_1 . alias ( \"top1\" ), Ranks . TOP_3 . alias ( \"top3\" ), Ranks . TOP_5 . alias ( \"top5\" ), Ranks . TOP_10 . alias ( \"top10\" ), Ranks . FOUND . alias ( \"found\" ), Ranks . TOTAL . alias ( \"total\" ), Ranks . NUMBER_OF_SAMPLES . alias ( \"number_of_samples\" ), Ranks . MRR . alias ( \"mrr\" ), ] ) return rankings . select ( [ pl . col ( \"run_identifier\" ), pl . col ( \"top1\" ), pl . col ( \"top3\" ), pl . col ( \"top5\" ), pl . col ( \"top10\" ), pl . col ( \"found\" ), pl . col ( \"total\" ), pl . col ( \"number_of_samples\" ), pl . col ( \"mrr\" ), Ranks . percentage_at_k ( 1 ), Ranks . percentage_at_k ( 3 ), Ranks . percentage_at_k ( 5 ), Ranks . percentage_at_k ( 10 ), Ranks . percentage_found (), Ranks . precision_at_k ( 1 ), Ranks . precision_at_k ( 3 ), Ranks . precision_at_k ( 5 ), Ranks . precision_at_k ( 10 ), Ranks . f_beta_score_at_k ( 1 ), Ranks . f_beta_score_at_k ( 3 ), Ranks . f_beta_score_at_k ( 5 ), Ranks . f_beta_score_at_k ( 10 ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 1 )) . alias ( \"MAP@1\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 3 )) . alias ( \"MAP@3\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 5 )) . alias ( \"MAP@5\" ), pl . lit ( Ranks . mean_average_precision_at_k ( true_positive_scan , 10 )) . alias ( \"MAP@10\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 3 )) . alias ( \"NDCG@3\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 5 )) . alias ( \"NDCG@5\" ), pl . lit ( Ranks . mean_normalised_discounted_cumulative_gain ( true_positive_scan , 10 )) . alias ( \"NDCG@10\" ), ] )","title":"compute_rank_stats"},{"location":"api/pheval/analyse/run_data_parser/","text":"Config Bases: BaseModel Store configurations for a runs. Attributes: runs (List[RunConfig]): The list of run configurations. Source code in src/pheval/analyse/run_data_parser.py 90 91 92 93 94 95 96 97 98 99 class Config ( BaseModel ): \"\"\" Store configurations for a runs. Attributes: runs (List[RunConfig]): The list of run configurations. \"\"\" benchmark_name : str runs : List [ RunConfig ] plot_customisation : PlotCustomisation PlotCustomisation Bases: BaseModel Store customisations for all plots. Attributes: gene_plots (SinglePlotCustomisation): Customisation for all gene benchmarking plots. disease_plots (SinglePlotCustomisation): Customisation for all disease benchmarking plots. variant_plots (SinglePlotCustomisation): Customisation for all variant benchmarking plots. Source code in src/pheval/analyse/run_data_parser.py 76 77 78 79 80 81 82 83 84 85 86 87 class PlotCustomisation ( BaseModel ): \"\"\" Store customisations for all plots. Attributes: gene_plots (SinglePlotCustomisation): Customisation for all gene benchmarking plots. disease_plots (SinglePlotCustomisation): Customisation for all disease benchmarking plots. variant_plots (SinglePlotCustomisation): Customisation for all variant benchmarking plots. \"\"\" gene_plots : SinglePlotCustomisation disease_plots : SinglePlotCustomisation variant_plots : SinglePlotCustomisation RunConfig Bases: BaseModel Store configurations for a run. Attributes: Name Type Description run_identifier str The run identifier. phenopacket_dir str The path to the phenopacket directory used for generating the results. results_dir str The path to the result directory. gene_analysis bool Whether to benchmark gene analysis results. variant_analysis bool Whether to benchmark variant analysis results. disease_analysis bool Whether to benchmark disease analysis results. threshold Optional [ float ] The threshold to consider for benchmarking. score_order Optional [ str ] The order of scores to consider for benchmarking, either ascending or descending. Source code in src/pheval/analyse/run_data_parser.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class RunConfig ( BaseModel ): \"\"\" Store configurations for a run. Attributes: run_identifier (str): The run identifier. phenopacket_dir (str): The path to the phenopacket directory used for generating the results. results_dir (str): The path to the result directory. gene_analysis (bool): Whether to benchmark gene analysis results. variant_analysis (bool): Whether to benchmark variant analysis results. disease_analysis (bool): Whether to benchmark disease analysis results. threshold (Optional[float]): The threshold to consider for benchmarking. score_order (Optional[str]): The order of scores to consider for benchmarking, either ascending or descending. \"\"\" run_identifier : str phenopacket_dir : Path results_dir : Path gene_analysis : bool variant_analysis : bool disease_analysis : bool threshold : Optional [ float ] score_order : Optional [ str ] @field_validator ( \"threshold\" , mode = \"before\" ) @classmethod def set_threshold ( cls , threshold ): return threshold or None @field_validator ( \"score_order\" , mode = \"before\" ) @classmethod def set_score_order ( cls , score_order ): return score_order or \"descending\" @field_validator ( \"results_dir\" , mode = \"after\" ) @classmethod def check_results_dir_exists ( cls , results_dir : Path ): if not results_dir . exists (): raise FileNotFoundError ( f \"The specified results directory does not exist: { results_dir } \" ) return results_dir SinglePlotCustomisation Bases: BaseModel Store customisations for plots. Attributes: Name Type Description plot_type str The plot type. rank_plot_title str The title for the rank summary plot. roc_curve_title str The title for the roc curve plot. precision_recall_title str The title for the precision-recall plot. Source code in src/pheval/analyse/run_data_parser.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class SinglePlotCustomisation ( BaseModel ): \"\"\" Store customisations for plots. Attributes: plot_type (str): The plot type. rank_plot_title (str): The title for the rank summary plot. roc_curve_title (str): The title for the roc curve plot. precision_recall_title (str): The title for the precision-recall plot. \"\"\" plot_type : Optional [ str ] = \"bar_cumulative\" rank_plot_title : Optional [ str ] roc_curve_title : Optional [ str ] precision_recall_title : Optional [ str ] @field_validator ( \"plot_type\" , mode = \"before\" ) @classmethod def set_plot_type ( cls , plot_type ): return plot_type or \"bar_cumulative\" parse_run_config ( run_config ) Parse a run configuration yaml file. Args: run_config (Path): The path to the run data yaml configuration. Returns: Config: The parsed run configurations. Source code in src/pheval/analyse/run_data_parser.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def parse_run_config ( run_config : Path ) -> Config : \"\"\" Parse a run configuration yaml file. Args: run_config (Path): The path to the run data yaml configuration. Returns: Config: The parsed run configurations. \"\"\" logger = get_logger () logger . info ( f \"Loading benchmark configuration from { run_config } \" ) with open ( run_config , \"r\" ) as f : config_data = yaml . safe_load ( f ) f . close () config = Config ( ** config_data ) return config","title":"Run data parser"},{"location":"api/pheval/analyse/run_data_parser/#src.pheval.analyse.run_data_parser.Config","text":"Bases: BaseModel Store configurations for a runs. Attributes: runs (List[RunConfig]): The list of run configurations. Source code in src/pheval/analyse/run_data_parser.py 90 91 92 93 94 95 96 97 98 99 class Config ( BaseModel ): \"\"\" Store configurations for a runs. Attributes: runs (List[RunConfig]): The list of run configurations. \"\"\" benchmark_name : str runs : List [ RunConfig ] plot_customisation : PlotCustomisation","title":"Config"},{"location":"api/pheval/analyse/run_data_parser/#src.pheval.analyse.run_data_parser.PlotCustomisation","text":"Bases: BaseModel Store customisations for all plots. Attributes: gene_plots (SinglePlotCustomisation): Customisation for all gene benchmarking plots. disease_plots (SinglePlotCustomisation): Customisation for all disease benchmarking plots. variant_plots (SinglePlotCustomisation): Customisation for all variant benchmarking plots. Source code in src/pheval/analyse/run_data_parser.py 76 77 78 79 80 81 82 83 84 85 86 87 class PlotCustomisation ( BaseModel ): \"\"\" Store customisations for all plots. Attributes: gene_plots (SinglePlotCustomisation): Customisation for all gene benchmarking plots. disease_plots (SinglePlotCustomisation): Customisation for all disease benchmarking plots. variant_plots (SinglePlotCustomisation): Customisation for all variant benchmarking plots. \"\"\" gene_plots : SinglePlotCustomisation disease_plots : SinglePlotCustomisation variant_plots : SinglePlotCustomisation","title":"PlotCustomisation"},{"location":"api/pheval/analyse/run_data_parser/#src.pheval.analyse.run_data_parser.RunConfig","text":"Bases: BaseModel Store configurations for a run. Attributes: Name Type Description run_identifier str The run identifier. phenopacket_dir str The path to the phenopacket directory used for generating the results. results_dir str The path to the result directory. gene_analysis bool Whether to benchmark gene analysis results. variant_analysis bool Whether to benchmark variant analysis results. disease_analysis bool Whether to benchmark disease analysis results. threshold Optional [ float ] The threshold to consider for benchmarking. score_order Optional [ str ] The order of scores to consider for benchmarking, either ascending or descending. Source code in src/pheval/analyse/run_data_parser.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class RunConfig ( BaseModel ): \"\"\" Store configurations for a run. Attributes: run_identifier (str): The run identifier. phenopacket_dir (str): The path to the phenopacket directory used for generating the results. results_dir (str): The path to the result directory. gene_analysis (bool): Whether to benchmark gene analysis results. variant_analysis (bool): Whether to benchmark variant analysis results. disease_analysis (bool): Whether to benchmark disease analysis results. threshold (Optional[float]): The threshold to consider for benchmarking. score_order (Optional[str]): The order of scores to consider for benchmarking, either ascending or descending. \"\"\" run_identifier : str phenopacket_dir : Path results_dir : Path gene_analysis : bool variant_analysis : bool disease_analysis : bool threshold : Optional [ float ] score_order : Optional [ str ] @field_validator ( \"threshold\" , mode = \"before\" ) @classmethod def set_threshold ( cls , threshold ): return threshold or None @field_validator ( \"score_order\" , mode = \"before\" ) @classmethod def set_score_order ( cls , score_order ): return score_order or \"descending\" @field_validator ( \"results_dir\" , mode = \"after\" ) @classmethod def check_results_dir_exists ( cls , results_dir : Path ): if not results_dir . exists (): raise FileNotFoundError ( f \"The specified results directory does not exist: { results_dir } \" ) return results_dir","title":"RunConfig"},{"location":"api/pheval/analyse/run_data_parser/#src.pheval.analyse.run_data_parser.SinglePlotCustomisation","text":"Bases: BaseModel Store customisations for plots. Attributes: Name Type Description plot_type str The plot type. rank_plot_title str The title for the rank summary plot. roc_curve_title str The title for the roc curve plot. precision_recall_title str The title for the precision-recall plot. Source code in src/pheval/analyse/run_data_parser.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class SinglePlotCustomisation ( BaseModel ): \"\"\" Store customisations for plots. Attributes: plot_type (str): The plot type. rank_plot_title (str): The title for the rank summary plot. roc_curve_title (str): The title for the roc curve plot. precision_recall_title (str): The title for the precision-recall plot. \"\"\" plot_type : Optional [ str ] = \"bar_cumulative\" rank_plot_title : Optional [ str ] roc_curve_title : Optional [ str ] precision_recall_title : Optional [ str ] @field_validator ( \"plot_type\" , mode = \"before\" ) @classmethod def set_plot_type ( cls , plot_type ): return plot_type or \"bar_cumulative\"","title":"SinglePlotCustomisation"},{"location":"api/pheval/analyse/run_data_parser/#src.pheval.analyse.run_data_parser.parse_run_config","text":"Parse a run configuration yaml file. Args: run_config (Path): The path to the run data yaml configuration. Returns: Config: The parsed run configurations. Source code in src/pheval/analyse/run_data_parser.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def parse_run_config ( run_config : Path ) -> Config : \"\"\" Parse a run configuration yaml file. Args: run_config (Path): The path to the run data yaml configuration. Returns: Config: The parsed run configurations. \"\"\" logger = get_logger () logger . info ( f \"Loading benchmark configuration from { run_config } \" ) with open ( run_config , \"r\" ) as f : config_data = yaml . safe_load ( f ) f . close () config = Config ( ** config_data ) return config","title":"parse_run_config"},{"location":"api/pheval/implementations/pheval_class_resolver/","text":"PhevalClassResolver Bases: ClassResolver Source code in src/pheval/implementations/pheval_class_resolver.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PhevalClassResolver ( ClassResolver ): def __init__ ( self , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) # Modified _from_entrypoint method to raise error instead of warning @staticmethod def _from_entrypoint_custom ( group : str ) -> set [ X ]: elements : set [ X ] = set () for entry in entry_points ( group = group ): try : element = entry . load () logger . info ( f \"Loaded { entry . name } correctly\" ) except ( ImportError , AttributeError ): logger . warn ( f \"could not load { entry . name } . See error message below.\" ) raise else : elements . add ( element ) return elements def register_entrypoint ( self , group : str ) -> None : \"\"\"Register additional entries from an entrypoint.\"\"\" for element in self . _from_entrypoint_custom ( group ) . difference ( self . lookup_dict . values ()): self . register ( element ) register_entrypoint ( group ) Register additional entries from an entrypoint. Source code in src/pheval/implementations/pheval_class_resolver.py 37 38 39 40 def register_entrypoint ( self , group : str ) -> None : \"\"\"Register additional entries from an entrypoint.\"\"\" for element in self . _from_entrypoint_custom ( group ) . difference ( self . lookup_dict . values ()): self . register ( element )","title":"Pheval class resolver"},{"location":"api/pheval/implementations/pheval_class_resolver/#src.pheval.implementations.pheval_class_resolver.PhevalClassResolver","text":"Bases: ClassResolver Source code in src/pheval/implementations/pheval_class_resolver.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PhevalClassResolver ( ClassResolver ): def __init__ ( self , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) # Modified _from_entrypoint method to raise error instead of warning @staticmethod def _from_entrypoint_custom ( group : str ) -> set [ X ]: elements : set [ X ] = set () for entry in entry_points ( group = group ): try : element = entry . load () logger . info ( f \"Loaded { entry . name } correctly\" ) except ( ImportError , AttributeError ): logger . warn ( f \"could not load { entry . name } . See error message below.\" ) raise else : elements . add ( element ) return elements def register_entrypoint ( self , group : str ) -> None : \"\"\"Register additional entries from an entrypoint.\"\"\" for element in self . _from_entrypoint_custom ( group ) . difference ( self . lookup_dict . values ()): self . register ( element )","title":"PhevalClassResolver"},{"location":"api/pheval/implementations/pheval_class_resolver/#src.pheval.implementations.pheval_class_resolver.PhevalClassResolver.register_entrypoint","text":"Register additional entries from an entrypoint. Source code in src/pheval/implementations/pheval_class_resolver.py 37 38 39 40 def register_entrypoint ( self , group : str ) -> None : \"\"\"Register additional entries from an entrypoint.\"\"\" for element in self . _from_entrypoint_custom ( group ) . difference ( self . lookup_dict . values ()): self . register ( element )","title":"register_entrypoint"},{"location":"api/pheval/infra/exomiserdb/","text":"DBConnection Source code in src/pheval/infra/exomiserdb.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class DBConnection : connection = None def __init__ ( self , connection ): DBConnection . connection = connection @classmethod def get_connection ( cls ) -> jaydebeapi . Connection : \"\"\"Creates return new Singleton database connection\"\"\" return DBConnection . connection def close ( self ): return self . connection . close () @classmethod def get_cursor ( cls ) -> jaydebeapi . Cursor : connection = cls . get_connection () return connection . cursor () get_connection () classmethod Creates return new Singleton database connection Source code in src/pheval/infra/exomiserdb.py 49 50 51 52 @classmethod def get_connection ( cls ) -> jaydebeapi . Connection : \"\"\"Creates return new Singleton database connection\"\"\" return DBConnection . connection DBConnector Source code in src/pheval/infra/exomiserdb.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class DBConnector : def __init__ ( self , jar : Path , driver : str , server : str , database : str , user : str , password : str ): self . jar = jar self . driver = driver self . server = server self . database = database self . user = user self . password = password self . dbconn = None def create_connection ( self ) -> jaydebeapi . Connection : \"\"\"creates h2 database connection\"\"\" return jaydebeapi . connect ( self . driver , f \" { self . server }{ self . database } \" , [ self . user , self . password ], self . jar , ) def __enter__ ( self ) -> jaydebeapi . Connection : self . dbconn = self . create_connection () return self . dbconn def __exit__ ( self , * other ): self . dbconn . close () create_connection () creates h2 database connection Source code in src/pheval/infra/exomiserdb.py 26 27 28 29 30 31 32 33 def create_connection ( self ) -> jaydebeapi . Connection : \"\"\"creates h2 database connection\"\"\" return jaydebeapi . connect ( self . driver , f \" { self . server }{ self . database } \" , [ self . user , self . password ], self . jar , ) ExomiserDB Source code in src/pheval/infra/exomiserdb.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class ExomiserDB : def __init__ ( self , db_path : Path ): try : self . connector = DBConnector ( # noqa jar = os . path . join ( os . path . dirname ( __file__ ), \"../../../lib/h2-1.4.199.jar\" ), driver = \"org.h2.Driver\" , server = f \"jdbc:h2: { db_path } \" , user = \"sa\" , password = \"\" , database = \"\" , ) except Exception as e : print ( \"An exception occurred\" , e ) def import_from_semsim_file ( self , input_file : Path , subject_prefix : str , object_prefix : str ): \"\"\"imports semsim tsv profile into exomiser phenotype database Args: input_file (Path): semsim profile subject_prefix (str): Subject Prefix. e.g HP object_prefix (str): Object Prefix. e.g MP \"\"\" with self . connector as cnn : conn = DBConnection ( cnn ) reader = pl . read_csv_batched ( input_file , separator = \" \\t \" ) batch_length = 5 batches = reader . next_batches ( batch_length ) cursor = conn . get_cursor () # # TODO: Refactor this with open ( input_file , \"r\" ) as f : total = sum ( 1 for line in f ) pbar = tqdm ( total = total - 1 ) mapping_id = 1 while batches : input_data = pl . concat ( batches ) sql = _semsim2h2 ( input_data , object_prefix , subject_prefix , mapping_id = mapping_id ) cursor . execute ( sql ) len_input_data = len ( input_data ) mapping_id += len_input_data pbar . update ( len_input_data ) batches = reader . next_batches ( batch_length ) import_from_semsim_file ( input_file , subject_prefix , object_prefix ) imports semsim tsv profile into exomiser phenotype database Parameters: Name Type Description Default input_file Path semsim profile required subject_prefix str Subject Prefix. e.g HP required object_prefix str Object Prefix. e.g MP required Source code in src/pheval/infra/exomiserdb.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def import_from_semsim_file ( self , input_file : Path , subject_prefix : str , object_prefix : str ): \"\"\"imports semsim tsv profile into exomiser phenotype database Args: input_file (Path): semsim profile subject_prefix (str): Subject Prefix. e.g HP object_prefix (str): Object Prefix. e.g MP \"\"\" with self . connector as cnn : conn = DBConnection ( cnn ) reader = pl . read_csv_batched ( input_file , separator = \" \\t \" ) batch_length = 5 batches = reader . next_batches ( batch_length ) cursor = conn . get_cursor () # # TODO: Refactor this with open ( input_file , \"r\" ) as f : total = sum ( 1 for line in f ) pbar = tqdm ( total = total - 1 ) mapping_id = 1 while batches : input_data = pl . concat ( batches ) sql = _semsim2h2 ( input_data , object_prefix , subject_prefix , mapping_id = mapping_id ) cursor . execute ( sql ) len_input_data = len ( input_data ) mapping_id += len_input_data pbar . update ( len_input_data ) batches = reader . next_batches ( batch_length )","title":"Exomiserdb"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.DBConnection","text":"Source code in src/pheval/infra/exomiserdb.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class DBConnection : connection = None def __init__ ( self , connection ): DBConnection . connection = connection @classmethod def get_connection ( cls ) -> jaydebeapi . Connection : \"\"\"Creates return new Singleton database connection\"\"\" return DBConnection . connection def close ( self ): return self . connection . close () @classmethod def get_cursor ( cls ) -> jaydebeapi . Cursor : connection = cls . get_connection () return connection . cursor ()","title":"DBConnection"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.DBConnection.get_connection","text":"Creates return new Singleton database connection Source code in src/pheval/infra/exomiserdb.py 49 50 51 52 @classmethod def get_connection ( cls ) -> jaydebeapi . Connection : \"\"\"Creates return new Singleton database connection\"\"\" return DBConnection . connection","title":"get_connection"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.DBConnector","text":"Source code in src/pheval/infra/exomiserdb.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class DBConnector : def __init__ ( self , jar : Path , driver : str , server : str , database : str , user : str , password : str ): self . jar = jar self . driver = driver self . server = server self . database = database self . user = user self . password = password self . dbconn = None def create_connection ( self ) -> jaydebeapi . Connection : \"\"\"creates h2 database connection\"\"\" return jaydebeapi . connect ( self . driver , f \" { self . server }{ self . database } \" , [ self . user , self . password ], self . jar , ) def __enter__ ( self ) -> jaydebeapi . Connection : self . dbconn = self . create_connection () return self . dbconn def __exit__ ( self , * other ): self . dbconn . close ()","title":"DBConnector"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.DBConnector.create_connection","text":"creates h2 database connection Source code in src/pheval/infra/exomiserdb.py 26 27 28 29 30 31 32 33 def create_connection ( self ) -> jaydebeapi . Connection : \"\"\"creates h2 database connection\"\"\" return jaydebeapi . connect ( self . driver , f \" { self . server }{ self . database } \" , [ self . user , self . password ], self . jar , )","title":"create_connection"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.ExomiserDB","text":"Source code in src/pheval/infra/exomiserdb.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class ExomiserDB : def __init__ ( self , db_path : Path ): try : self . connector = DBConnector ( # noqa jar = os . path . join ( os . path . dirname ( __file__ ), \"../../../lib/h2-1.4.199.jar\" ), driver = \"org.h2.Driver\" , server = f \"jdbc:h2: { db_path } \" , user = \"sa\" , password = \"\" , database = \"\" , ) except Exception as e : print ( \"An exception occurred\" , e ) def import_from_semsim_file ( self , input_file : Path , subject_prefix : str , object_prefix : str ): \"\"\"imports semsim tsv profile into exomiser phenotype database Args: input_file (Path): semsim profile subject_prefix (str): Subject Prefix. e.g HP object_prefix (str): Object Prefix. e.g MP \"\"\" with self . connector as cnn : conn = DBConnection ( cnn ) reader = pl . read_csv_batched ( input_file , separator = \" \\t \" ) batch_length = 5 batches = reader . next_batches ( batch_length ) cursor = conn . get_cursor () # # TODO: Refactor this with open ( input_file , \"r\" ) as f : total = sum ( 1 for line in f ) pbar = tqdm ( total = total - 1 ) mapping_id = 1 while batches : input_data = pl . concat ( batches ) sql = _semsim2h2 ( input_data , object_prefix , subject_prefix , mapping_id = mapping_id ) cursor . execute ( sql ) len_input_data = len ( input_data ) mapping_id += len_input_data pbar . update ( len_input_data ) batches = reader . next_batches ( batch_length )","title":"ExomiserDB"},{"location":"api/pheval/infra/exomiserdb/#src.pheval.infra.exomiserdb.ExomiserDB.import_from_semsim_file","text":"imports semsim tsv profile into exomiser phenotype database Parameters: Name Type Description Default input_file Path semsim profile required subject_prefix str Subject Prefix. e.g HP required object_prefix str Object Prefix. e.g MP required Source code in src/pheval/infra/exomiserdb.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def import_from_semsim_file ( self , input_file : Path , subject_prefix : str , object_prefix : str ): \"\"\"imports semsim tsv profile into exomiser phenotype database Args: input_file (Path): semsim profile subject_prefix (str): Subject Prefix. e.g HP object_prefix (str): Object Prefix. e.g MP \"\"\" with self . connector as cnn : conn = DBConnection ( cnn ) reader = pl . read_csv_batched ( input_file , separator = \" \\t \" ) batch_length = 5 batches = reader . next_batches ( batch_length ) cursor = conn . get_cursor () # # TODO: Refactor this with open ( input_file , \"r\" ) as f : total = sum ( 1 for line in f ) pbar = tqdm ( total = total - 1 ) mapping_id = 1 while batches : input_data = pl . concat ( batches ) sql = _semsim2h2 ( input_data , object_prefix , subject_prefix , mapping_id = mapping_id ) cursor . execute ( sql ) len_input_data = len ( input_data ) mapping_id += len_input_data pbar . update ( len_input_data ) batches = reader . next_batches ( batch_length )","title":"import_from_semsim_file"},{"location":"api/pheval/post_processing/mondo_mapping/","text":"map_disease_id ( disease_identifier , mondo_mapping_table ) Map a disease identifier to MONDO ID using the Mondo SSSOM mapping. Args: disease_identifier (str): The disease identifier to map to MONDO. mondo_mapping_table (pl.DataFrame): The Mondo SSSOM table. Returns: str: The MONDO ID. Source code in src/pheval/post_processing/mondo_mapping.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def map_disease_id ( disease_identifier : str , mondo_mapping_table : pl . DataFrame ) -> str : \"\"\" Map a disease identifier to MONDO ID using the Mondo SSSOM mapping. Args: disease_identifier (str): The disease identifier to map to MONDO. mondo_mapping_table (pl.DataFrame): The Mondo SSSOM table. Returns: str: The MONDO ID. \"\"\" mapped_identifier = mondo_mapping_table . filter ( pl . col ( \"object_id\" ) == disease_identifier ) if mapped_identifier . height > 0 : return mapped_identifier [ \"subject_id\" ] . item () return disease_identifier parse_mondo_mapping_table () Parse the Mondo SSSOM table. Returns: pl.DataFrame: Mondo SSSOM table. Source code in src/pheval/post_processing/mondo_mapping.py 6 7 8 9 10 11 12 13 14 15 16 def parse_mondo_mapping_table () -> pl . DataFrame : \"\"\" Parse the Mondo SSSOM table. Returns: pl.DataFrame: Mondo SSSOM table. \"\"\" return pl . read_csv ( Path ( __file__ ) . parent . parent / \"resources\" / \"mondo.sssom.tsv\" , separator = \" \\t \" , comment_prefix = \"#\" , )","title":"Mondo mapping"},{"location":"api/pheval/post_processing/mondo_mapping/#src.pheval.post_processing.mondo_mapping.map_disease_id","text":"Map a disease identifier to MONDO ID using the Mondo SSSOM mapping. Args: disease_identifier (str): The disease identifier to map to MONDO. mondo_mapping_table (pl.DataFrame): The Mondo SSSOM table. Returns: str: The MONDO ID. Source code in src/pheval/post_processing/mondo_mapping.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def map_disease_id ( disease_identifier : str , mondo_mapping_table : pl . DataFrame ) -> str : \"\"\" Map a disease identifier to MONDO ID using the Mondo SSSOM mapping. Args: disease_identifier (str): The disease identifier to map to MONDO. mondo_mapping_table (pl.DataFrame): The Mondo SSSOM table. Returns: str: The MONDO ID. \"\"\" mapped_identifier = mondo_mapping_table . filter ( pl . col ( \"object_id\" ) == disease_identifier ) if mapped_identifier . height > 0 : return mapped_identifier [ \"subject_id\" ] . item () return disease_identifier","title":"map_disease_id"},{"location":"api/pheval/post_processing/mondo_mapping/#src.pheval.post_processing.mondo_mapping.parse_mondo_mapping_table","text":"Parse the Mondo SSSOM table. Returns: pl.DataFrame: Mondo SSSOM table. Source code in src/pheval/post_processing/mondo_mapping.py 6 7 8 9 10 11 12 13 14 15 16 def parse_mondo_mapping_table () -> pl . DataFrame : \"\"\" Parse the Mondo SSSOM table. Returns: pl.DataFrame: Mondo SSSOM table. \"\"\" return pl . read_csv ( Path ( __file__ ) . parent . parent / \"resources\" / \"mondo.sssom.tsv\" , separator = \" \\t \" , comment_prefix = \"#\" , )","title":"parse_mondo_mapping_table"},{"location":"api/pheval/post_processing/phenopacket_truth_set/","text":"PhenopacketTruthSet Class for finding the causative gene/disease/variant from a phenopacket Source code in src/pheval/post_processing/phenopacket_truth_set.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class PhenopacketTruthSet : \"\"\"Class for finding the causative gene/disease/variant from a phenopacket\"\"\" def __init__ ( self , phenopacket_dir : Path ): self . phenopacket_dir = phenopacket_dir def _get_phenopacket_path ( self , phenopacket_name : str ) -> Path : \"\"\" Get the phenopacket path for a given phenopacket name. Args: phenopacket_name (str): Name of the phenopacket. Returns: Path: Path to the phenopacket path. \"\"\" phenopacket_path = self . phenopacket_dir . joinpath ( f \" { phenopacket_name } .json\" ) if not phenopacket_path . exists (): raise FileNotFoundError ( phenopacket_name + \" not found in corpus!\" ) return phenopacket_path def _get_phenopacket_util ( self , phenopacket_name : str ) -> PhenopacketUtil : \"\"\" Get the phenopacket util for a given phenopacket name. Args: phenopacket_name (str): Name of the phenopacket. Returns: PhenopacketUtil: PhenopacketUtil object. \"\"\" phenopacket_path = self . _get_phenopacket_path ( phenopacket_name ) phenopacket = phenopacket_reader ( phenopacket_path ) return PhenopacketUtil ( phenopacket ) def _get_causative_genes ( self , phenopacket_name : str ) -> List [ ProbandCausativeGene ]: \"\"\" Get the causative genes for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[ProbandCausativeGene]: List of ProbandCausativeGene. \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnosed_genes () def _get_causative_variants ( self , phenopacket_name : str ) -> List [ GenomicVariant ]: \"\"\" Get the causative variants for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[GenomicVariant]: List of GenomicVariant. \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnosed_variants () def _get_causative_diseases ( self , phenopacket_name : str ) -> List [ ProbandDisease ]: \"\"\" Get the diseases for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[ProbandDisease]: List of ProbandDisease \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnoses () def classified_gene ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. \"\"\" causative_genes = self . _get_causative_genes ( result_name ) gene_symbols = [ causative_gene . gene_symbol for causative_gene in causative_genes ] gene_identifiers = [ causative_gene . gene_identifier for causative_gene in causative_genes ] return pl . DataFrame ( { \"gene_symbol\" : [ g for g in gene_symbols ], \"gene_identifier\" : [ g for g in gene_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) @staticmethod def merge_gene_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( ( pl . col ( \"gene_symbol\" ) . is_in ( classified_results [ \"gene_symbol\" ]) | pl . col ( \"gene_identifier\" ) . is_in ( classified_results [ \"gene_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"gene_symbol\" ) . is_in ( ranked_results [ \"gene_symbol\" ]) ) ) ) def classified_variant ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. \"\"\" variants = self . _get_causative_variants ( result_name ) return pl . DataFrame ( { \"chrom\" : [ v . chrom for v in variants ], \"start\" : [ v . pos for v in variants ], \"end\" : [ calculate_end_pos ( v . pos , v . ref ) for v in variants ], \"ref\" : [ v . ref for v in variants ], \"alt\" : [ v . alt for v in variants ], } ) . with_columns ( [ pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ), pl . lit ( 0.0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) @staticmethod def merge_variant_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( [ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( classified_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) . alias ( \"true_positive\" ) ] ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( ranked_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) ) ) ) def classified_disease ( self , result_name : str , mondo_mapping_table : pl . DataFrame ) -> pl . DataFrame : \"\"\" Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. \"\"\" diseases = self . _get_causative_diseases ( result_name ) disease_identifiers = list ( set ( disease . disease_identifier for disease in diseases )) return pl . DataFrame ( { \"disease_identifier\" : [ d for d in disease_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . Utf8 ) . alias ( \"mondo_identifier\" ), ] ) @staticmethod def merge_disease_results ( ranked_results : pl . DataFrame , output_file : Path , mondo_mapping_table : pl . DataFrame , ) -> pl . DataFrame : \"\"\" Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. \"\"\" classified_results = pl . read_parquet ( output_file ) ranked_results = ranked_results . with_columns ( [ pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . String , ) . alias ( \"mondo_identifier\" ) ] ) return ( ranked_results . with_columns ( ( pl . col ( \"disease_identifier\" ) . is_in ( classified_results [ \"disease_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"disease_identifier\" ) . is_in ( ranked_results [ \"disease_identifier\" ]) ) ) ) classified_disease ( result_name , mondo_mapping_table ) Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def classified_disease ( self , result_name : str , mondo_mapping_table : pl . DataFrame ) -> pl . DataFrame : \"\"\" Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. \"\"\" diseases = self . _get_causative_diseases ( result_name ) disease_identifiers = list ( set ( disease . disease_identifier for disease in diseases )) return pl . DataFrame ( { \"disease_identifier\" : [ d for d in disease_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . Utf8 ) . alias ( \"mondo_identifier\" ), ] ) classified_gene ( result_name ) Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def classified_gene ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. \"\"\" causative_genes = self . _get_causative_genes ( result_name ) gene_symbols = [ causative_gene . gene_symbol for causative_gene in causative_genes ] gene_identifiers = [ causative_gene . gene_identifier for causative_gene in causative_genes ] return pl . DataFrame ( { \"gene_symbol\" : [ g for g in gene_symbols ], \"gene_identifier\" : [ g for g in gene_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) classified_variant ( result_name ) Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def classified_variant ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. \"\"\" variants = self . _get_causative_variants ( result_name ) return pl . DataFrame ( { \"chrom\" : [ v . chrom for v in variants ], \"start\" : [ v . pos for v in variants ], \"end\" : [ calculate_end_pos ( v . pos , v . ref ) for v in variants ], \"ref\" : [ v . ref for v in variants ], \"alt\" : [ v . alt for v in variants ], } ) . with_columns ( [ pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ), pl . lit ( 0.0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) merge_disease_results ( ranked_results , output_file , mondo_mapping_table ) staticmethod Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @staticmethod def merge_disease_results ( ranked_results : pl . DataFrame , output_file : Path , mondo_mapping_table : pl . DataFrame , ) -> pl . DataFrame : \"\"\" Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. \"\"\" classified_results = pl . read_parquet ( output_file ) ranked_results = ranked_results . with_columns ( [ pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . String , ) . alias ( \"mondo_identifier\" ) ] ) return ( ranked_results . with_columns ( ( pl . col ( \"disease_identifier\" ) . is_in ( classified_results [ \"disease_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"disease_identifier\" ) . is_in ( ranked_results [ \"disease_identifier\" ]) ) ) ) merge_gene_results ( ranked_results , output_file ) staticmethod Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod def merge_gene_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( ( pl . col ( \"gene_symbol\" ) . is_in ( classified_results [ \"gene_symbol\" ]) | pl . col ( \"gene_identifier\" ) . is_in ( classified_results [ \"gene_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"gene_symbol\" ) . is_in ( ranked_results [ \"gene_symbol\" ]) ) ) ) merge_variant_results ( ranked_results , output_file ) staticmethod Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def merge_variant_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( [ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( classified_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) . alias ( \"true_positive\" ) ] ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( ranked_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) ) ) ) calculate_end_pos ( variant_start , variant_ref ) Calculate the end position for a variant Args: variant_start (int): The start position of the variant variant_ref (str): The reference allele of the variant Returns: Name Type Description int int The end position of the variant Source code in src/pheval/post_processing/phenopacket_truth_set.py 16 17 18 19 20 21 22 23 24 25 def calculate_end_pos ( variant_start : int , variant_ref : str ) -> int : \"\"\"Calculate the end position for a variant Args: variant_start (int): The start position of the variant variant_ref (str): The reference allele of the variant Returns: int: The end position of the variant \"\"\" return variant_start + len ( variant_ref ) - 1","title":"Phenopacket truth set"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet","text":"Class for finding the causative gene/disease/variant from a phenopacket Source code in src/pheval/post_processing/phenopacket_truth_set.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class PhenopacketTruthSet : \"\"\"Class for finding the causative gene/disease/variant from a phenopacket\"\"\" def __init__ ( self , phenopacket_dir : Path ): self . phenopacket_dir = phenopacket_dir def _get_phenopacket_path ( self , phenopacket_name : str ) -> Path : \"\"\" Get the phenopacket path for a given phenopacket name. Args: phenopacket_name (str): Name of the phenopacket. Returns: Path: Path to the phenopacket path. \"\"\" phenopacket_path = self . phenopacket_dir . joinpath ( f \" { phenopacket_name } .json\" ) if not phenopacket_path . exists (): raise FileNotFoundError ( phenopacket_name + \" not found in corpus!\" ) return phenopacket_path def _get_phenopacket_util ( self , phenopacket_name : str ) -> PhenopacketUtil : \"\"\" Get the phenopacket util for a given phenopacket name. Args: phenopacket_name (str): Name of the phenopacket. Returns: PhenopacketUtil: PhenopacketUtil object. \"\"\" phenopacket_path = self . _get_phenopacket_path ( phenopacket_name ) phenopacket = phenopacket_reader ( phenopacket_path ) return PhenopacketUtil ( phenopacket ) def _get_causative_genes ( self , phenopacket_name : str ) -> List [ ProbandCausativeGene ]: \"\"\" Get the causative genes for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[ProbandCausativeGene]: List of ProbandCausativeGene. \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnosed_genes () def _get_causative_variants ( self , phenopacket_name : str ) -> List [ GenomicVariant ]: \"\"\" Get the causative variants for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[GenomicVariant]: List of GenomicVariant. \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnosed_variants () def _get_causative_diseases ( self , phenopacket_name : str ) -> List [ ProbandDisease ]: \"\"\" Get the diseases for a given phenopacket. Args: phenopacket_name (str): Name of the phenopacket. Returns: List[ProbandDisease]: List of ProbandDisease \"\"\" phenopacket_util = self . _get_phenopacket_util ( phenopacket_name ) return phenopacket_util . diagnoses () def classified_gene ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. \"\"\" causative_genes = self . _get_causative_genes ( result_name ) gene_symbols = [ causative_gene . gene_symbol for causative_gene in causative_genes ] gene_identifiers = [ causative_gene . gene_identifier for causative_gene in causative_genes ] return pl . DataFrame ( { \"gene_symbol\" : [ g for g in gene_symbols ], \"gene_identifier\" : [ g for g in gene_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) @staticmethod def merge_gene_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( ( pl . col ( \"gene_symbol\" ) . is_in ( classified_results [ \"gene_symbol\" ]) | pl . col ( \"gene_identifier\" ) . is_in ( classified_results [ \"gene_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"gene_symbol\" ) . is_in ( ranked_results [ \"gene_symbol\" ]) ) ) ) def classified_variant ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. \"\"\" variants = self . _get_causative_variants ( result_name ) return pl . DataFrame ( { \"chrom\" : [ v . chrom for v in variants ], \"start\" : [ v . pos for v in variants ], \"end\" : [ calculate_end_pos ( v . pos , v . ref ) for v in variants ], \"ref\" : [ v . ref for v in variants ], \"alt\" : [ v . alt for v in variants ], } ) . with_columns ( [ pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ), pl . lit ( 0.0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] ) @staticmethod def merge_variant_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( [ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( classified_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) . alias ( \"true_positive\" ) ] ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( ranked_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) ) ) ) def classified_disease ( self , result_name : str , mondo_mapping_table : pl . DataFrame ) -> pl . DataFrame : \"\"\" Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. \"\"\" diseases = self . _get_causative_diseases ( result_name ) disease_identifiers = list ( set ( disease . disease_identifier for disease in diseases )) return pl . DataFrame ( { \"disease_identifier\" : [ d for d in disease_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . Utf8 ) . alias ( \"mondo_identifier\" ), ] ) @staticmethod def merge_disease_results ( ranked_results : pl . DataFrame , output_file : Path , mondo_mapping_table : pl . DataFrame , ) -> pl . DataFrame : \"\"\" Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. \"\"\" classified_results = pl . read_parquet ( output_file ) ranked_results = ranked_results . with_columns ( [ pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . String , ) . alias ( \"mondo_identifier\" ) ] ) return ( ranked_results . with_columns ( ( pl . col ( \"disease_identifier\" ) . is_in ( classified_results [ \"disease_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"disease_identifier\" ) . is_in ( ranked_results [ \"disease_identifier\" ]) ) ) )","title":"PhenopacketTruthSet"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.classified_disease","text":"Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def classified_disease ( self , result_name : str , mondo_mapping_table : pl . DataFrame ) -> pl . DataFrame : \"\"\" Classify disease results for a given phenopacket. Args: result_name (str): Name of the result file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Classified ranked disease results. \"\"\" diseases = self . _get_causative_diseases ( result_name ) disease_identifiers = list ( set ( disease . disease_identifier for disease in diseases )) return pl . DataFrame ( { \"disease_identifier\" : [ d for d in disease_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . Utf8 ) . alias ( \"mondo_identifier\" ), ] )","title":"classified_disease"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.classified_gene","text":"Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def classified_gene ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classify gene results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked gene results. \"\"\" causative_genes = self . _get_causative_genes ( result_name ) gene_symbols = [ causative_gene . gene_symbol for causative_gene in causative_genes ] gene_identifiers = [ causative_gene . gene_identifier for causative_gene in causative_genes ] return pl . DataFrame ( { \"gene_symbol\" : [ g for g in gene_symbols ], \"gene_identifier\" : [ g for g in gene_identifiers ], } ) . with_columns ( [ pl . lit ( 0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] )","title":"classified_gene"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.classified_variant","text":"Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def classified_variant ( self , result_name : str ) -> pl . DataFrame : \"\"\" Classified variant results for a given phenopacket. Args: result_name (str): Name of the result file. Returns: pl.DataFrame: Classified ranked variant results. \"\"\" variants = self . _get_causative_variants ( result_name ) return pl . DataFrame ( { \"chrom\" : [ v . chrom for v in variants ], \"start\" : [ v . pos for v in variants ], \"end\" : [ calculate_end_pos ( v . pos , v . ref ) for v in variants ], \"ref\" : [ v . ref for v in variants ], \"alt\" : [ v . alt for v in variants ], } ) . with_columns ( [ pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ), pl . lit ( 0.0 ) . cast ( pl . Float64 ) . alias ( \"score\" ), pl . lit ( 0 ) . cast ( pl . Int64 ) . alias ( \"rank\" ), pl . lit ( True ) . alias ( \"true_positive\" ), ] )","title":"classified_variant"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.merge_disease_results","text":"Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @staticmethod def merge_disease_results ( ranked_results : pl . DataFrame , output_file : Path , mondo_mapping_table : pl . DataFrame , ) -> pl . DataFrame : \"\"\" Merge ranked disease results with the classified diseases. Args: ranked_results (pl.DataFrame): Ranked disease results. output_file (Path): Path to the output file. mondo_mapping_table (pl.DataFrame): Mondo mapping table. Returns: pl.DataFrame: Merged ranked disease results. \"\"\" classified_results = pl . read_parquet ( output_file ) ranked_results = ranked_results . with_columns ( [ pl . col ( \"disease_identifier\" ) . map_elements ( lambda x : map_disease_id ( x , mondo_mapping_table ), return_dtype = pl . String , ) . alias ( \"mondo_identifier\" ) ] ) return ( ranked_results . with_columns ( ( pl . col ( \"disease_identifier\" ) . is_in ( classified_results [ \"disease_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"disease_identifier\" ) . is_in ( ranked_results [ \"disease_identifier\" ]) ) ) )","title":"merge_disease_results"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.merge_gene_results","text":"Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod def merge_gene_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked gene results with the classified genes. Args: ranked_results (pl.DataFrame): Ranked gene results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked gene results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( ( pl . col ( \"gene_symbol\" ) . is_in ( classified_results [ \"gene_symbol\" ]) | pl . col ( \"gene_identifier\" ) . is_in ( classified_results [ \"gene_identifier\" ]) ) . alias ( \"true_positive\" ) ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . col ( \"gene_symbol\" ) . is_in ( ranked_results [ \"gene_symbol\" ]) ) ) )","title":"merge_gene_results"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.PhenopacketTruthSet.merge_variant_results","text":"Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. Source code in src/pheval/post_processing/phenopacket_truth_set.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def merge_variant_results ( ranked_results : pl . DataFrame , output_file : Path ) -> pl . DataFrame : \"\"\" Merge ranked variant results with the classified variants. Args: ranked_results (pl.DataFrame): Ranked variant results. output_file (Path): Path to the output file. Returns: pl.DataFrame: Merged ranked variant results. \"\"\" classified_results = pl . read_parquet ( output_file ) return ( ranked_results . with_columns ( [ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( classified_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) . alias ( \"true_positive\" ) ] ) . with_columns ( pl . col ( \"rank\" ) . cast ( pl . Int64 )) . select ( classified_results . columns ) . vstack ( classified_results . filter ( ~ pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) . is_in ( ranked_results . select ( pl . struct ([ \"chrom\" , \"start\" , \"end\" , \"ref\" , \"alt\" ]) ) . to_series () ) ) ) )","title":"merge_variant_results"},{"location":"api/pheval/post_processing/phenopacket_truth_set/#src.pheval.post_processing.phenopacket_truth_set.calculate_end_pos","text":"Calculate the end position for a variant Args: variant_start (int): The start position of the variant variant_ref (str): The reference allele of the variant Returns: Name Type Description int int The end position of the variant Source code in src/pheval/post_processing/phenopacket_truth_set.py 16 17 18 19 20 21 22 23 24 25 def calculate_end_pos ( variant_start : int , variant_ref : str ) -> int : \"\"\"Calculate the end position for a variant Args: variant_start (int): The start position of the variant variant_ref (str): The reference allele of the variant Returns: int: The end position of the variant \"\"\" return variant_start + len ( variant_ref ) - 1","title":"calculate_end_pos"},{"location":"api/pheval/post_processing/post_processing/","text":"ResultType Bases: Enum Enumeration of the possible result types. Source code in src/pheval/post_processing/post_processing.py 20 21 22 23 24 25 class ResultType ( Enum ): \"\"\"Enumeration of the possible result types.\"\"\" GENE = \"gene\" DISEASE = \"disease\" VARIANT = \"variant\" SortOrder Bases: Enum Enumeration representing sorting orders. Source code in src/pheval/post_processing/post_processing.py 28 29 30 31 32 33 34 class SortOrder ( Enum ): \"\"\"Enumeration representing sorting orders.\"\"\" ASCENDING = 1 \"\"\"Ascending sort order.\"\"\" DESCENDING = 2 \"\"\"Descending sort order.\"\"\" ASCENDING = 1 class-attribute instance-attribute Ascending sort order. DESCENDING = 2 class-attribute instance-attribute Descending sort order. create_empty_pheval_result ( phenopacket_dir , output_dir , result_type ) Create an empty PhEval result for a given result type (gene, variant, or disease). Notes This is necessary because some tools may not generate a result output for certain cases. By explicitly creating an empty result, which will contain the known entity with a rank and score of 0, we can track and identify false negatives during benchmarking, ensuring that missing predictions are accounted for in the evaluation. Parameters: Name Type Description Default phenopacket_dir Path The directory containing the phenopackets. required output_dir Path The output directory. required result_type ResultType The result type. required Source code in src/pheval/post_processing/post_processing.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def create_empty_pheval_result ( phenopacket_dir : Path , output_dir : Path , result_type : ResultType ) -> None : \"\"\" Create an empty PhEval result for a given result type (gene, variant, or disease). Notes: This is necessary because some tools may not generate a result output for certain cases. By explicitly creating an empty result, which will contain the known entity with a rank and score of 0, we can track and identify false negatives during benchmarking, ensuring that missing predictions are accounted for in the evaluation. Args: phenopacket_dir (Path): The directory containing the phenopackets. output_dir (Path): The output directory. result_type (ResultType): The result type. \"\"\" if result_type in executed_results : return logger . info ( f \"Writing classified results for { len ( all_files ( phenopacket_dir )) } \" f \"phenopackets to { output_dir } \" ) executed_results . add ( result_type ) phenopacket_truth_set = PhenopacketTruthSet ( phenopacket_dir ) classify_method , write_method = _get_result_type ( result_type , phenopacket_truth_set ) for file in all_files ( phenopacket_dir ): classified_results = classify_method ( file . stem ) write_method ( classified_results , output_dir . joinpath ( f \" { file . stem } - { result_type . value } _result.parquet\" ), ) generate_disease_result ( results , sort_order , output_dir , result_path , phenopacket_dir ) Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @validate_dataframe ( ResultSchema . DISEASE_RESULT_SCHEMA ) def generate_disease_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_disease_results/ { result_path . stem } -disease_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_disease_results\" ), ResultType . DISEASE , ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_disease_results ( ranked_results , output_file , mondo_mapping_table ) _write_disease_result ( classified_results , output_file ) generate_gene_result ( results , sort_order , output_dir , result_path , phenopacket_dir ) Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @validate_dataframe ( ResultSchema . GENE_RESULT_SCHEMA ) def generate_gene_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_gene_results/ { result_path . stem } -gene_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_gene_results\" ), ResultType . GENE ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_gene_results ( ranked_results , output_file ) _write_gene_result ( classified_results , output_file ) generate_variant_result ( results , sort_order , output_dir , result_path , phenopacket_dir ) Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @validate_dataframe ( ResultSchema . VARIANT_RESULT_SCHEMA ) def generate_variant_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_variant_results/ { result_path . stem } -variant_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_variant_results\" ), ResultType . VARIANT , ) ranked_results = _rank_results ( results , sort_order ) . with_columns ( pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ) ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_variant_results ( ranked_results , output_file ) _write_variant_result ( classified_results , output_file )","title":"Post processing"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.ResultType","text":"Bases: Enum Enumeration of the possible result types. Source code in src/pheval/post_processing/post_processing.py 20 21 22 23 24 25 class ResultType ( Enum ): \"\"\"Enumeration of the possible result types.\"\"\" GENE = \"gene\" DISEASE = \"disease\" VARIANT = \"variant\"","title":"ResultType"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.SortOrder","text":"Bases: Enum Enumeration representing sorting orders. Source code in src/pheval/post_processing/post_processing.py 28 29 30 31 32 33 34 class SortOrder ( Enum ): \"\"\"Enumeration representing sorting orders.\"\"\" ASCENDING = 1 \"\"\"Ascending sort order.\"\"\" DESCENDING = 2 \"\"\"Descending sort order.\"\"\"","title":"SortOrder"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.SortOrder.ASCENDING","text":"Ascending sort order.","title":"ASCENDING"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.SortOrder.DESCENDING","text":"Descending sort order.","title":"DESCENDING"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.create_empty_pheval_result","text":"Create an empty PhEval result for a given result type (gene, variant, or disease). Notes This is necessary because some tools may not generate a result output for certain cases. By explicitly creating an empty result, which will contain the known entity with a rank and score of 0, we can track and identify false negatives during benchmarking, ensuring that missing predictions are accounted for in the evaluation. Parameters: Name Type Description Default phenopacket_dir Path The directory containing the phenopackets. required output_dir Path The output directory. required result_type ResultType The result type. required Source code in src/pheval/post_processing/post_processing.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def create_empty_pheval_result ( phenopacket_dir : Path , output_dir : Path , result_type : ResultType ) -> None : \"\"\" Create an empty PhEval result for a given result type (gene, variant, or disease). Notes: This is necessary because some tools may not generate a result output for certain cases. By explicitly creating an empty result, which will contain the known entity with a rank and score of 0, we can track and identify false negatives during benchmarking, ensuring that missing predictions are accounted for in the evaluation. Args: phenopacket_dir (Path): The directory containing the phenopackets. output_dir (Path): The output directory. result_type (ResultType): The result type. \"\"\" if result_type in executed_results : return logger . info ( f \"Writing classified results for { len ( all_files ( phenopacket_dir )) } \" f \"phenopackets to { output_dir } \" ) executed_results . add ( result_type ) phenopacket_truth_set = PhenopacketTruthSet ( phenopacket_dir ) classify_method , write_method = _get_result_type ( result_type , phenopacket_truth_set ) for file in all_files ( phenopacket_dir ): classified_results = classify_method ( file . stem ) write_method ( classified_results , output_dir . joinpath ( f \" { file . stem } - { result_type . value } _result.parquet\" ), )","title":"create_empty_pheval_result"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.generate_disease_result","text":"Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @validate_dataframe ( ResultSchema . DISEASE_RESULT_SCHEMA ) def generate_disease_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval disease results to a compressed Parquet output. Args: results (pl.DataFrame): The disease results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_disease_results/ { result_path . stem } -disease_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_disease_results\" ), ResultType . DISEASE , ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_disease_results ( ranked_results , output_file , mondo_mapping_table ) _write_disease_result ( classified_results , output_file )","title":"generate_disease_result"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.generate_gene_result","text":"Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @validate_dataframe ( ResultSchema . GENE_RESULT_SCHEMA ) def generate_gene_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval gene results to a compressed Parquet output. Args: results (pl.DataFrame): The gene results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_gene_results/ { result_path . stem } -gene_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_gene_results\" ), ResultType . GENE ) ranked_results = _rank_results ( results , sort_order ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_gene_results ( ranked_results , output_file ) _write_gene_result ( classified_results , output_file )","title":"generate_gene_result"},{"location":"api/pheval/post_processing/post_processing/#src.pheval.post_processing.post_processing.generate_variant_result","text":"Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory Source code in src/pheval/post_processing/post_processing.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @validate_dataframe ( ResultSchema . VARIANT_RESULT_SCHEMA ) def generate_variant_result ( results : pl . DataFrame , sort_order : SortOrder , output_dir : Path , result_path : Path , phenopacket_dir : Path , ) -> None : \"\"\" Generate PhEval variant results to a compressed Parquet output. Args: results (pl.DataFrame): The variant results. sort_order (SortOrder): The sort order to use. output_dir (Path): Path to the output directory result_path (Path): Path to the tool-specific result file. phenopacket_dir (Path): Path to the Phenopacket directory \"\"\" output_file = output_dir . joinpath ( f \"pheval_variant_results/ { result_path . stem } -variant_result.parquet\" ) create_empty_pheval_result ( phenopacket_dir , output_dir . joinpath ( \"pheval_variant_results\" ), ResultType . VARIANT , ) ranked_results = _rank_results ( results , sort_order ) . with_columns ( pl . concat_str ([ \"chrom\" , \"start\" , \"ref\" , \"alt\" ], separator = \"-\" ) . alias ( \"variant_id\" ) ) classified_results = PhenopacketTruthSet ( phenopacket_dir ) . merge_variant_results ( ranked_results , output_file ) _write_variant_result ( classified_results , output_file )","title":"generate_variant_result"},{"location":"api/pheval/post_processing/validate_result_format/","text":"ResultSchema Bases: Enum Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. Source code in src/pheval/post_processing/validate_result_format.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ResultSchema ( Enum ): \"\"\" Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. \"\"\" GENE_RESULT_SCHEMA = pl . Schema ( { \"gene_symbol\" : pl . String , \"gene_identifier\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) VARIANT_RESULT_SCHEMA = pl . Schema ( { \"chrom\" : pl . String , \"start\" : pl . Int64 , \"end\" : pl . Int64 , \"ref\" : pl . String , \"alt\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) DISEASE_RESULT_SCHEMA = pl . Schema ( { \"disease_identifier\" : pl . String , \"score\" : pl . Float64 , } ) def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True validate ( results ) Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. Source code in src/pheval/post_processing/validate_result_format.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True validate_dataframe ( schema ) Decorator to validate DataFrame input based on a ResultSchema. Args: schema (ResultSchema): The expected schema from the ResultSchema enum. Returns: Callable: A wrapped function that validates the DataFrame before execution. Source code in src/pheval/post_processing/validate_result_format.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def validate_dataframe ( schema : ResultSchema ) -> Callable : \"\"\" Decorator to validate DataFrame input based on a ResultSchema. Args: schema (ResultSchema): The expected schema from the `ResultSchema` enum. Returns: Callable: A wrapped function that validates the DataFrame before execution. \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( results : pl . DataFrame , * args , ** kwargs ): schema . validate ( results ) return func ( results , * args , ** kwargs ) return wrapper return decorator","title":"Validate result format"},{"location":"api/pheval/post_processing/validate_result_format/#src.pheval.post_processing.validate_result_format.ResultSchema","text":"Bases: Enum Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. Source code in src/pheval/post_processing/validate_result_format.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ResultSchema ( Enum ): \"\"\" Enum for different result schema formats. Attributes: GENE_RESULT_SCHEMA (pl.Schema): Schema for gene-based results. VARIANT_RESULT_SCHEMA (pl.Schema): Schema for variant-based results. DISEASE_RESULT_SCHEMA (pl.Schema): Schema for disease-based results. \"\"\" GENE_RESULT_SCHEMA = pl . Schema ( { \"gene_symbol\" : pl . String , \"gene_identifier\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) VARIANT_RESULT_SCHEMA = pl . Schema ( { \"chrom\" : pl . String , \"start\" : pl . Int64 , \"end\" : pl . Int64 , \"ref\" : pl . String , \"alt\" : pl . String , \"score\" : pl . Float64 , \"grouping_id\" : pl . Utf8 , } ) DISEASE_RESULT_SCHEMA = pl . Schema ( { \"disease_identifier\" : pl . String , \"score\" : pl . Float64 , } ) def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True","title":"ResultSchema"},{"location":"api/pheval/post_processing/validate_result_format/#src.pheval.post_processing.validate_result_format.ResultSchema.validate","text":"Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. Source code in src/pheval/post_processing/validate_result_format.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def validate ( self , results : pl . DataFrame ) -> bool : \"\"\" Validate that a DataFrame follows the expected schema. Args: results (pl.DataFrame): The DataFrame to validate. Raises: ValueError: If a required column is missing or the grouping_id column contains a null value. TypeError: If a column exists but has an incorrect data type. Returns: bool: True if the DataFrame is valid according to the schema. \"\"\" expected_schema = self . value if \"grouping_id\" in results . columns and results [ \"grouping_id\" ] . null_count () > 0 : raise ValueError ( \"'grouping_id' column should not contain null values if provided.\" ) for col_name , expected_type in expected_schema . items (): if col_name not in results . schema : if col_name == \"grouping_id\" : continue raise ValueError ( f \"Missing required column: { col_name } \" ) if results . schema [ col_name ] != expected_type : raise TypeError ( f \"Column ' { col_name } ' has type { results . schema [ col_name ] } , expected { expected_type } \" ) return True","title":"validate"},{"location":"api/pheval/post_processing/validate_result_format/#src.pheval.post_processing.validate_result_format.validate_dataframe","text":"Decorator to validate DataFrame input based on a ResultSchema. Args: schema (ResultSchema): The expected schema from the ResultSchema enum. Returns: Callable: A wrapped function that validates the DataFrame before execution. Source code in src/pheval/post_processing/validate_result_format.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def validate_dataframe ( schema : ResultSchema ) -> Callable : \"\"\" Decorator to validate DataFrame input based on a ResultSchema. Args: schema (ResultSchema): The expected schema from the `ResultSchema` enum. Returns: Callable: A wrapped function that validates the DataFrame before execution. \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( results : pl . DataFrame , * args , ** kwargs ): schema . validate ( results ) return func ( results , * args , ** kwargs ) return wrapper return decorator","title":"validate_dataframe"},{"location":"api/pheval/prepare/create_noisy_phenopackets/","text":"HpoRandomiser Class for randomising phenopacket phenotypic features using Human Phenotype Ontology (HPO). Source code in src/pheval/prepare/create_noisy_phenopackets.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class HpoRandomiser : \"\"\"Class for randomising phenopacket phenotypic features using Human Phenotype Ontology (HPO).\"\"\" def __init__ ( self , hpo_ontology : ProntoImplementation , scramble_factor : float ): \"\"\" Initialise the HpoRandomiser. Args: hpo_ontology (ProntoImplementation): The instance of the HPO ontology. scramble_factor (float): A factor for scrambling phenotypic features. \"\"\" self . hpo_ontology = hpo_ontology self . phenotypic_abnormalities = set ( hpo_ontology . roots ( predicates = [ \"HP:0000118\" ])) self . scramble_factor = scramble_factor def scramble_factor_proportions ( self , phenotypic_features : list [ PhenotypicFeature ]) -> int : \"\"\" Calculate the proportion of scrambled HPO terms based on the scramble factor. Args: phenotypic_features (list[PhenotypicFeature]): List of phenotypic features. Returns: int: The calculated number of phenotypic features to be scrambled. \"\"\" if len ( phenotypic_features ) == 1 : return 1 else : return int ( round ( len ( phenotypic_features ) * self . scramble_factor , 0 )) def retrieve_hpo_term ( self , hpo_id : str ) -> PhenotypicFeature : \"\"\" Retrieve an HPO term based on the provided HPO ID. Args: hpo_id (str): The HPO ID of the term to retrieve. Returns: PhenotypicFeature: The PhenotypicFeature object representing the retrieved HPO term. \"\"\" rels = self . hpo_ontology . entity_alias_map ( hpo_id ) hpo_term = \"\" . join ( rels [( list ( rels . keys ())[ 0 ])]) return PhenotypicFeature ( type = OntologyClass ( id = hpo_id , label = hpo_term )) @staticmethod def retain_real_patient_terms ( phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. \"\"\" if len ( phenotypic_features ) > 1 : number_of_real_id = len ( phenotypic_features ) - number_of_scrambled_terms else : number_of_real_id = 1 return random . sample ( phenotypic_features , number_of_real_id ) def convert_patient_terms_to_parent ( self , phenotypic_features : List [ PhenotypicFeature ], retained_phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Convert a subset of patient HPO terms to their respective parent terms. Args: phenotypic_features (List[PhenotypicFeature]): List of all phenotypic features. retained_phenotypic_features (List[PhenotypicFeature]): List of retained non-scrambled phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note: This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. \"\"\" remaining_hpo = [ i for i in phenotypic_features if i not in retained_phenotypic_features ] if len ( remaining_hpo ) == 0 : number_of_scrambled_terms = 0 hpo_terms_to_be_changed = list ( random . sample ( remaining_hpo , number_of_scrambled_terms )) parent_terms = [] for term in hpo_terms_to_be_changed : if self . hpo_ontology . label ( term . type . id ) . startswith ( \"obsolete\" ): obsolete_term = self . hpo_ontology . entity_metadata_map ( term . type . id ) updated_term = list ( obsolete_term . values ())[ 0 ][ 0 ] parents = self . hpo_ontology . hierarchical_parents ( updated_term ) else : parents = self . hpo_ontology . hierarchical_parents ( term . type . id ) if not parents : parent_terms . append ( term ) else : parent_terms . append ( self . retrieve_hpo_term ( random . choice ( parents ))) return parent_terms def create_random_hpo_terms ( self , number_of_scrambled_terms : int ) -> List [ PhenotypicFeature ]: \"\"\" Generate a list of random HPO terms. Args: number_of_scrambled_terms (int): The count of random HPO terms to be generated. Returns: List[PhenotypicFeature]: A list of randomly selected HPO terms. \"\"\" random_ids = list ( random . sample ( sorted ( self . phenotypic_abnormalities ), number_of_scrambled_terms ) ) return [ self . retrieve_hpo_term ( random_id ) for random_id in random_ids ] def randomise_hpo_terms ( self , phenotypic_features : List [ PhenotypicFeature ], ) -> List [ PhenotypicFeature ]: \"\"\" Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features to be randomised. Returns: List[PhenotypicFeature]: A list of randomised HPO terms. Note: This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. \"\"\" number_of_scrambled_terms = self . scramble_factor_proportions ( phenotypic_features ) retained_patient_terms = self . retain_real_patient_terms ( phenotypic_features , number_of_scrambled_terms ) return ( retained_patient_terms + self . convert_patient_terms_to_parent ( phenotypic_features , retained_patient_terms , number_of_scrambled_terms ) + self . create_random_hpo_terms ( number_of_scrambled_terms ) ) def add_noise_to_phenotypic_profile ( self , phenopacket : Union [ Phenopacket , Family ], ) -> Union [ Phenopacket , Family ]: \"\"\" Randomise the phenotypic profile of a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family to be randomised. Returns: Union[Phenopacket, Family]: The randomised Phenopacket or Family. \"\"\" phenotypic_features = PhenopacketUtil ( phenopacket ) . observed_phenotypic_features () random_phenotypes = self . randomise_hpo_terms ( phenotypic_features ) randomised_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_randomised_hpo ( random_phenotypes ) return randomised_phenopacket def create_scrambled_phenopacket ( self , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Create a scrambled version of a Phenopacket. Args: output_dir (Path): The directory to store the output scrambled Phenopacket. phenopacket_path (Path): The path to the original Phenopacket file. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket , ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name ), ) def create_scrambled_phenopackets ( self , output_dir : Path , phenopacket_dir : Path , ) -> None : \"\"\" Create scrambled versions of Phenopackets within a directory. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_dir (Path): The directory containing the original Phenopacket files. \"\"\" phenopacket_files = files_with_suffix ( phenopacket_dir , \".json\" ) for phenopacket_path in phenopacket_files : logger . info ( f \"Scrambling { phenopacket_path . name } .\" ) phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name , ), ) __init__ ( hpo_ontology , scramble_factor ) Initialise the HpoRandomiser. Parameters: Name Type Description Default hpo_ontology ProntoImplementation The instance of the HPO ontology. required scramble_factor float A factor for scrambling phenotypic features. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , hpo_ontology : ProntoImplementation , scramble_factor : float ): \"\"\" Initialise the HpoRandomiser. Args: hpo_ontology (ProntoImplementation): The instance of the HPO ontology. scramble_factor (float): A factor for scrambling phenotypic features. \"\"\" self . hpo_ontology = hpo_ontology self . phenotypic_abnormalities = set ( hpo_ontology . roots ( predicates = [ \"HP:0000118\" ])) self . scramble_factor = scramble_factor add_noise_to_phenotypic_profile ( phenopacket ) Randomise the phenotypic profile of a Phenopacket or Family. Parameters: Name Type Description Default phenopacket Union [ Phenopacket , Family ] The Phenopacket or Family to be randomised. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The randomised Phenopacket or Family. Source code in src/pheval/prepare/create_noisy_phenopackets.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def add_noise_to_phenotypic_profile ( self , phenopacket : Union [ Phenopacket , Family ], ) -> Union [ Phenopacket , Family ]: \"\"\" Randomise the phenotypic profile of a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family to be randomised. Returns: Union[Phenopacket, Family]: The randomised Phenopacket or Family. \"\"\" phenotypic_features = PhenopacketUtil ( phenopacket ) . observed_phenotypic_features () random_phenotypes = self . randomise_hpo_terms ( phenotypic_features ) randomised_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_randomised_hpo ( random_phenotypes ) return randomised_phenopacket convert_patient_terms_to_parent ( phenotypic_features , retained_phenotypic_features , number_of_scrambled_terms ) Convert a subset of patient HPO terms to their respective parent terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of all phenotypic features. required retained_phenotypic_features List [ PhenotypicFeature ] List of retained non-scrambled phenotypic features. required number_of_scrambled_terms int The count of scrambled HPO terms. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. Source code in src/pheval/prepare/create_noisy_phenopackets.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def convert_patient_terms_to_parent ( self , phenotypic_features : List [ PhenotypicFeature ], retained_phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Convert a subset of patient HPO terms to their respective parent terms. Args: phenotypic_features (List[PhenotypicFeature]): List of all phenotypic features. retained_phenotypic_features (List[PhenotypicFeature]): List of retained non-scrambled phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note: This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. \"\"\" remaining_hpo = [ i for i in phenotypic_features if i not in retained_phenotypic_features ] if len ( remaining_hpo ) == 0 : number_of_scrambled_terms = 0 hpo_terms_to_be_changed = list ( random . sample ( remaining_hpo , number_of_scrambled_terms )) parent_terms = [] for term in hpo_terms_to_be_changed : if self . hpo_ontology . label ( term . type . id ) . startswith ( \"obsolete\" ): obsolete_term = self . hpo_ontology . entity_metadata_map ( term . type . id ) updated_term = list ( obsolete_term . values ())[ 0 ][ 0 ] parents = self . hpo_ontology . hierarchical_parents ( updated_term ) else : parents = self . hpo_ontology . hierarchical_parents ( term . type . id ) if not parents : parent_terms . append ( term ) else : parent_terms . append ( self . retrieve_hpo_term ( random . choice ( parents ))) return parent_terms create_random_hpo_terms ( number_of_scrambled_terms ) Generate a list of random HPO terms. Parameters: Name Type Description Default number_of_scrambled_terms int The count of random HPO terms to be generated. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of randomly selected HPO terms. Source code in src/pheval/prepare/create_noisy_phenopackets.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def create_random_hpo_terms ( self , number_of_scrambled_terms : int ) -> List [ PhenotypicFeature ]: \"\"\" Generate a list of random HPO terms. Args: number_of_scrambled_terms (int): The count of random HPO terms to be generated. Returns: List[PhenotypicFeature]: A list of randomly selected HPO terms. \"\"\" random_ids = list ( random . sample ( sorted ( self . phenotypic_abnormalities ), number_of_scrambled_terms ) ) return [ self . retrieve_hpo_term ( random_id ) for random_id in random_ids ] create_scrambled_phenopacket ( output_dir , phenopacket_path ) Create a scrambled version of a Phenopacket. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopacket. required phenopacket_path Path The path to the original Phenopacket file. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def create_scrambled_phenopacket ( self , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Create a scrambled version of a Phenopacket. Args: output_dir (Path): The directory to store the output scrambled Phenopacket. phenopacket_path (Path): The path to the original Phenopacket file. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket , ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name ), ) create_scrambled_phenopackets ( output_dir , phenopacket_dir ) Create scrambled versions of Phenopackets within a directory. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopackets. required phenopacket_dir Path The directory containing the original Phenopacket files. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def create_scrambled_phenopackets ( self , output_dir : Path , phenopacket_dir : Path , ) -> None : \"\"\" Create scrambled versions of Phenopackets within a directory. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_dir (Path): The directory containing the original Phenopacket files. \"\"\" phenopacket_files = files_with_suffix ( phenopacket_dir , \".json\" ) for phenopacket_path in phenopacket_files : logger . info ( f \"Scrambling { phenopacket_path . name } .\" ) phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name , ), ) randomise_hpo_terms ( phenotypic_features ) Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of phenotypic features to be randomised. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of randomised HPO terms. Note This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. Source code in src/pheval/prepare/create_noisy_phenopackets.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def randomise_hpo_terms ( self , phenotypic_features : List [ PhenotypicFeature ], ) -> List [ PhenotypicFeature ]: \"\"\" Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features to be randomised. Returns: List[PhenotypicFeature]: A list of randomised HPO terms. Note: This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. \"\"\" number_of_scrambled_terms = self . scramble_factor_proportions ( phenotypic_features ) retained_patient_terms = self . retain_real_patient_terms ( phenotypic_features , number_of_scrambled_terms ) return ( retained_patient_terms + self . convert_patient_terms_to_parent ( phenotypic_features , retained_patient_terms , number_of_scrambled_terms ) + self . create_random_hpo_terms ( number_of_scrambled_terms ) ) retain_real_patient_terms ( phenotypic_features , number_of_scrambled_terms ) staticmethod Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of phenotypic features. required number_of_scrambled_terms int The count of scrambled HPO terms. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. Source code in src/pheval/prepare/create_noisy_phenopackets.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @staticmethod def retain_real_patient_terms ( phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. \"\"\" if len ( phenotypic_features ) > 1 : number_of_real_id = len ( phenotypic_features ) - number_of_scrambled_terms else : number_of_real_id = 1 return random . sample ( phenotypic_features , number_of_real_id ) retrieve_hpo_term ( hpo_id ) Retrieve an HPO term based on the provided HPO ID. Parameters: Name Type Description Default hpo_id str The HPO ID of the term to retrieve. required Returns: Name Type Description PhenotypicFeature PhenotypicFeature The PhenotypicFeature object representing the retrieved HPO term. Source code in src/pheval/prepare/create_noisy_phenopackets.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def retrieve_hpo_term ( self , hpo_id : str ) -> PhenotypicFeature : \"\"\" Retrieve an HPO term based on the provided HPO ID. Args: hpo_id (str): The HPO ID of the term to retrieve. Returns: PhenotypicFeature: The PhenotypicFeature object representing the retrieved HPO term. \"\"\" rels = self . hpo_ontology . entity_alias_map ( hpo_id ) hpo_term = \"\" . join ( rels [( list ( rels . keys ())[ 0 ])]) return PhenotypicFeature ( type = OntologyClass ( id = hpo_id , label = hpo_term )) scramble_factor_proportions ( phenotypic_features ) Calculate the proportion of scrambled HPO terms based on the scramble factor. Parameters: Name Type Description Default phenotypic_features list [ PhenotypicFeature ] List of phenotypic features. required Returns: Name Type Description int int The calculated number of phenotypic features to be scrambled. Source code in src/pheval/prepare/create_noisy_phenopackets.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def scramble_factor_proportions ( self , phenotypic_features : list [ PhenotypicFeature ]) -> int : \"\"\" Calculate the proportion of scrambled HPO terms based on the scramble factor. Args: phenotypic_features (list[PhenotypicFeature]): List of phenotypic features. Returns: int: The calculated number of phenotypic features to be scrambled. \"\"\" if len ( phenotypic_features ) == 1 : return 1 else : return int ( round ( len ( phenotypic_features ) * self . scramble_factor , 0 )) load_ontology ( local_cached_ontology = None ) Load the Human Phenotype Ontology (HPO). Args: local_cached_ontology(Path): Path to the local cached ontology. Returns: ProntoImplementation: An instance of ProntoImplementation containing the loaded HPO. Source code in src/pheval/prepare/create_noisy_phenopackets.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def load_ontology ( local_cached_ontology : Path = None ) -> ProntoImplementation : \"\"\" Load the Human Phenotype Ontology (HPO). Args: local_cached_ontology(Path): Path to the local cached ontology. Returns: ProntoImplementation: An instance of ProntoImplementation containing the loaded HPO. \"\"\" if local_cached_ontology is None : logger . warning ( \"No local cached ontology found, using default ontology.\" ) resource = OntologyResource ( slug = \"hp.obo\" , local = False ) return ProntoImplementation ( resource ) else : logger . info ( f \"Loading local ontology from { local_cached_ontology } .\" ) resource = OntologyResource ( slug = local_cached_ontology , local = True ) return ProntoImplementation ( resource ) scramble_phenopackets ( output_dir , phenopacket_path , phenopacket_dir , scramble_factor , local_cached_ontology ) Create scrambled phenopackets from either a single phenopacket or a directory of phenopackets. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopackets. required phenopacket_path Path The path to a single Phenopacket file (if applicable). required phenopacket_dir Path The directory containing multiple Phenopacket files (if applicable). required scramble_factor float A factor determining the level of scrambling for phenotypic features. required local_cached_ontology Path The path to the local cached ontology. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def scramble_phenopackets ( output_dir : Path , phenopacket_path : Path , phenopacket_dir : Path , scramble_factor : float , local_cached_ontology : Path , ) -> None : \"\"\" Create scrambled phenopackets from either a single phenopacket or a directory of phenopackets. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_path (Path): The path to a single Phenopacket file (if applicable). phenopacket_dir (Path): The directory containing multiple Phenopacket files (if applicable). scramble_factor (float): A factor determining the level of scrambling for phenotypic features. local_cached_ontology (Path): The path to the local cached ontology. \"\"\" start_time = time . perf_counter () logger . info ( \"Initiating scrambling.\" ) logger . info ( f \"Created directory { output_dir } .\" ) logger . info ( f \"Scramble factor set to { scramble_factor } .\" ) output_dir . mkdir ( exist_ok = True ) ontology = load_ontology ( local_cached_ontology ) if phenopacket_path is not None : logger . info ( f \"Scrambling { phenopacket_path } .\" ) HpoRandomiser ( ontology , scramble_factor ) . create_scrambled_phenopacket ( output_dir , phenopacket_path ) elif phenopacket_dir is not None : logger . info ( f \"Scrambling { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) HpoRandomiser ( ontology , scramble_factor ) . create_scrambled_phenopackets ( output_dir , phenopacket_dir , ) logger . info ( f \"Finished scrambling! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"Create noisy phenopackets"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser","text":"Class for randomising phenopacket phenotypic features using Human Phenotype Ontology (HPO). Source code in src/pheval/prepare/create_noisy_phenopackets.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class HpoRandomiser : \"\"\"Class for randomising phenopacket phenotypic features using Human Phenotype Ontology (HPO).\"\"\" def __init__ ( self , hpo_ontology : ProntoImplementation , scramble_factor : float ): \"\"\" Initialise the HpoRandomiser. Args: hpo_ontology (ProntoImplementation): The instance of the HPO ontology. scramble_factor (float): A factor for scrambling phenotypic features. \"\"\" self . hpo_ontology = hpo_ontology self . phenotypic_abnormalities = set ( hpo_ontology . roots ( predicates = [ \"HP:0000118\" ])) self . scramble_factor = scramble_factor def scramble_factor_proportions ( self , phenotypic_features : list [ PhenotypicFeature ]) -> int : \"\"\" Calculate the proportion of scrambled HPO terms based on the scramble factor. Args: phenotypic_features (list[PhenotypicFeature]): List of phenotypic features. Returns: int: The calculated number of phenotypic features to be scrambled. \"\"\" if len ( phenotypic_features ) == 1 : return 1 else : return int ( round ( len ( phenotypic_features ) * self . scramble_factor , 0 )) def retrieve_hpo_term ( self , hpo_id : str ) -> PhenotypicFeature : \"\"\" Retrieve an HPO term based on the provided HPO ID. Args: hpo_id (str): The HPO ID of the term to retrieve. Returns: PhenotypicFeature: The PhenotypicFeature object representing the retrieved HPO term. \"\"\" rels = self . hpo_ontology . entity_alias_map ( hpo_id ) hpo_term = \"\" . join ( rels [( list ( rels . keys ())[ 0 ])]) return PhenotypicFeature ( type = OntologyClass ( id = hpo_id , label = hpo_term )) @staticmethod def retain_real_patient_terms ( phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. \"\"\" if len ( phenotypic_features ) > 1 : number_of_real_id = len ( phenotypic_features ) - number_of_scrambled_terms else : number_of_real_id = 1 return random . sample ( phenotypic_features , number_of_real_id ) def convert_patient_terms_to_parent ( self , phenotypic_features : List [ PhenotypicFeature ], retained_phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Convert a subset of patient HPO terms to their respective parent terms. Args: phenotypic_features (List[PhenotypicFeature]): List of all phenotypic features. retained_phenotypic_features (List[PhenotypicFeature]): List of retained non-scrambled phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note: This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. \"\"\" remaining_hpo = [ i for i in phenotypic_features if i not in retained_phenotypic_features ] if len ( remaining_hpo ) == 0 : number_of_scrambled_terms = 0 hpo_terms_to_be_changed = list ( random . sample ( remaining_hpo , number_of_scrambled_terms )) parent_terms = [] for term in hpo_terms_to_be_changed : if self . hpo_ontology . label ( term . type . id ) . startswith ( \"obsolete\" ): obsolete_term = self . hpo_ontology . entity_metadata_map ( term . type . id ) updated_term = list ( obsolete_term . values ())[ 0 ][ 0 ] parents = self . hpo_ontology . hierarchical_parents ( updated_term ) else : parents = self . hpo_ontology . hierarchical_parents ( term . type . id ) if not parents : parent_terms . append ( term ) else : parent_terms . append ( self . retrieve_hpo_term ( random . choice ( parents ))) return parent_terms def create_random_hpo_terms ( self , number_of_scrambled_terms : int ) -> List [ PhenotypicFeature ]: \"\"\" Generate a list of random HPO terms. Args: number_of_scrambled_terms (int): The count of random HPO terms to be generated. Returns: List[PhenotypicFeature]: A list of randomly selected HPO terms. \"\"\" random_ids = list ( random . sample ( sorted ( self . phenotypic_abnormalities ), number_of_scrambled_terms ) ) return [ self . retrieve_hpo_term ( random_id ) for random_id in random_ids ] def randomise_hpo_terms ( self , phenotypic_features : List [ PhenotypicFeature ], ) -> List [ PhenotypicFeature ]: \"\"\" Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features to be randomised. Returns: List[PhenotypicFeature]: A list of randomised HPO terms. Note: This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. \"\"\" number_of_scrambled_terms = self . scramble_factor_proportions ( phenotypic_features ) retained_patient_terms = self . retain_real_patient_terms ( phenotypic_features , number_of_scrambled_terms ) return ( retained_patient_terms + self . convert_patient_terms_to_parent ( phenotypic_features , retained_patient_terms , number_of_scrambled_terms ) + self . create_random_hpo_terms ( number_of_scrambled_terms ) ) def add_noise_to_phenotypic_profile ( self , phenopacket : Union [ Phenopacket , Family ], ) -> Union [ Phenopacket , Family ]: \"\"\" Randomise the phenotypic profile of a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family to be randomised. Returns: Union[Phenopacket, Family]: The randomised Phenopacket or Family. \"\"\" phenotypic_features = PhenopacketUtil ( phenopacket ) . observed_phenotypic_features () random_phenotypes = self . randomise_hpo_terms ( phenotypic_features ) randomised_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_randomised_hpo ( random_phenotypes ) return randomised_phenopacket def create_scrambled_phenopacket ( self , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Create a scrambled version of a Phenopacket. Args: output_dir (Path): The directory to store the output scrambled Phenopacket. phenopacket_path (Path): The path to the original Phenopacket file. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket , ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name ), ) def create_scrambled_phenopackets ( self , output_dir : Path , phenopacket_dir : Path , ) -> None : \"\"\" Create scrambled versions of Phenopackets within a directory. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_dir (Path): The directory containing the original Phenopacket files. \"\"\" phenopacket_files = files_with_suffix ( phenopacket_dir , \".json\" ) for phenopacket_path in phenopacket_files : logger . info ( f \"Scrambling { phenopacket_path . name } .\" ) phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name , ), )","title":"HpoRandomiser"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.__init__","text":"Initialise the HpoRandomiser. Parameters: Name Type Description Default hpo_ontology ProntoImplementation The instance of the HPO ontology. required scramble_factor float A factor for scrambling phenotypic features. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , hpo_ontology : ProntoImplementation , scramble_factor : float ): \"\"\" Initialise the HpoRandomiser. Args: hpo_ontology (ProntoImplementation): The instance of the HPO ontology. scramble_factor (float): A factor for scrambling phenotypic features. \"\"\" self . hpo_ontology = hpo_ontology self . phenotypic_abnormalities = set ( hpo_ontology . roots ( predicates = [ \"HP:0000118\" ])) self . scramble_factor = scramble_factor","title":"__init__"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.add_noise_to_phenotypic_profile","text":"Randomise the phenotypic profile of a Phenopacket or Family. Parameters: Name Type Description Default phenopacket Union [ Phenopacket , Family ] The Phenopacket or Family to be randomised. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The randomised Phenopacket or Family. Source code in src/pheval/prepare/create_noisy_phenopackets.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def add_noise_to_phenotypic_profile ( self , phenopacket : Union [ Phenopacket , Family ], ) -> Union [ Phenopacket , Family ]: \"\"\" Randomise the phenotypic profile of a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family to be randomised. Returns: Union[Phenopacket, Family]: The randomised Phenopacket or Family. \"\"\" phenotypic_features = PhenopacketUtil ( phenopacket ) . observed_phenotypic_features () random_phenotypes = self . randomise_hpo_terms ( phenotypic_features ) randomised_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_randomised_hpo ( random_phenotypes ) return randomised_phenopacket","title":"add_noise_to_phenotypic_profile"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.convert_patient_terms_to_parent","text":"Convert a subset of patient HPO terms to their respective parent terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of all phenotypic features. required retained_phenotypic_features List [ PhenotypicFeature ] List of retained non-scrambled phenotypic features. required number_of_scrambled_terms int The count of scrambled HPO terms. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. Source code in src/pheval/prepare/create_noisy_phenopackets.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def convert_patient_terms_to_parent ( self , phenotypic_features : List [ PhenotypicFeature ], retained_phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Convert a subset of patient HPO terms to their respective parent terms. Args: phenotypic_features (List[PhenotypicFeature]): List of all phenotypic features. retained_phenotypic_features (List[PhenotypicFeature]): List of retained non-scrambled phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of HPO terms converted to their parent terms. Note: This method identifies a subset of patient HPO terms that are not retained among the non-scrambled phenotypic features and converts them to their respective parent terms. It then returns a list of parent HPO terms based on the provided scrambled terms count. If no remaining HPO terms are available for conversion, no parent terms are returned. \"\"\" remaining_hpo = [ i for i in phenotypic_features if i not in retained_phenotypic_features ] if len ( remaining_hpo ) == 0 : number_of_scrambled_terms = 0 hpo_terms_to_be_changed = list ( random . sample ( remaining_hpo , number_of_scrambled_terms )) parent_terms = [] for term in hpo_terms_to_be_changed : if self . hpo_ontology . label ( term . type . id ) . startswith ( \"obsolete\" ): obsolete_term = self . hpo_ontology . entity_metadata_map ( term . type . id ) updated_term = list ( obsolete_term . values ())[ 0 ][ 0 ] parents = self . hpo_ontology . hierarchical_parents ( updated_term ) else : parents = self . hpo_ontology . hierarchical_parents ( term . type . id ) if not parents : parent_terms . append ( term ) else : parent_terms . append ( self . retrieve_hpo_term ( random . choice ( parents ))) return parent_terms","title":"convert_patient_terms_to_parent"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.create_random_hpo_terms","text":"Generate a list of random HPO terms. Parameters: Name Type Description Default number_of_scrambled_terms int The count of random HPO terms to be generated. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of randomly selected HPO terms. Source code in src/pheval/prepare/create_noisy_phenopackets.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def create_random_hpo_terms ( self , number_of_scrambled_terms : int ) -> List [ PhenotypicFeature ]: \"\"\" Generate a list of random HPO terms. Args: number_of_scrambled_terms (int): The count of random HPO terms to be generated. Returns: List[PhenotypicFeature]: A list of randomly selected HPO terms. \"\"\" random_ids = list ( random . sample ( sorted ( self . phenotypic_abnormalities ), number_of_scrambled_terms ) ) return [ self . retrieve_hpo_term ( random_id ) for random_id in random_ids ]","title":"create_random_hpo_terms"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.create_scrambled_phenopacket","text":"Create a scrambled version of a Phenopacket. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopacket. required phenopacket_path Path The path to the original Phenopacket file. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def create_scrambled_phenopacket ( self , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Create a scrambled version of a Phenopacket. Args: output_dir (Path): The directory to store the output scrambled Phenopacket. phenopacket_path (Path): The path to the original Phenopacket file. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket , ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name ), )","title":"create_scrambled_phenopacket"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.create_scrambled_phenopackets","text":"Create scrambled versions of Phenopackets within a directory. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopackets. required phenopacket_dir Path The directory containing the original Phenopacket files. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def create_scrambled_phenopackets ( self , output_dir : Path , phenopacket_dir : Path , ) -> None : \"\"\" Create scrambled versions of Phenopackets within a directory. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_dir (Path): The directory containing the original Phenopacket files. \"\"\" phenopacket_files = files_with_suffix ( phenopacket_dir , \".json\" ) for phenopacket_path in phenopacket_files : logger . info ( f \"Scrambling { phenopacket_path . name } .\" ) phenopacket = phenopacket_reader ( phenopacket_path ) created_noisy_phenopacket = self . add_noise_to_phenotypic_profile ( phenopacket ) write_phenopacket ( created_noisy_phenopacket , output_dir . joinpath ( phenopacket_path . name , ), )","title":"create_scrambled_phenopackets"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.randomise_hpo_terms","text":"Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of phenotypic features to be randomised. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of randomised HPO terms. Note This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. Source code in src/pheval/prepare/create_noisy_phenopackets.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def randomise_hpo_terms ( self , phenotypic_features : List [ PhenotypicFeature ], ) -> List [ PhenotypicFeature ]: \"\"\" Randomise the provided phenotypic features by combining retained, parent-converted, and random HPO terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features to be randomised. Returns: List[PhenotypicFeature]: A list of randomised HPO terms. Note: This method randomises the provided phenotypic features by incorporating three types of HPO terms: 1. Retained Patient Terms: Non-scrambled (real patient) HPO terms retained based on the scramble factor. 2. Converted to Parent Terms: Subset of HPO terms converted to their respective parent terms. 3. Random HPO Terms: Newly generated random HPO terms based on the scramble factor. The method determines the count of terms for each category and combines them to form a final list of randomised HPO terms to be used in the phenotypic features. \"\"\" number_of_scrambled_terms = self . scramble_factor_proportions ( phenotypic_features ) retained_patient_terms = self . retain_real_patient_terms ( phenotypic_features , number_of_scrambled_terms ) return ( retained_patient_terms + self . convert_patient_terms_to_parent ( phenotypic_features , retained_patient_terms , number_of_scrambled_terms ) + self . create_random_hpo_terms ( number_of_scrambled_terms ) )","title":"randomise_hpo_terms"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.retain_real_patient_terms","text":"Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Parameters: Name Type Description Default phenotypic_features List [ PhenotypicFeature ] List of phenotypic features. required number_of_scrambled_terms int The count of scrambled HPO terms. required Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. Source code in src/pheval/prepare/create_noisy_phenopackets.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @staticmethod def retain_real_patient_terms ( phenotypic_features : List [ PhenotypicFeature ], number_of_scrambled_terms : int , ) -> List [ PhenotypicFeature ]: \"\"\" Return a list of real patient HPO terms, retaining a specific number of non-scrambled terms. Args: phenotypic_features (List[PhenotypicFeature]): List of phenotypic features. number_of_scrambled_terms (int): The count of scrambled HPO terms. Returns: List[PhenotypicFeature]: A list of non-scrambled (real patient) HPO terms. \"\"\" if len ( phenotypic_features ) > 1 : number_of_real_id = len ( phenotypic_features ) - number_of_scrambled_terms else : number_of_real_id = 1 return random . sample ( phenotypic_features , number_of_real_id )","title":"retain_real_patient_terms"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.retrieve_hpo_term","text":"Retrieve an HPO term based on the provided HPO ID. Parameters: Name Type Description Default hpo_id str The HPO ID of the term to retrieve. required Returns: Name Type Description PhenotypicFeature PhenotypicFeature The PhenotypicFeature object representing the retrieved HPO term. Source code in src/pheval/prepare/create_noisy_phenopackets.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def retrieve_hpo_term ( self , hpo_id : str ) -> PhenotypicFeature : \"\"\" Retrieve an HPO term based on the provided HPO ID. Args: hpo_id (str): The HPO ID of the term to retrieve. Returns: PhenotypicFeature: The PhenotypicFeature object representing the retrieved HPO term. \"\"\" rels = self . hpo_ontology . entity_alias_map ( hpo_id ) hpo_term = \"\" . join ( rels [( list ( rels . keys ())[ 0 ])]) return PhenotypicFeature ( type = OntologyClass ( id = hpo_id , label = hpo_term ))","title":"retrieve_hpo_term"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.HpoRandomiser.scramble_factor_proportions","text":"Calculate the proportion of scrambled HPO terms based on the scramble factor. Parameters: Name Type Description Default phenotypic_features list [ PhenotypicFeature ] List of phenotypic features. required Returns: Name Type Description int int The calculated number of phenotypic features to be scrambled. Source code in src/pheval/prepare/create_noisy_phenopackets.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def scramble_factor_proportions ( self , phenotypic_features : list [ PhenotypicFeature ]) -> int : \"\"\" Calculate the proportion of scrambled HPO terms based on the scramble factor. Args: phenotypic_features (list[PhenotypicFeature]): List of phenotypic features. Returns: int: The calculated number of phenotypic features to be scrambled. \"\"\" if len ( phenotypic_features ) == 1 : return 1 else : return int ( round ( len ( phenotypic_features ) * self . scramble_factor , 0 ))","title":"scramble_factor_proportions"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.load_ontology","text":"Load the Human Phenotype Ontology (HPO). Args: local_cached_ontology(Path): Path to the local cached ontology. Returns: ProntoImplementation: An instance of ProntoImplementation containing the loaded HPO. Source code in src/pheval/prepare/create_noisy_phenopackets.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def load_ontology ( local_cached_ontology : Path = None ) -> ProntoImplementation : \"\"\" Load the Human Phenotype Ontology (HPO). Args: local_cached_ontology(Path): Path to the local cached ontology. Returns: ProntoImplementation: An instance of ProntoImplementation containing the loaded HPO. \"\"\" if local_cached_ontology is None : logger . warning ( \"No local cached ontology found, using default ontology.\" ) resource = OntologyResource ( slug = \"hp.obo\" , local = False ) return ProntoImplementation ( resource ) else : logger . info ( f \"Loading local ontology from { local_cached_ontology } .\" ) resource = OntologyResource ( slug = local_cached_ontology , local = True ) return ProntoImplementation ( resource )","title":"load_ontology"},{"location":"api/pheval/prepare/create_noisy_phenopackets/#src.pheval.prepare.create_noisy_phenopackets.scramble_phenopackets","text":"Create scrambled phenopackets from either a single phenopacket or a directory of phenopackets. Parameters: Name Type Description Default output_dir Path The directory to store the output scrambled Phenopackets. required phenopacket_path Path The path to a single Phenopacket file (if applicable). required phenopacket_dir Path The directory containing multiple Phenopacket files (if applicable). required scramble_factor float A factor determining the level of scrambling for phenotypic features. required local_cached_ontology Path The path to the local cached ontology. required Source code in src/pheval/prepare/create_noisy_phenopackets.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def scramble_phenopackets ( output_dir : Path , phenopacket_path : Path , phenopacket_dir : Path , scramble_factor : float , local_cached_ontology : Path , ) -> None : \"\"\" Create scrambled phenopackets from either a single phenopacket or a directory of phenopackets. Args: output_dir (Path): The directory to store the output scrambled Phenopackets. phenopacket_path (Path): The path to a single Phenopacket file (if applicable). phenopacket_dir (Path): The directory containing multiple Phenopacket files (if applicable). scramble_factor (float): A factor determining the level of scrambling for phenotypic features. local_cached_ontology (Path): The path to the local cached ontology. \"\"\" start_time = time . perf_counter () logger . info ( \"Initiating scrambling.\" ) logger . info ( f \"Created directory { output_dir } .\" ) logger . info ( f \"Scramble factor set to { scramble_factor } .\" ) output_dir . mkdir ( exist_ok = True ) ontology = load_ontology ( local_cached_ontology ) if phenopacket_path is not None : logger . info ( f \"Scrambling { phenopacket_path } .\" ) HpoRandomiser ( ontology , scramble_factor ) . create_scrambled_phenopacket ( output_dir , phenopacket_path ) elif phenopacket_dir is not None : logger . info ( f \"Scrambling { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) HpoRandomiser ( ontology , scramble_factor ) . create_scrambled_phenopackets ( output_dir , phenopacket_dir , ) logger . info ( f \"Finished scrambling! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"scramble_phenopackets"},{"location":"api/pheval/prepare/create_spiked_vcf/","text":"VcfFile dataclass Represents a VCF file with its name, contents, and header information. Attributes: Name Type Description vcf_file_name str The name of the VCF file. vcf_contents List [ str ] The contents of the VCF file. vcf_header VcfHeader The parsed header information of the VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @dataclass class VcfFile : \"\"\" Represents a VCF file with its name, contents, and header information. Attributes: vcf_file_name (str): The name of the VCF file. vcf_contents (List[str]): The contents of the VCF file. vcf_header (VcfHeader): The parsed header information of the VCF file. \"\"\" vcf_file_name : str = None vcf_contents : List [ str ] = None vcf_header : VcfHeader = None @staticmethod def populate_fields ( template_vcf : Path ): \"\"\" Populate the fields of the VcfFile instance using the contents of a template VCF file. Args: template_vcf (Path): The path to the template VCF file. Returns: VcfFile: An instance of VcfFile with populated fields. \"\"\" contents = read_vcf ( template_vcf ) return VcfFile ( template_vcf . name , contents , VcfHeaderParser ( contents ) . parse_vcf_header ()) populate_fields ( template_vcf ) staticmethod Populate the fields of the VcfFile instance using the contents of a template VCF file. Parameters: Name Type Description Default template_vcf Path The path to the template VCF file. required Returns: Name Type Description VcfFile An instance of VcfFile with populated fields. Source code in src/pheval/prepare/create_spiked_vcf.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def populate_fields ( template_vcf : Path ): \"\"\" Populate the fields of the VcfFile instance using the contents of a template VCF file. Args: template_vcf (Path): The path to the template VCF file. Returns: VcfFile: An instance of VcfFile with populated fields. \"\"\" contents = read_vcf ( template_vcf ) return VcfFile ( template_vcf . name , contents , VcfHeaderParser ( contents ) . parse_vcf_header ()) VcfHeader dataclass Data obtained from VCF header. Parameters: Name Type Description Default sample_id str The sample identifier from the VCF header. required assembly str The assembly information obtained from the VCF header. required chr_status bool A boolean indicating whether the VCF denotes chromosomes as chr or not. required Source code in src/pheval/prepare/create_spiked_vcf.py 78 79 80 81 82 83 84 85 86 87 88 89 90 @dataclass class VcfHeader : \"\"\"Data obtained from VCF header. Args: sample_id (str): The sample identifier from the VCF header. assembly (str): The assembly information obtained from the VCF header. chr_status (bool): A boolean indicating whether the VCF denotes chromosomes as chr or not. \"\"\" sample_id : str assembly : str chr_status : bool VcfHeaderParser Class for parsing the header of a VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class VcfHeaderParser : \"\"\"Class for parsing the header of a VCF file.\"\"\" def __init__ ( self , vcf_contents : list [ str ]): \"\"\" Initialise the VcfHeaderParser. Args: vcf_contents (list[str]): The contents of the VCF file as a list of strings. \"\"\" self . vcf_contents = vcf_contents def parse_assembly ( self ) -> tuple [ str , bool ]: \"\"\" Parse the genome assembly and format of vcf_records. Returns: Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). \"\"\" vcf_assembly = {} chr_status = False for line in self . vcf_contents : if line . startswith ( \"##contig=<ID\" ): tokens = line . split ( \",\" ) chromosome = re . sub ( r \"^.*?ID=\" , \"\" , [ token for token in tokens if \"ID=\" in token ][ 0 ] ) if \"chr\" in chromosome : chr_status = True chromosome = chromosome . replace ( \"chr\" , \"\" ) contig_length = re . sub ( \"[^0-9]+\" , \"\" , [ token for token in tokens if \"length=\" in token ][ 0 ], ) vcf_assembly [ chromosome ] = int ( contig_length ) vcf_assembly = { i : vcf_assembly [ i ] for i in vcf_assembly if i . isdigit ()} assembly = [ k for k , v in genome_assemblies . items () if v == vcf_assembly ][ 0 ] return assembly , chr_status def parse_sample_id ( self ) -> str : \"\"\" Parse the sample ID of the VCF. Returns: str: The sample ID extracted from the VCF header. \"\"\" for line in self . vcf_contents : if line . startswith ( \"#CHROM\" ): return line . split ( \" \\t \" )[ 9 ] . rstrip () def parse_vcf_header ( self ) -> VcfHeader : \"\"\" Parse the header of the VCF. Returns: VcfHeader: An instance of VcfHeader containing sample ID, assembly, and chromosome status. \"\"\" assembly , chr_status = self . parse_assembly () sample_id = self . parse_sample_id () return VcfHeader ( sample_id , assembly , chr_status ) __init__ ( vcf_contents ) Initialise the VcfHeaderParser. Parameters: Name Type Description Default vcf_contents list [ str ] The contents of the VCF file as a list of strings. required Source code in src/pheval/prepare/create_spiked_vcf.py 115 116 117 118 119 120 121 122 def __init__ ( self , vcf_contents : list [ str ]): \"\"\" Initialise the VcfHeaderParser. Args: vcf_contents (list[str]): The contents of the VCF file as a list of strings. \"\"\" self . vcf_contents = vcf_contents parse_assembly () Parse the genome assembly and format of vcf_records. Returns: Type Description tuple [ str , bool ] Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). Source code in src/pheval/prepare/create_spiked_vcf.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def parse_assembly ( self ) -> tuple [ str , bool ]: \"\"\" Parse the genome assembly and format of vcf_records. Returns: Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). \"\"\" vcf_assembly = {} chr_status = False for line in self . vcf_contents : if line . startswith ( \"##contig=<ID\" ): tokens = line . split ( \",\" ) chromosome = re . sub ( r \"^.*?ID=\" , \"\" , [ token for token in tokens if \"ID=\" in token ][ 0 ] ) if \"chr\" in chromosome : chr_status = True chromosome = chromosome . replace ( \"chr\" , \"\" ) contig_length = re . sub ( \"[^0-9]+\" , \"\" , [ token for token in tokens if \"length=\" in token ][ 0 ], ) vcf_assembly [ chromosome ] = int ( contig_length ) vcf_assembly = { i : vcf_assembly [ i ] for i in vcf_assembly if i . isdigit ()} assembly = [ k for k , v in genome_assemblies . items () if v == vcf_assembly ][ 0 ] return assembly , chr_status parse_sample_id () Parse the sample ID of the VCF. Returns: Name Type Description str str The sample ID extracted from the VCF header. Source code in src/pheval/prepare/create_spiked_vcf.py 152 153 154 155 156 157 158 159 160 161 def parse_sample_id ( self ) -> str : \"\"\" Parse the sample ID of the VCF. Returns: str: The sample ID extracted from the VCF header. \"\"\" for line in self . vcf_contents : if line . startswith ( \"#CHROM\" ): return line . split ( \" \\t \" )[ 9 ] . rstrip () parse_vcf_header () Parse the header of the VCF. Returns: Name Type Description VcfHeader VcfHeader An instance of VcfHeader containing sample ID, assembly, and chromosome status. Source code in src/pheval/prepare/create_spiked_vcf.py 163 164 165 166 167 168 169 170 171 172 def parse_vcf_header ( self ) -> VcfHeader : \"\"\" Parse the header of the VCF. Returns: VcfHeader: An instance of VcfHeader containing sample ID, assembly, and chromosome status. \"\"\" assembly , chr_status = self . parse_assembly () sample_id = self . parse_sample_id () return VcfHeader ( sample_id , assembly , chr_status ) VcfSpiker Class for spiking proband variants into template VCF file contents. Source code in src/pheval/prepare/create_spiked_vcf.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class VcfSpiker : \"\"\"Class for spiking proband variants into template VCF file contents.\"\"\" def __init__ ( self , vcf_contents : list [ str ], proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , ): \"\"\" Initialise the VcfSpiker. Args: vcf_contents (List[str]): Contents of the template VCF file. proband_causative_variants (List[ProbandCausativeVariant]): List of proband causative variants. vcf_header (VcfHeader): The VCF header information. \"\"\" self . vcf_contents = vcf_contents self . proband_causative_variants = proband_causative_variants self . vcf_header = vcf_header def construct_variant_entry ( self , proband_variant_data : ProbandCausativeVariant ) -> List [ str ]: \"\"\" Construct variant entries. Args: proband_variant_data (ProbandCausativeVariant): Data for the proband variant. Returns: List[str]: Constructed variant entry as a list of strings. \"\"\" genotype_codes = { \"hemizygous\" : \"0/1\" , \"homozygous\" : \"1/1\" , \"heterozygous\" : \"0/1\" , \"compound heterozygous\" : \"0/1\" , } if self . vcf_header . chr_status is True and \"chr\" not in proband_variant_data . variant . chrom : proband_variant_data . variant . chrom = \"chr\" + proband_variant_data . variant . chrom return [ proband_variant_data . variant . chrom , str ( proband_variant_data . variant . pos ), \".\" , proband_variant_data . variant . ref , ( f \"< { proband_variant_data . variant . alt } >\" if proband_variant_data . variant . ref == \"N\" else proband_variant_data . variant . alt ), \"100\" , \"PASS\" , proband_variant_data . info if proband_variant_data . info else \".\" , \"GT\" , genotype_codes [ proband_variant_data . genotype . lower ()] + \" \\n \" , ] def construct_vcf_records ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: Updated VCF records containing the spiked variants. \"\"\" updated_vcf_records = copy ( self . vcf_contents ) for variant in self . proband_causative_variants : variant_entry = self . construct_variant_entry ( variant ) matching_indices = [ i for i , val in enumerate ( updated_vcf_records ) if val . split ( \" \\t \" )[ 0 ] == variant_entry [ 0 ] and int ( val . split ( \" \\t \" )[ 1 ]) < int ( variant_entry [ 1 ]) ] if matching_indices : logger . info ( f \"Successfully spiked variant { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } \" ) variant_entry_position = matching_indices [ - 1 ] + 1 else : logger . warning ( f \"Could not find entry position for { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } , \" \"inserting at end of VCF contents.\" ) variant_entry_position = len ( updated_vcf_records ) updated_vcf_records . insert ( variant_entry_position , \" \\t \" . join ( variant_entry )) return updated_vcf_records def construct_header ( self , updated_vcf_records : List [ str ]) -> List [ str ]: \"\"\" Construct the header of the VCF. Args: updated_vcf_records (List[str]): Updated VCF records. Returns: List[str]: Constructed header as a list of strings. \"\"\" updated_vcf_file = [] for line in updated_vcf_records : if line . startswith ( \"#\" ): text = line . replace ( self . vcf_header . sample_id , self . proband_causative_variants [ 0 ] . proband_id , ) else : text = line updated_vcf_file . append ( text ) return updated_vcf_file def construct_vcf ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: The complete spiked VCF file content as a list of strings. \"\"\" return self . construct_header ( self . construct_vcf_records ( template_vcf_name )) __init__ ( vcf_contents , proband_causative_variants , vcf_header ) Initialise the VcfSpiker. Parameters: Name Type Description Default vcf_contents List [ str ] Contents of the template VCF file. required proband_causative_variants List [ ProbandCausativeVariant ] List of proband causative variants. required vcf_header VcfHeader The VCF header information. required Source code in src/pheval/prepare/create_spiked_vcf.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def __init__ ( self , vcf_contents : list [ str ], proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , ): \"\"\" Initialise the VcfSpiker. Args: vcf_contents (List[str]): Contents of the template VCF file. proband_causative_variants (List[ProbandCausativeVariant]): List of proband causative variants. vcf_header (VcfHeader): The VCF header information. \"\"\" self . vcf_contents = vcf_contents self . proband_causative_variants = proband_causative_variants self . vcf_header = vcf_header construct_header ( updated_vcf_records ) Construct the header of the VCF. Parameters: Name Type Description Default updated_vcf_records List [ str ] Updated VCF records. required Returns: Type Description List [ str ] List[str]: Constructed header as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def construct_header ( self , updated_vcf_records : List [ str ]) -> List [ str ]: \"\"\" Construct the header of the VCF. Args: updated_vcf_records (List[str]): Updated VCF records. Returns: List[str]: Constructed header as a list of strings. \"\"\" updated_vcf_file = [] for line in updated_vcf_records : if line . startswith ( \"#\" ): text = line . replace ( self . vcf_header . sample_id , self . proband_causative_variants [ 0 ] . proband_id , ) else : text = line updated_vcf_file . append ( text ) return updated_vcf_file construct_variant_entry ( proband_variant_data ) Construct variant entries. Parameters: Name Type Description Default proband_variant_data ProbandCausativeVariant Data for the proband variant. required Returns: Type Description List [ str ] List[str]: Constructed variant entry as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def construct_variant_entry ( self , proband_variant_data : ProbandCausativeVariant ) -> List [ str ]: \"\"\" Construct variant entries. Args: proband_variant_data (ProbandCausativeVariant): Data for the proband variant. Returns: List[str]: Constructed variant entry as a list of strings. \"\"\" genotype_codes = { \"hemizygous\" : \"0/1\" , \"homozygous\" : \"1/1\" , \"heterozygous\" : \"0/1\" , \"compound heterozygous\" : \"0/1\" , } if self . vcf_header . chr_status is True and \"chr\" not in proband_variant_data . variant . chrom : proband_variant_data . variant . chrom = \"chr\" + proband_variant_data . variant . chrom return [ proband_variant_data . variant . chrom , str ( proband_variant_data . variant . pos ), \".\" , proband_variant_data . variant . ref , ( f \"< { proband_variant_data . variant . alt } >\" if proband_variant_data . variant . ref == \"N\" else proband_variant_data . variant . alt ), \"100\" , \"PASS\" , proband_variant_data . info if proband_variant_data . info else \".\" , \"GT\" , genotype_codes [ proband_variant_data . genotype . lower ()] + \" \\n \" , ] construct_vcf ( template_vcf_name ) Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Parameters: Name Type Description Default template_vcf_name str Name of the template VCF file. required Returns: Type Description List [ str ] List[str]: The complete spiked VCF file content as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 397 398 399 400 401 402 403 404 405 406 407 def construct_vcf ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: The complete spiked VCF file content as a list of strings. \"\"\" return self . construct_header ( self . construct_vcf_records ( template_vcf_name )) construct_vcf_records ( template_vcf_name ) Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Parameters: Name Type Description Default template_vcf_name str Name of the template VCF file. required Returns: Type Description List [ str ] List[str]: Updated VCF records containing the spiked variants. Source code in src/pheval/prepare/create_spiked_vcf.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def construct_vcf_records ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: Updated VCF records containing the spiked variants. \"\"\" updated_vcf_records = copy ( self . vcf_contents ) for variant in self . proband_causative_variants : variant_entry = self . construct_variant_entry ( variant ) matching_indices = [ i for i , val in enumerate ( updated_vcf_records ) if val . split ( \" \\t \" )[ 0 ] == variant_entry [ 0 ] and int ( val . split ( \" \\t \" )[ 1 ]) < int ( variant_entry [ 1 ]) ] if matching_indices : logger . info ( f \"Successfully spiked variant { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } \" ) variant_entry_position = matching_indices [ - 1 ] + 1 else : logger . warning ( f \"Could not find entry position for { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } , \" \"inserting at end of VCF contents.\" ) variant_entry_position = len ( updated_vcf_records ) updated_vcf_records . insert ( variant_entry_position , \" \\t \" . join ( variant_entry )) return updated_vcf_records VcfWriter Class for writing VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 class VcfWriter : \"\"\"Class for writing VCF file.\"\"\" def __init__ ( self , vcf_contents : List [ str ], spiked_vcf_file_path : Path , ): \"\"\" Initialise the VcfWriter class. Args: vcf_contents (List[str]): Contents of the VCF file to be written. spiked_vcf_file_path (Path): Path to the spiked VCF file to be created. \"\"\" self . vcf_contents = vcf_contents self . spiked_vcf_file_path = spiked_vcf_file_path def write_gzip ( self ) -> None : \"\"\" Write the VCF contents to a gzipped VCF file. \"\"\" encoded_contents = [ line . encode () for line in self . vcf_contents ] with gzip . open ( self . spiked_vcf_file_path , \"wb\" ) as f : for line in encoded_contents : f . write ( line ) f . close () def write_uncompressed ( self ) -> None : \"\"\" Write the VCF contents to an uncompressed VCF file. \"\"\" with open ( self . spiked_vcf_file_path , \"w\" ) as file : file . writelines ( self . vcf_contents ) file . close () def write_vcf_file ( self ) -> None : \"\"\" Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). \"\"\" self . write_gzip () if is_gzipped ( self . spiked_vcf_file_path ) else self . write_uncompressed () __init__ ( vcf_contents , spiked_vcf_file_path ) Initialise the VcfWriter class. Parameters: Name Type Description Default vcf_contents List [ str ] Contents of the VCF file to be written. required spiked_vcf_file_path Path Path to the spiked VCF file to be created. required Source code in src/pheval/prepare/create_spiked_vcf.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def __init__ ( self , vcf_contents : List [ str ], spiked_vcf_file_path : Path , ): \"\"\" Initialise the VcfWriter class. Args: vcf_contents (List[str]): Contents of the VCF file to be written. spiked_vcf_file_path (Path): Path to the spiked VCF file to be created. \"\"\" self . vcf_contents = vcf_contents self . spiked_vcf_file_path = spiked_vcf_file_path write_gzip () Write the VCF contents to a gzipped VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 428 429 430 431 432 433 434 435 436 def write_gzip ( self ) -> None : \"\"\" Write the VCF contents to a gzipped VCF file. \"\"\" encoded_contents = [ line . encode () for line in self . vcf_contents ] with gzip . open ( self . spiked_vcf_file_path , \"wb\" ) as f : for line in encoded_contents : f . write ( line ) f . close () write_uncompressed () Write the VCF contents to an uncompressed VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 438 439 440 441 442 443 444 def write_uncompressed ( self ) -> None : \"\"\" Write the VCF contents to an uncompressed VCF file. \"\"\" with open ( self . spiked_vcf_file_path , \"w\" ) as file : file . writelines ( self . vcf_contents ) file . close () write_vcf_file () Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). Source code in src/pheval/prepare/create_spiked_vcf.py 446 447 448 449 450 451 452 453 def write_vcf_file ( self ) -> None : \"\"\" Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). \"\"\" self . write_gzip () if is_gzipped ( self . spiked_vcf_file_path ) else self . write_uncompressed () check_variant_assembly ( proband_causative_variants , vcf_header , phenopacket_path ) Check the assembly of the variant assembly against the VCF. Parameters: Name Type Description Default proband_causative_variants List [ ProbandCausativeVariant ] A list of causative variants from the proband. required vcf_header VcfHeader An instance of VcfHeader representing the VCF file's header. required phenopacket_path Path The path to the Phenopacket file. required Raises: Type Description ValueError If there are too many or incompatible genome assemblies found. IncompatibleGenomeAssemblyError If the assembly in the Phenopacket does not match the VCF assembly. Source code in src/pheval/prepare/create_spiked_vcf.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def check_variant_assembly ( proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , phenopacket_path : Path , ) -> None : \"\"\" Check the assembly of the variant assembly against the VCF. Args: proband_causative_variants (List[ProbandCausativeVariant]): A list of causative variants from the proband. vcf_header (VcfHeader): An instance of VcfHeader representing the VCF file's header. phenopacket_path (Path): The path to the Phenopacket file. Raises: ValueError: If there are too many or incompatible genome assemblies found. IncompatibleGenomeAssemblyError: If the assembly in the Phenopacket does not match the VCF assembly. \"\"\" compatible_genome_assembly = { \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" } phenopacket_assembly = list ({ variant . assembly for variant in proband_causative_variants }) if len ( phenopacket_assembly ) > 1 : raise ValueError ( \"Too many genome assemblies!\" ) if phenopacket_assembly [ 0 ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( phenopacket_assembly , phenopacket_path ) if ( phenopacket_assembly [ 0 ] in { \"hg19\" , \"GRCh37\" } and vcf_header . assembly not in { \"hg19\" , \"GRCh37\" } ) or ( phenopacket_assembly [ 0 ] in { \"hg38\" , \"GRCh38\" } and vcf_header . assembly not in { \"hg38\" , \"GRCh38\" } ): raise IncompatibleGenomeAssemblyError ( assembly = phenopacket_assembly , phenopacket = phenopacket_path ) create_spiked_vcf ( output_dir , phenopacket_path , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir ) Create a spiked VCF for a Phenopacket. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file. required phenopacket_path Path Path to the Phenopacket file. required hg19_template_vcf Path Path to the hg19 template VCF file (optional). required hg38_template_vcf Path Path to the hg38 template VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Raises: Type Description InputError If both hg19_template_vcf and hg38_template_vcf are None. Source code in src/pheval/prepare/create_spiked_vcf.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 def create_spiked_vcf ( output_dir : Path , phenopacket_path : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create a spiked VCF for a Phenopacket. Args: output_dir (Path): The directory to store the generated spiked VCF file. phenopacket_path (Path): Path to the Phenopacket file. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). Raises: InputError: If both hg19_template_vcf and hg38_template_vcf are None. \"\"\" if hg19_template_vcf is None and hg38_template_vcf is None : raise InputError ( \"Either a hg19 template vcf or hg38 template vcf must be specified\" ) hg19_vcf_info = VcfFile . populate_fields ( hg19_template_vcf ) if hg19_template_vcf else None hg38_vcf_info = VcfFile . populate_fields ( hg38_template_vcf ) if hg38_template_vcf else None spike_and_update_phenopacket ( hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , output_dir , phenopacket_path ) create_spiked_vcfs ( output_dir , phenopacket_dir , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir ) Create a spiked VCF for a directory of Phenopackets. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file. required phenopacket_dir Path Path to the Phenopacket directory. required hg19_template_vcf Path Path to the template hg19 VCF file (optional). required hg38_template_vcf Path Path to the template hg19 VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Raises: Type Description InputError If both hg19_template_vcf and hg38_template_vcf are None. Source code in src/pheval/prepare/create_spiked_vcf.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def create_spiked_vcfs ( output_dir : Path , phenopacket_dir : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create a spiked VCF for a directory of Phenopackets. Args: output_dir (Path): The directory to store the generated spiked VCF file. phenopacket_dir (Path): Path to the Phenopacket directory. hg19_template_vcf (Path): Path to the template hg19 VCF file (optional). hg38_template_vcf (Path): Path to the template hg19 VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). Raises: InputError: If both hg19_template_vcf and hg38_template_vcf are None. \"\"\" if ( hg19_template_vcf is None and hg38_template_vcf is None and hg19_vcf_dir is None and hg38_vcf_dir is None ): raise InputError ( \"Need to specify a VCF!\" ) hg19_vcf_info = VcfFile . populate_fields ( hg19_template_vcf ) if hg19_template_vcf else None hg38_vcf_info = VcfFile . populate_fields ( hg38_template_vcf ) if hg38_template_vcf else None for phenopacket_path in files_with_suffix ( phenopacket_dir , \".json\" ): logger . info ( f \"Creating spiked VCF for: { phenopacket_path . name } \" ) spike_and_update_phenopacket ( hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , output_dir , phenopacket_path ) generate_spiked_vcf_file ( output_dir , phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir ) Write spiked VCF contents to a new file. Parameters: Name Type Description Default output_dir Path Path to the directory to store the generated file. required phenopacket Union [ Phenopacket , Family ] Phenopacket or Family containing causative variants. required phenopacket_path Path Path to the Phenopacket file. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: File: The generated File object representing the newly created spiked VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def generate_spiked_vcf_file ( output_dir : Path , phenopacket : Union [ Phenopacket , Family ], phenopacket_path : Path , hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> File : \"\"\" Write spiked VCF contents to a new file. Args: output_dir (Path): Path to the directory to store the generated file. phenopacket (Union[Phenopacket, Family]): Phenopacket or Family containing causative variants. phenopacket_path (Path): Path to the Phenopacket file. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: File: The generated File object representing the newly created spiked VCF file. \"\"\" vcf_assembly , spiked_vcf = spike_vcf_contents ( phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir ) spiked_vcf_path = output_dir . joinpath ( phenopacket_path . name . replace ( \".json\" , \".vcf.gz\" )) VcfWriter ( spiked_vcf , spiked_vcf_path ) . write_vcf_file () return File ( uri = urllib . parse . unquote ( spiked_vcf_path . as_uri ()), file_attributes = { \"fileFormat\" : \"vcf\" , \"genomeAssembly\" : vcf_assembly }, ) read_vcf ( vcf_file ) Read the contents of a VCF file into memory, handling both uncompressed and gzipped files. Parameters: Name Type Description Default vcf_file Path The path to the VCF file to be read. required Returns: Type Description List [ str ] List[str]: A list containing the lines of the VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_vcf ( vcf_file : Path ) -> List [ str ]: \"\"\" Read the contents of a VCF file into memory, handling both uncompressed and gzipped files. Args: vcf_file (Path): The path to the VCF file to be read. Returns: List[str]: A list containing the lines of the VCF file. \"\"\" open_fn = gzip . open if is_gzipped ( vcf_file ) else open vcf = open_fn ( vcf_file ) vcf_contents = ( [ line . decode () for line in vcf . readlines ()] if is_gzipped ( vcf_file ) else vcf . readlines () ) vcf . close () return vcf_contents select_vcf_template ( phenopacket_path , proband_causative_variants , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir ) Select the appropriate VCF template based on the assembly information of the proband causative variants. Parameters: Name Type Description Default phenopacket_path Path The path to the Phenopacket file. required proband_causative_variants List [ ProbandCausativeVariant ] A list of causative variants from the proband. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile CF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: Name Type Description VcfFile VcfFile The selected VCF template file based on the assembly information of the proband causative variants. Source code in src/pheval/prepare/create_spiked_vcf.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def select_vcf_template ( phenopacket_path : Path , proband_causative_variants : List [ ProbandCausativeVariant ], hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> VcfFile : \"\"\" Select the appropriate VCF template based on the assembly information of the proband causative variants. Args: phenopacket_path (Path): The path to the Phenopacket file. proband_causative_variants (List[ProbandCausativeVariant]): A list of causative variants from the proband. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): CF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: VcfFile: The selected VCF template file based on the assembly information of the proband causative variants. \"\"\" if proband_causative_variants [ 0 ] . assembly in [ \"hg19\" , \"GRCh37\" ]: if hg19_vcf_info : return hg19_vcf_info elif hg19_vcf_dir : return VcfFile . populate_fields ( random . choice ( all_files ( hg19_vcf_dir ))) else : raise InputError ( \"Must specify hg19 template VCF!\" ) elif proband_causative_variants [ 0 ] . assembly in [ \"hg38\" , \"GRCh38\" ]: if hg38_vcf_info : return hg38_vcf_info elif hg38_vcf_dir : return VcfFile . populate_fields ( random . choice ( all_files ( hg38_vcf_dir ))) else : raise InputError ( \"Must specify hg38 template VCF!\" ) else : raise IncompatibleGenomeAssemblyError ( proband_causative_variants [ 0 ] . assembly , phenopacket_path ) spike_and_update_phenopacket ( hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , output_dir , phenopacket_path ) Spike the VCF files with genetic variants relevant to the provided Phenopacket, update the Phenopacket accordingly, and write the updated Phenopacket to the specified output directory. Parameters: Name Type Description Default hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required output_dir Path Directory where the updated Phenopacket will be saved. required phenopacket_path Path Path to the original Phenopacket file. required Returns: Type Description None None Source code in src/pheval/prepare/create_spiked_vcf.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 def spike_and_update_phenopacket ( hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Spike the VCF files with genetic variants relevant to the provided Phenopacket, update the Phenopacket accordingly, and write the updated Phenopacket to the specified output directory. Args: hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. output_dir (Path): Directory where the updated Phenopacket will be saved. phenopacket_path (Path): Path to the original Phenopacket file. Returns: None \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) spiked_vcf_file_message = generate_spiked_vcf_file ( output_dir , phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , ) updated_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_spiked_vcf_path ( spiked_vcf_file_message ) write_phenopacket ( updated_phenopacket , phenopacket_path ) spike_vcf_contents ( phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir ) Spike VCF records with variants obtained from a Phenopacket or Family. Parameters: Name Type Description Default phenopacket Union [ Phenopacket , Family ] Phenopacket or Family containing causative variants. required phenopacket_path Path Path to the Phenopacket file. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: Type Description tuple [ str , List [ str ]] A tuple containing: assembly (str): The genome assembly information extracted from VCF header. modified_vcf_contents (List[str]): Modified VCF records with spiked variants. Source code in src/pheval/prepare/create_spiked_vcf.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def spike_vcf_contents ( phenopacket : Union [ Phenopacket , Family ], phenopacket_path : Path , hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> tuple [ str , List [ str ]]: \"\"\" Spike VCF records with variants obtained from a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family containing causative variants. phenopacket_path (Path): Path to the Phenopacket file. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: A tuple containing: assembly (str): The genome assembly information extracted from VCF header. modified_vcf_contents (List[str]): Modified VCF records with spiked variants. \"\"\" phenopacket_causative_variants = PhenopacketUtil ( phenopacket ) . causative_variants () chosen_template_vcf = select_vcf_template ( phenopacket_path , phenopacket_causative_variants , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , ) check_variant_assembly ( phenopacket_causative_variants , chosen_template_vcf . vcf_header , phenopacket_path ) return ( chosen_template_vcf . vcf_header . assembly , VcfSpiker ( chosen_template_vcf . vcf_contents , phenopacket_causative_variants , chosen_template_vcf . vcf_header , ) . construct_vcf ( chosen_template_vcf . vcf_file_name ), ) spike_vcfs ( output_dir , phenopacket_path , phenopacket_dir , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir ) Create spiked VCF from either a Phenopacket or a Phenopacket directory. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file(s). required phenopacket_path Path Path to a single Phenopacket file (optional). required phenopacket_dir Path Path to a directory containing Phenopacket files (optional). required hg19_template_vcf Path Path to the hg19 template VCF file (optional). required hg38_template_vcf Path Path to the hg38 template VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Source code in src/pheval/prepare/create_spiked_vcf.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 def spike_vcfs ( output_dir : Path , phenopacket_path : Path , phenopacket_dir : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: output_dir (Path): The directory to store the generated spiked VCF file(s). phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). \"\"\" start_time = time . perf_counter () logger . info ( \"Creating spiked VCFs.\" ) output_dir . mkdir ( exist_ok = True ) logger . info ( f \" Created output directory: { output_dir } \" ) if phenopacket_path is not None : logger . info ( f \"Spiking variants from { phenopacket_path } .\" ) create_spiked_vcf ( output_dir , phenopacket_path , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) elif phenopacket_dir is not None : logger . info ( f \"Spiking variants from { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) create_spiked_vcfs ( output_dir , phenopacket_dir , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) logger . info ( f \"Finished spiking! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"Create spiked vcf"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfFile","text":"Represents a VCF file with its name, contents, and header information. Attributes: Name Type Description vcf_file_name str The name of the VCF file. vcf_contents List [ str ] The contents of the VCF file. vcf_header VcfHeader The parsed header information of the VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @dataclass class VcfFile : \"\"\" Represents a VCF file with its name, contents, and header information. Attributes: vcf_file_name (str): The name of the VCF file. vcf_contents (List[str]): The contents of the VCF file. vcf_header (VcfHeader): The parsed header information of the VCF file. \"\"\" vcf_file_name : str = None vcf_contents : List [ str ] = None vcf_header : VcfHeader = None @staticmethod def populate_fields ( template_vcf : Path ): \"\"\" Populate the fields of the VcfFile instance using the contents of a template VCF file. Args: template_vcf (Path): The path to the template VCF file. Returns: VcfFile: An instance of VcfFile with populated fields. \"\"\" contents = read_vcf ( template_vcf ) return VcfFile ( template_vcf . name , contents , VcfHeaderParser ( contents ) . parse_vcf_header ())","title":"VcfFile"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfFile.populate_fields","text":"Populate the fields of the VcfFile instance using the contents of a template VCF file. Parameters: Name Type Description Default template_vcf Path The path to the template VCF file. required Returns: Name Type Description VcfFile An instance of VcfFile with populated fields. Source code in src/pheval/prepare/create_spiked_vcf.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def populate_fields ( template_vcf : Path ): \"\"\" Populate the fields of the VcfFile instance using the contents of a template VCF file. Args: template_vcf (Path): The path to the template VCF file. Returns: VcfFile: An instance of VcfFile with populated fields. \"\"\" contents = read_vcf ( template_vcf ) return VcfFile ( template_vcf . name , contents , VcfHeaderParser ( contents ) . parse_vcf_header ())","title":"populate_fields"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeader","text":"Data obtained from VCF header. Parameters: Name Type Description Default sample_id str The sample identifier from the VCF header. required assembly str The assembly information obtained from the VCF header. required chr_status bool A boolean indicating whether the VCF denotes chromosomes as chr or not. required Source code in src/pheval/prepare/create_spiked_vcf.py 78 79 80 81 82 83 84 85 86 87 88 89 90 @dataclass class VcfHeader : \"\"\"Data obtained from VCF header. Args: sample_id (str): The sample identifier from the VCF header. assembly (str): The assembly information obtained from the VCF header. chr_status (bool): A boolean indicating whether the VCF denotes chromosomes as chr or not. \"\"\" sample_id : str assembly : str chr_status : bool","title":"VcfHeader"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeaderParser","text":"Class for parsing the header of a VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class VcfHeaderParser : \"\"\"Class for parsing the header of a VCF file.\"\"\" def __init__ ( self , vcf_contents : list [ str ]): \"\"\" Initialise the VcfHeaderParser. Args: vcf_contents (list[str]): The contents of the VCF file as a list of strings. \"\"\" self . vcf_contents = vcf_contents def parse_assembly ( self ) -> tuple [ str , bool ]: \"\"\" Parse the genome assembly and format of vcf_records. Returns: Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). \"\"\" vcf_assembly = {} chr_status = False for line in self . vcf_contents : if line . startswith ( \"##contig=<ID\" ): tokens = line . split ( \",\" ) chromosome = re . sub ( r \"^.*?ID=\" , \"\" , [ token for token in tokens if \"ID=\" in token ][ 0 ] ) if \"chr\" in chromosome : chr_status = True chromosome = chromosome . replace ( \"chr\" , \"\" ) contig_length = re . sub ( \"[^0-9]+\" , \"\" , [ token for token in tokens if \"length=\" in token ][ 0 ], ) vcf_assembly [ chromosome ] = int ( contig_length ) vcf_assembly = { i : vcf_assembly [ i ] for i in vcf_assembly if i . isdigit ()} assembly = [ k for k , v in genome_assemblies . items () if v == vcf_assembly ][ 0 ] return assembly , chr_status def parse_sample_id ( self ) -> str : \"\"\" Parse the sample ID of the VCF. Returns: str: The sample ID extracted from the VCF header. \"\"\" for line in self . vcf_contents : if line . startswith ( \"#CHROM\" ): return line . split ( \" \\t \" )[ 9 ] . rstrip () def parse_vcf_header ( self ) -> VcfHeader : \"\"\" Parse the header of the VCF. Returns: VcfHeader: An instance of VcfHeader containing sample ID, assembly, and chromosome status. \"\"\" assembly , chr_status = self . parse_assembly () sample_id = self . parse_sample_id () return VcfHeader ( sample_id , assembly , chr_status )","title":"VcfHeaderParser"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeaderParser.__init__","text":"Initialise the VcfHeaderParser. Parameters: Name Type Description Default vcf_contents list [ str ] The contents of the VCF file as a list of strings. required Source code in src/pheval/prepare/create_spiked_vcf.py 115 116 117 118 119 120 121 122 def __init__ ( self , vcf_contents : list [ str ]): \"\"\" Initialise the VcfHeaderParser. Args: vcf_contents (list[str]): The contents of the VCF file as a list of strings. \"\"\" self . vcf_contents = vcf_contents","title":"__init__"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeaderParser.parse_assembly","text":"Parse the genome assembly and format of vcf_records. Returns: Type Description tuple [ str , bool ] Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). Source code in src/pheval/prepare/create_spiked_vcf.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def parse_assembly ( self ) -> tuple [ str , bool ]: \"\"\" Parse the genome assembly and format of vcf_records. Returns: Tuple[str, bool]: A tuple containing the assembly and chromosome status (True/False). \"\"\" vcf_assembly = {} chr_status = False for line in self . vcf_contents : if line . startswith ( \"##contig=<ID\" ): tokens = line . split ( \",\" ) chromosome = re . sub ( r \"^.*?ID=\" , \"\" , [ token for token in tokens if \"ID=\" in token ][ 0 ] ) if \"chr\" in chromosome : chr_status = True chromosome = chromosome . replace ( \"chr\" , \"\" ) contig_length = re . sub ( \"[^0-9]+\" , \"\" , [ token for token in tokens if \"length=\" in token ][ 0 ], ) vcf_assembly [ chromosome ] = int ( contig_length ) vcf_assembly = { i : vcf_assembly [ i ] for i in vcf_assembly if i . isdigit ()} assembly = [ k for k , v in genome_assemblies . items () if v == vcf_assembly ][ 0 ] return assembly , chr_status","title":"parse_assembly"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeaderParser.parse_sample_id","text":"Parse the sample ID of the VCF. Returns: Name Type Description str str The sample ID extracted from the VCF header. Source code in src/pheval/prepare/create_spiked_vcf.py 152 153 154 155 156 157 158 159 160 161 def parse_sample_id ( self ) -> str : \"\"\" Parse the sample ID of the VCF. Returns: str: The sample ID extracted from the VCF header. \"\"\" for line in self . vcf_contents : if line . startswith ( \"#CHROM\" ): return line . split ( \" \\t \" )[ 9 ] . rstrip ()","title":"parse_sample_id"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfHeaderParser.parse_vcf_header","text":"Parse the header of the VCF. Returns: Name Type Description VcfHeader VcfHeader An instance of VcfHeader containing sample ID, assembly, and chromosome status. Source code in src/pheval/prepare/create_spiked_vcf.py 163 164 165 166 167 168 169 170 171 172 def parse_vcf_header ( self ) -> VcfHeader : \"\"\" Parse the header of the VCF. Returns: VcfHeader: An instance of VcfHeader containing sample ID, assembly, and chromosome status. \"\"\" assembly , chr_status = self . parse_assembly () sample_id = self . parse_sample_id () return VcfHeader ( sample_id , assembly , chr_status )","title":"parse_vcf_header"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker","text":"Class for spiking proband variants into template VCF file contents. Source code in src/pheval/prepare/create_spiked_vcf.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class VcfSpiker : \"\"\"Class for spiking proband variants into template VCF file contents.\"\"\" def __init__ ( self , vcf_contents : list [ str ], proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , ): \"\"\" Initialise the VcfSpiker. Args: vcf_contents (List[str]): Contents of the template VCF file. proband_causative_variants (List[ProbandCausativeVariant]): List of proband causative variants. vcf_header (VcfHeader): The VCF header information. \"\"\" self . vcf_contents = vcf_contents self . proband_causative_variants = proband_causative_variants self . vcf_header = vcf_header def construct_variant_entry ( self , proband_variant_data : ProbandCausativeVariant ) -> List [ str ]: \"\"\" Construct variant entries. Args: proband_variant_data (ProbandCausativeVariant): Data for the proband variant. Returns: List[str]: Constructed variant entry as a list of strings. \"\"\" genotype_codes = { \"hemizygous\" : \"0/1\" , \"homozygous\" : \"1/1\" , \"heterozygous\" : \"0/1\" , \"compound heterozygous\" : \"0/1\" , } if self . vcf_header . chr_status is True and \"chr\" not in proband_variant_data . variant . chrom : proband_variant_data . variant . chrom = \"chr\" + proband_variant_data . variant . chrom return [ proband_variant_data . variant . chrom , str ( proband_variant_data . variant . pos ), \".\" , proband_variant_data . variant . ref , ( f \"< { proband_variant_data . variant . alt } >\" if proband_variant_data . variant . ref == \"N\" else proband_variant_data . variant . alt ), \"100\" , \"PASS\" , proband_variant_data . info if proband_variant_data . info else \".\" , \"GT\" , genotype_codes [ proband_variant_data . genotype . lower ()] + \" \\n \" , ] def construct_vcf_records ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: Updated VCF records containing the spiked variants. \"\"\" updated_vcf_records = copy ( self . vcf_contents ) for variant in self . proband_causative_variants : variant_entry = self . construct_variant_entry ( variant ) matching_indices = [ i for i , val in enumerate ( updated_vcf_records ) if val . split ( \" \\t \" )[ 0 ] == variant_entry [ 0 ] and int ( val . split ( \" \\t \" )[ 1 ]) < int ( variant_entry [ 1 ]) ] if matching_indices : logger . info ( f \"Successfully spiked variant { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } \" ) variant_entry_position = matching_indices [ - 1 ] + 1 else : logger . warning ( f \"Could not find entry position for { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } , \" \"inserting at end of VCF contents.\" ) variant_entry_position = len ( updated_vcf_records ) updated_vcf_records . insert ( variant_entry_position , \" \\t \" . join ( variant_entry )) return updated_vcf_records def construct_header ( self , updated_vcf_records : List [ str ]) -> List [ str ]: \"\"\" Construct the header of the VCF. Args: updated_vcf_records (List[str]): Updated VCF records. Returns: List[str]: Constructed header as a list of strings. \"\"\" updated_vcf_file = [] for line in updated_vcf_records : if line . startswith ( \"#\" ): text = line . replace ( self . vcf_header . sample_id , self . proband_causative_variants [ 0 ] . proband_id , ) else : text = line updated_vcf_file . append ( text ) return updated_vcf_file def construct_vcf ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: The complete spiked VCF file content as a list of strings. \"\"\" return self . construct_header ( self . construct_vcf_records ( template_vcf_name ))","title":"VcfSpiker"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker.__init__","text":"Initialise the VcfSpiker. Parameters: Name Type Description Default vcf_contents List [ str ] Contents of the template VCF file. required proband_causative_variants List [ ProbandCausativeVariant ] List of proband causative variants. required vcf_header VcfHeader The VCF header information. required Source code in src/pheval/prepare/create_spiked_vcf.py 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def __init__ ( self , vcf_contents : list [ str ], proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , ): \"\"\" Initialise the VcfSpiker. Args: vcf_contents (List[str]): Contents of the template VCF file. proband_causative_variants (List[ProbandCausativeVariant]): List of proband causative variants. vcf_header (VcfHeader): The VCF header information. \"\"\" self . vcf_contents = vcf_contents self . proband_causative_variants = proband_causative_variants self . vcf_header = vcf_header","title":"__init__"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker.construct_header","text":"Construct the header of the VCF. Parameters: Name Type Description Default updated_vcf_records List [ str ] Updated VCF records. required Returns: Type Description List [ str ] List[str]: Constructed header as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def construct_header ( self , updated_vcf_records : List [ str ]) -> List [ str ]: \"\"\" Construct the header of the VCF. Args: updated_vcf_records (List[str]): Updated VCF records. Returns: List[str]: Constructed header as a list of strings. \"\"\" updated_vcf_file = [] for line in updated_vcf_records : if line . startswith ( \"#\" ): text = line . replace ( self . vcf_header . sample_id , self . proband_causative_variants [ 0 ] . proband_id , ) else : text = line updated_vcf_file . append ( text ) return updated_vcf_file","title":"construct_header"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker.construct_variant_entry","text":"Construct variant entries. Parameters: Name Type Description Default proband_variant_data ProbandCausativeVariant Data for the proband variant. required Returns: Type Description List [ str ] List[str]: Constructed variant entry as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def construct_variant_entry ( self , proband_variant_data : ProbandCausativeVariant ) -> List [ str ]: \"\"\" Construct variant entries. Args: proband_variant_data (ProbandCausativeVariant): Data for the proband variant. Returns: List[str]: Constructed variant entry as a list of strings. \"\"\" genotype_codes = { \"hemizygous\" : \"0/1\" , \"homozygous\" : \"1/1\" , \"heterozygous\" : \"0/1\" , \"compound heterozygous\" : \"0/1\" , } if self . vcf_header . chr_status is True and \"chr\" not in proband_variant_data . variant . chrom : proband_variant_data . variant . chrom = \"chr\" + proband_variant_data . variant . chrom return [ proband_variant_data . variant . chrom , str ( proband_variant_data . variant . pos ), \".\" , proband_variant_data . variant . ref , ( f \"< { proband_variant_data . variant . alt } >\" if proband_variant_data . variant . ref == \"N\" else proband_variant_data . variant . alt ), \"100\" , \"PASS\" , proband_variant_data . info if proband_variant_data . info else \".\" , \"GT\" , genotype_codes [ proband_variant_data . genotype . lower ()] + \" \\n \" , ]","title":"construct_variant_entry"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker.construct_vcf","text":"Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Parameters: Name Type Description Default template_vcf_name str Name of the template VCF file. required Returns: Type Description List [ str ] List[str]: The complete spiked VCF file content as a list of strings. Source code in src/pheval/prepare/create_spiked_vcf.py 397 398 399 400 401 402 403 404 405 406 407 def construct_vcf ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct the entire spiked VCF file by incorporating the spiked variants into the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: The complete spiked VCF file content as a list of strings. \"\"\" return self . construct_header ( self . construct_vcf_records ( template_vcf_name ))","title":"construct_vcf"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfSpiker.construct_vcf_records","text":"Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Parameters: Name Type Description Default template_vcf_name str Name of the template VCF file. required Returns: Type Description List [ str ] List[str]: Updated VCF records containing the spiked variants. Source code in src/pheval/prepare/create_spiked_vcf.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def construct_vcf_records ( self , template_vcf_name : str ) -> List [ str ]: \"\"\" Construct updated VCF records by inserting spiked variants into the correct positions within the VCF. Args: template_vcf_name (str): Name of the template VCF file. Returns: List[str]: Updated VCF records containing the spiked variants. \"\"\" updated_vcf_records = copy ( self . vcf_contents ) for variant in self . proband_causative_variants : variant_entry = self . construct_variant_entry ( variant ) matching_indices = [ i for i , val in enumerate ( updated_vcf_records ) if val . split ( \" \\t \" )[ 0 ] == variant_entry [ 0 ] and int ( val . split ( \" \\t \" )[ 1 ]) < int ( variant_entry [ 1 ]) ] if matching_indices : logger . info ( f \"Successfully spiked variant { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } \" ) variant_entry_position = matching_indices [ - 1 ] + 1 else : logger . warning ( f \"Could not find entry position for { variant . variant . chrom } - { variant . variant . pos } -\" f \" { variant . variant . ref } - { variant . variant . alt } in { template_vcf_name } , \" \"inserting at end of VCF contents.\" ) variant_entry_position = len ( updated_vcf_records ) updated_vcf_records . insert ( variant_entry_position , \" \\t \" . join ( variant_entry )) return updated_vcf_records","title":"construct_vcf_records"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfWriter","text":"Class for writing VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 class VcfWriter : \"\"\"Class for writing VCF file.\"\"\" def __init__ ( self , vcf_contents : List [ str ], spiked_vcf_file_path : Path , ): \"\"\" Initialise the VcfWriter class. Args: vcf_contents (List[str]): Contents of the VCF file to be written. spiked_vcf_file_path (Path): Path to the spiked VCF file to be created. \"\"\" self . vcf_contents = vcf_contents self . spiked_vcf_file_path = spiked_vcf_file_path def write_gzip ( self ) -> None : \"\"\" Write the VCF contents to a gzipped VCF file. \"\"\" encoded_contents = [ line . encode () for line in self . vcf_contents ] with gzip . open ( self . spiked_vcf_file_path , \"wb\" ) as f : for line in encoded_contents : f . write ( line ) f . close () def write_uncompressed ( self ) -> None : \"\"\" Write the VCF contents to an uncompressed VCF file. \"\"\" with open ( self . spiked_vcf_file_path , \"w\" ) as file : file . writelines ( self . vcf_contents ) file . close () def write_vcf_file ( self ) -> None : \"\"\" Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). \"\"\" self . write_gzip () if is_gzipped ( self . spiked_vcf_file_path ) else self . write_uncompressed ()","title":"VcfWriter"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfWriter.__init__","text":"Initialise the VcfWriter class. Parameters: Name Type Description Default vcf_contents List [ str ] Contents of the VCF file to be written. required spiked_vcf_file_path Path Path to the spiked VCF file to be created. required Source code in src/pheval/prepare/create_spiked_vcf.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def __init__ ( self , vcf_contents : List [ str ], spiked_vcf_file_path : Path , ): \"\"\" Initialise the VcfWriter class. Args: vcf_contents (List[str]): Contents of the VCF file to be written. spiked_vcf_file_path (Path): Path to the spiked VCF file to be created. \"\"\" self . vcf_contents = vcf_contents self . spiked_vcf_file_path = spiked_vcf_file_path","title":"__init__"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfWriter.write_gzip","text":"Write the VCF contents to a gzipped VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 428 429 430 431 432 433 434 435 436 def write_gzip ( self ) -> None : \"\"\" Write the VCF contents to a gzipped VCF file. \"\"\" encoded_contents = [ line . encode () for line in self . vcf_contents ] with gzip . open ( self . spiked_vcf_file_path , \"wb\" ) as f : for line in encoded_contents : f . write ( line ) f . close ()","title":"write_gzip"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfWriter.write_uncompressed","text":"Write the VCF contents to an uncompressed VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 438 439 440 441 442 443 444 def write_uncompressed ( self ) -> None : \"\"\" Write the VCF contents to an uncompressed VCF file. \"\"\" with open ( self . spiked_vcf_file_path , \"w\" ) as file : file . writelines ( self . vcf_contents ) file . close ()","title":"write_uncompressed"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.VcfWriter.write_vcf_file","text":"Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). Source code in src/pheval/prepare/create_spiked_vcf.py 446 447 448 449 450 451 452 453 def write_vcf_file ( self ) -> None : \"\"\" Write the VCF file based on compression type. Determines the file writing method based on the compression type of the spiked VCF file path. Writes the VCF contents to the corresponding file format (gzip or uncompressed). \"\"\" self . write_gzip () if is_gzipped ( self . spiked_vcf_file_path ) else self . write_uncompressed ()","title":"write_vcf_file"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.check_variant_assembly","text":"Check the assembly of the variant assembly against the VCF. Parameters: Name Type Description Default proband_causative_variants List [ ProbandCausativeVariant ] A list of causative variants from the proband. required vcf_header VcfHeader An instance of VcfHeader representing the VCF file's header. required phenopacket_path Path The path to the Phenopacket file. required Raises: Type Description ValueError If there are too many or incompatible genome assemblies found. IncompatibleGenomeAssemblyError If the assembly in the Phenopacket does not match the VCF assembly. Source code in src/pheval/prepare/create_spiked_vcf.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 def check_variant_assembly ( proband_causative_variants : list [ ProbandCausativeVariant ], vcf_header : VcfHeader , phenopacket_path : Path , ) -> None : \"\"\" Check the assembly of the variant assembly against the VCF. Args: proband_causative_variants (List[ProbandCausativeVariant]): A list of causative variants from the proband. vcf_header (VcfHeader): An instance of VcfHeader representing the VCF file's header. phenopacket_path (Path): The path to the Phenopacket file. Raises: ValueError: If there are too many or incompatible genome assemblies found. IncompatibleGenomeAssemblyError: If the assembly in the Phenopacket does not match the VCF assembly. \"\"\" compatible_genome_assembly = { \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" } phenopacket_assembly = list ({ variant . assembly for variant in proband_causative_variants }) if len ( phenopacket_assembly ) > 1 : raise ValueError ( \"Too many genome assemblies!\" ) if phenopacket_assembly [ 0 ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( phenopacket_assembly , phenopacket_path ) if ( phenopacket_assembly [ 0 ] in { \"hg19\" , \"GRCh37\" } and vcf_header . assembly not in { \"hg19\" , \"GRCh37\" } ) or ( phenopacket_assembly [ 0 ] in { \"hg38\" , \"GRCh38\" } and vcf_header . assembly not in { \"hg38\" , \"GRCh38\" } ): raise IncompatibleGenomeAssemblyError ( assembly = phenopacket_assembly , phenopacket = phenopacket_path )","title":"check_variant_assembly"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.create_spiked_vcf","text":"Create a spiked VCF for a Phenopacket. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file. required phenopacket_path Path Path to the Phenopacket file. required hg19_template_vcf Path Path to the hg19 template VCF file (optional). required hg38_template_vcf Path Path to the hg38 template VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Raises: Type Description InputError If both hg19_template_vcf and hg38_template_vcf are None. Source code in src/pheval/prepare/create_spiked_vcf.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 def create_spiked_vcf ( output_dir : Path , phenopacket_path : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create a spiked VCF for a Phenopacket. Args: output_dir (Path): The directory to store the generated spiked VCF file. phenopacket_path (Path): Path to the Phenopacket file. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). Raises: InputError: If both hg19_template_vcf and hg38_template_vcf are None. \"\"\" if hg19_template_vcf is None and hg38_template_vcf is None : raise InputError ( \"Either a hg19 template vcf or hg38 template vcf must be specified\" ) hg19_vcf_info = VcfFile . populate_fields ( hg19_template_vcf ) if hg19_template_vcf else None hg38_vcf_info = VcfFile . populate_fields ( hg38_template_vcf ) if hg38_template_vcf else None spike_and_update_phenopacket ( hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , output_dir , phenopacket_path )","title":"create_spiked_vcf"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.create_spiked_vcfs","text":"Create a spiked VCF for a directory of Phenopackets. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file. required phenopacket_dir Path Path to the Phenopacket directory. required hg19_template_vcf Path Path to the template hg19 VCF file (optional). required hg38_template_vcf Path Path to the template hg19 VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Raises: Type Description InputError If both hg19_template_vcf and hg38_template_vcf are None. Source code in src/pheval/prepare/create_spiked_vcf.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def create_spiked_vcfs ( output_dir : Path , phenopacket_dir : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create a spiked VCF for a directory of Phenopackets. Args: output_dir (Path): The directory to store the generated spiked VCF file. phenopacket_dir (Path): Path to the Phenopacket directory. hg19_template_vcf (Path): Path to the template hg19 VCF file (optional). hg38_template_vcf (Path): Path to the template hg19 VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). Raises: InputError: If both hg19_template_vcf and hg38_template_vcf are None. \"\"\" if ( hg19_template_vcf is None and hg38_template_vcf is None and hg19_vcf_dir is None and hg38_vcf_dir is None ): raise InputError ( \"Need to specify a VCF!\" ) hg19_vcf_info = VcfFile . populate_fields ( hg19_template_vcf ) if hg19_template_vcf else None hg38_vcf_info = VcfFile . populate_fields ( hg38_template_vcf ) if hg38_template_vcf else None for phenopacket_path in files_with_suffix ( phenopacket_dir , \".json\" ): logger . info ( f \"Creating spiked VCF for: { phenopacket_path . name } \" ) spike_and_update_phenopacket ( hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , output_dir , phenopacket_path )","title":"create_spiked_vcfs"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.generate_spiked_vcf_file","text":"Write spiked VCF contents to a new file. Parameters: Name Type Description Default output_dir Path Path to the directory to store the generated file. required phenopacket Union [ Phenopacket , Family ] Phenopacket or Family containing causative variants. required phenopacket_path Path Path to the Phenopacket file. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: File: The generated File object representing the newly created spiked VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def generate_spiked_vcf_file ( output_dir : Path , phenopacket : Union [ Phenopacket , Family ], phenopacket_path : Path , hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> File : \"\"\" Write spiked VCF contents to a new file. Args: output_dir (Path): Path to the directory to store the generated file. phenopacket (Union[Phenopacket, Family]): Phenopacket or Family containing causative variants. phenopacket_path (Path): Path to the Phenopacket file. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: File: The generated File object representing the newly created spiked VCF file. \"\"\" vcf_assembly , spiked_vcf = spike_vcf_contents ( phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir ) spiked_vcf_path = output_dir . joinpath ( phenopacket_path . name . replace ( \".json\" , \".vcf.gz\" )) VcfWriter ( spiked_vcf , spiked_vcf_path ) . write_vcf_file () return File ( uri = urllib . parse . unquote ( spiked_vcf_path . as_uri ()), file_attributes = { \"fileFormat\" : \"vcf\" , \"genomeAssembly\" : vcf_assembly }, )","title":"generate_spiked_vcf_file"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.read_vcf","text":"Read the contents of a VCF file into memory, handling both uncompressed and gzipped files. Parameters: Name Type Description Default vcf_file Path The path to the VCF file to be read. required Returns: Type Description List [ str ] List[str]: A list containing the lines of the VCF file. Source code in src/pheval/prepare/create_spiked_vcf.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_vcf ( vcf_file : Path ) -> List [ str ]: \"\"\" Read the contents of a VCF file into memory, handling both uncompressed and gzipped files. Args: vcf_file (Path): The path to the VCF file to be read. Returns: List[str]: A list containing the lines of the VCF file. \"\"\" open_fn = gzip . open if is_gzipped ( vcf_file ) else open vcf = open_fn ( vcf_file ) vcf_contents = ( [ line . decode () for line in vcf . readlines ()] if is_gzipped ( vcf_file ) else vcf . readlines () ) vcf . close () return vcf_contents","title":"read_vcf"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.select_vcf_template","text":"Select the appropriate VCF template based on the assembly information of the proband causative variants. Parameters: Name Type Description Default phenopacket_path Path The path to the Phenopacket file. required proband_causative_variants List [ ProbandCausativeVariant ] A list of causative variants from the proband. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile CF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: Name Type Description VcfFile VcfFile The selected VCF template file based on the assembly information of the proband causative variants. Source code in src/pheval/prepare/create_spiked_vcf.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def select_vcf_template ( phenopacket_path : Path , proband_causative_variants : List [ ProbandCausativeVariant ], hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> VcfFile : \"\"\" Select the appropriate VCF template based on the assembly information of the proband causative variants. Args: phenopacket_path (Path): The path to the Phenopacket file. proband_causative_variants (List[ProbandCausativeVariant]): A list of causative variants from the proband. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): CF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: VcfFile: The selected VCF template file based on the assembly information of the proband causative variants. \"\"\" if proband_causative_variants [ 0 ] . assembly in [ \"hg19\" , \"GRCh37\" ]: if hg19_vcf_info : return hg19_vcf_info elif hg19_vcf_dir : return VcfFile . populate_fields ( random . choice ( all_files ( hg19_vcf_dir ))) else : raise InputError ( \"Must specify hg19 template VCF!\" ) elif proband_causative_variants [ 0 ] . assembly in [ \"hg38\" , \"GRCh38\" ]: if hg38_vcf_info : return hg38_vcf_info elif hg38_vcf_dir : return VcfFile . populate_fields ( random . choice ( all_files ( hg38_vcf_dir ))) else : raise InputError ( \"Must specify hg38 template VCF!\" ) else : raise IncompatibleGenomeAssemblyError ( proband_causative_variants [ 0 ] . assembly , phenopacket_path )","title":"select_vcf_template"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.spike_and_update_phenopacket","text":"Spike the VCF files with genetic variants relevant to the provided Phenopacket, update the Phenopacket accordingly, and write the updated Phenopacket to the specified output directory. Parameters: Name Type Description Default hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required output_dir Path Directory where the updated Phenopacket will be saved. required phenopacket_path Path Path to the original Phenopacket file. required Returns: Type Description None None Source code in src/pheval/prepare/create_spiked_vcf.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 def spike_and_update_phenopacket ( hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , output_dir : Path , phenopacket_path : Path , ) -> None : \"\"\" Spike the VCF files with genetic variants relevant to the provided Phenopacket, update the Phenopacket accordingly, and write the updated Phenopacket to the specified output directory. Args: hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. output_dir (Path): Directory where the updated Phenopacket will be saved. phenopacket_path (Path): Path to the original Phenopacket file. Returns: None \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) spiked_vcf_file_message = generate_spiked_vcf_file ( output_dir , phenopacket , phenopacket_path , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , ) updated_phenopacket = PhenopacketRebuilder ( phenopacket ) . add_spiked_vcf_path ( spiked_vcf_file_message ) write_phenopacket ( updated_phenopacket , phenopacket_path )","title":"spike_and_update_phenopacket"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.spike_vcf_contents","text":"Spike VCF records with variants obtained from a Phenopacket or Family. Parameters: Name Type Description Default phenopacket Union [ Phenopacket , Family ] Phenopacket or Family containing causative variants. required phenopacket_path Path Path to the Phenopacket file. required hg19_vcf_info VcfFile VCF file info for hg19 template vcf. required hg38_vcf_info VcfFile VCF file info for hg38 template vcf. required hg19_vcf_dir Path The directory containing the hg19 VCF files. required hg38_vcf_dir Path The directory containing the hg38 VCF files. required Returns: Type Description tuple [ str , List [ str ]] A tuple containing: assembly (str): The genome assembly information extracted from VCF header. modified_vcf_contents (List[str]): Modified VCF records with spiked variants. Source code in src/pheval/prepare/create_spiked_vcf.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def spike_vcf_contents ( phenopacket : Union [ Phenopacket , Family ], phenopacket_path : Path , hg19_vcf_info : VcfFile , hg38_vcf_info : VcfFile , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> tuple [ str , List [ str ]]: \"\"\" Spike VCF records with variants obtained from a Phenopacket or Family. Args: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family containing causative variants. phenopacket_path (Path): Path to the Phenopacket file. hg19_vcf_info (VcfFile): VCF file info for hg19 template vcf. hg38_vcf_info (VcfFile): VCF file info for hg38 template vcf. hg19_vcf_dir (Path): The directory containing the hg19 VCF files. hg38_vcf_dir (Path): The directory containing the hg38 VCF files. Returns: A tuple containing: assembly (str): The genome assembly information extracted from VCF header. modified_vcf_contents (List[str]): Modified VCF records with spiked variants. \"\"\" phenopacket_causative_variants = PhenopacketUtil ( phenopacket ) . causative_variants () chosen_template_vcf = select_vcf_template ( phenopacket_path , phenopacket_causative_variants , hg19_vcf_info , hg38_vcf_info , hg19_vcf_dir , hg38_vcf_dir , ) check_variant_assembly ( phenopacket_causative_variants , chosen_template_vcf . vcf_header , phenopacket_path ) return ( chosen_template_vcf . vcf_header . assembly , VcfSpiker ( chosen_template_vcf . vcf_contents , phenopacket_causative_variants , chosen_template_vcf . vcf_header , ) . construct_vcf ( chosen_template_vcf . vcf_file_name ), )","title":"spike_vcf_contents"},{"location":"api/pheval/prepare/create_spiked_vcf/#src.pheval.prepare.create_spiked_vcf.spike_vcfs","text":"Create spiked VCF from either a Phenopacket or a Phenopacket directory. Parameters: Name Type Description Default output_dir Path The directory to store the generated spiked VCF file(s). required phenopacket_path Path Path to a single Phenopacket file (optional). required phenopacket_dir Path Path to a directory containing Phenopacket files (optional). required hg19_template_vcf Path Path to the hg19 template VCF file (optional). required hg38_template_vcf Path Path to the hg38 template VCF file (optional). required hg19_vcf_dir Path The directory containing the hg19 VCF files (optional). required hg38_vcf_dir Path The directory containing the hg38 VCF files (optional). required Source code in src/pheval/prepare/create_spiked_vcf.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 def spike_vcfs ( output_dir : Path , phenopacket_path : Path , phenopacket_dir : Path , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , ) -> None : \"\"\" Create spiked VCF from either a Phenopacket or a Phenopacket directory. Args: output_dir (Path): The directory to store the generated spiked VCF file(s). phenopacket_path (Path): Path to a single Phenopacket file (optional). phenopacket_dir (Path): Path to a directory containing Phenopacket files (optional). hg19_template_vcf (Path): Path to the hg19 template VCF file (optional). hg38_template_vcf (Path): Path to the hg38 template VCF file (optional). hg19_vcf_dir (Path): The directory containing the hg19 VCF files (optional). hg38_vcf_dir (Path): The directory containing the hg38 VCF files (optional). \"\"\" start_time = time . perf_counter () logger . info ( \"Creating spiked VCFs.\" ) output_dir . mkdir ( exist_ok = True ) logger . info ( f \" Created output directory: { output_dir } \" ) if phenopacket_path is not None : logger . info ( f \"Spiking variants from { phenopacket_path } .\" ) create_spiked_vcf ( output_dir , phenopacket_path , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) elif phenopacket_dir is not None : logger . info ( f \"Spiking variants from { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) create_spiked_vcfs ( output_dir , phenopacket_dir , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) logger . info ( f \"Finished spiking! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"spike_vcfs"},{"location":"api/pheval/prepare/custom_exceptions/","text":"InputError Bases: Exception Exception raised for missing required inputs. Source code in src/pheval/prepare/custom_exceptions.py 4 5 6 7 8 9 10 11 12 13 class InputError ( Exception ): \"\"\"Exception raised for missing required inputs.\"\"\" def __init__ ( self , file , message = \"Missing required input\" ): self . file : str = file self . message : str = message super () . __init__ ( self . message ) def __str__ ( self ): return f \" { self . message } -> { self . file } \" MutuallyExclusiveOptionError Bases: Option Exception raised for when Source code in src/pheval/prepare/custom_exceptions.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MutuallyExclusiveOptionError ( Option ): \"\"\"Exception raised for when\"\"\" def __init__ ( self , * args , ** kwargs ): self . mutually_exclusive = set ( kwargs . pop ( \"mutually_exclusive\" , [])) help_ = kwargs . get ( \"help\" , \"\" ) if self . mutually_exclusive : ex_str = \", \" . join ( self . mutually_exclusive ) kwargs [ \"help\" ] = help_ + ( \" NOTE: This argument is mutually exclusive with \" \" arguments: [\" + ex_str + \"].\" ) super ( MutuallyExclusiveOptionError , self ) . __init__ ( * args , ** kwargs ) def handle_parse_result ( self , ctx , opts , args ): if self . mutually_exclusive . intersection ( opts ) and self . name in opts : raise UsageError ( \"Illegal usage: ` {} ` is mutually exclusive with \" \"arguments ` {} `.\" . format ( self . name , \", \" . join ( self . mutually_exclusive )) ) return super ( MutuallyExclusiveOptionError , self ) . handle_parse_result ( ctx , opts , args )","title":"Custom exceptions"},{"location":"api/pheval/prepare/custom_exceptions/#src.pheval.prepare.custom_exceptions.InputError","text":"Bases: Exception Exception raised for missing required inputs. Source code in src/pheval/prepare/custom_exceptions.py 4 5 6 7 8 9 10 11 12 13 class InputError ( Exception ): \"\"\"Exception raised for missing required inputs.\"\"\" def __init__ ( self , file , message = \"Missing required input\" ): self . file : str = file self . message : str = message super () . __init__ ( self . message ) def __str__ ( self ): return f \" { self . message } -> { self . file } \"","title":"InputError"},{"location":"api/pheval/prepare/custom_exceptions/#src.pheval.prepare.custom_exceptions.MutuallyExclusiveOptionError","text":"Bases: Option Exception raised for when Source code in src/pheval/prepare/custom_exceptions.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class MutuallyExclusiveOptionError ( Option ): \"\"\"Exception raised for when\"\"\" def __init__ ( self , * args , ** kwargs ): self . mutually_exclusive = set ( kwargs . pop ( \"mutually_exclusive\" , [])) help_ = kwargs . get ( \"help\" , \"\" ) if self . mutually_exclusive : ex_str = \", \" . join ( self . mutually_exclusive ) kwargs [ \"help\" ] = help_ + ( \" NOTE: This argument is mutually exclusive with \" \" arguments: [\" + ex_str + \"].\" ) super ( MutuallyExclusiveOptionError , self ) . __init__ ( * args , ** kwargs ) def handle_parse_result ( self , ctx , opts , args ): if self . mutually_exclusive . intersection ( opts ) and self . name in opts : raise UsageError ( \"Illegal usage: ` {} ` is mutually exclusive with \" \"arguments ` {} `.\" . format ( self . name , \", \" . join ( self . mutually_exclusive )) ) return super ( MutuallyExclusiveOptionError , self ) . handle_parse_result ( ctx , opts , args )","title":"MutuallyExclusiveOptionError"},{"location":"api/pheval/prepare/prepare_corpus/","text":"prepare_corpus ( phenopacket_dir , variant_analysis , gene_analysis , disease_analysis , gene_identifier , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , output_dir ) Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Parameters: Name Type Description Default phenopacket_dir Path The path to the directory containing Phenopackets. required variant_analysis bool If True, check for complete variant records in the Phenopackets. required gene_analysis bool If True, check for complete gene records in the Phenopackets. required disease_analysis bool If True, check for complete disease records in the Phenopackets. required gene_identifier str Identifier for updating gene identifiers, if applicable. required hg19_template_vcf Path Path to the hg19 template VCF file (optional), to spike variants into required hg38_template_vcf Path Path to the hg38 template VCF file (optional), to spike variants into required hg19_vcf_dir Path Path to the directory containing hg19 template VCF files (optional). required hg38_vcf_dir Path Path to the directory containing hg38 template VCF files (optional). required output_dir Path The directory to save the prepared Phenopackets and, optionally, VCF files. required Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Source code in src/pheval/prepare/prepare_corpus.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def prepare_corpus ( phenopacket_dir : Path , variant_analysis : bool , gene_analysis : bool , disease_analysis : bool , gene_identifier : str , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , output_dir : Path , ) -> None : \"\"\" Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional), to spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf or hg38_template_vcf is required. hg38_template_vcf (Path): Path to the hg38 template VCF file (optional), to spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf or hg38_template_vcf is required. hg19_vcf_dir (Path): Path to the directory containing hg19 template VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing hg38 template VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. \"\"\" start_time = time . perf_counter () logger . info ( f \"Preparing corpus for { phenopacket_dir } \" ) output_dir . joinpath ( \"phenopackets\" ) . mkdir ( exist_ok = True , parents = True ) logger . info ( f \" Created output directory: { output_dir . joinpath ( 'phenopackets' ) } \" ) identifier_map = create_gene_identifier_map () for phenopacket_path in all_files ( phenopacket_dir ): phenopacket_util = PhenopacketUtil ( phenopacket_reader ( phenopacket_path )) if not phenopacket_util . observed_phenotypic_features (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to no observed phenotypic features.\" ) continue if variant_analysis : if phenopacket_util . check_incomplete_variant_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing variant fields.\" ) continue elif phenopacket_util . check_variant_alleles (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to identical \" \"reference and alternate allele fields.\" ) if gene_analysis : if phenopacket_util . check_incomplete_gene_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing gene fields.\" ) continue if disease_analysis : if phenopacket_util . check_incomplete_disease_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing disease fields.\" ) continue logger . info ( f \" { phenopacket_path . name } OK!\" ) if hg19_template_vcf or hg38_template_vcf : output_dir . joinpath ( \"vcf\" ) . mkdir ( exist_ok = True ) logger . info ( f \" Created output directory: { output_dir . joinpath ( 'vcf' ) } \" ) logger . info ( f \"Spiking VCF for { phenopacket_path } .\" ) create_spiked_vcf ( output_dir . joinpath ( \"vcf\" ), phenopacket_path , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) if gene_identifier : logger . info ( f \"Updating gene identifiers to { gene_identifier } for { phenopacket_dir } \" ) create_updated_phenopacket ( gene_identifier , phenopacket_path , output_dir . joinpath ( \"phenopackets\" ), identifier_map , ) else : # if not updating phenopacket gene identifiers then copy phenopacket as is to output directory ( shutil . copy ( phenopacket_path , output_dir . joinpath ( f \"phenopackets/ { phenopacket_path . name } \" ) ) if phenopacket_path != output_dir . joinpath ( f \"phenopackets/ { phenopacket_path . name } \" ) else None ) logger . info ( f \"Finished preparing corpus for { phenopacket_dir } . \" f \"Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"Prepare corpus"},{"location":"api/pheval/prepare/prepare_corpus/#src.pheval.prepare.prepare_corpus.prepare_corpus","text":"Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Parameters: Name Type Description Default phenopacket_dir Path The path to the directory containing Phenopackets. required variant_analysis bool If True, check for complete variant records in the Phenopackets. required gene_analysis bool If True, check for complete gene records in the Phenopackets. required disease_analysis bool If True, check for complete disease records in the Phenopackets. required gene_identifier str Identifier for updating gene identifiers, if applicable. required hg19_template_vcf Path Path to the hg19 template VCF file (optional), to spike variants into required hg38_template_vcf Path Path to the hg38 template VCF file (optional), to spike variants into required hg19_vcf_dir Path Path to the directory containing hg19 template VCF files (optional). required hg38_vcf_dir Path Path to the directory containing hg38 template VCF files (optional). required output_dir Path The directory to save the prepared Phenopackets and, optionally, VCF files. required Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. Source code in src/pheval/prepare/prepare_corpus.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def prepare_corpus ( phenopacket_dir : Path , variant_analysis : bool , gene_analysis : bool , disease_analysis : bool , gene_identifier : str , hg19_template_vcf : Path , hg38_template_vcf : Path , hg19_vcf_dir : Path , hg38_vcf_dir : Path , output_dir : Path , ) -> None : \"\"\" Prepare a corpus of Phenopackets for analysis, optionally checking for complete variant records and updating gene identifiers. Args: phenopacket_dir (Path): The path to the directory containing Phenopackets. variant_analysis (bool): If True, check for complete variant records in the Phenopackets. gene_analysis (bool): If True, check for complete gene records in the Phenopackets. disease_analysis (bool): If True, check for complete disease records in the Phenopackets. gene_identifier (str): Identifier for updating gene identifiers, if applicable. hg19_template_vcf (Path): Path to the hg19 template VCF file (optional), to spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf or hg38_template_vcf is required. hg38_template_vcf (Path): Path to the hg38 template VCF file (optional), to spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf or hg38_template_vcf is required. hg19_vcf_dir (Path): Path to the directory containing hg19 template VCF files (optional). hg38_vcf_dir (Path): Path to the directory containing hg38 template VCF files (optional). output_dir (Path): The directory to save the prepared Phenopackets and, optionally, VCF files. Notes: To spike variants into VCFs for variant-based analysis at least one of hg19_template_vcf, hg38_template_vcf, hg19_vcf_dir or hg38_vcf_dir is required. \"\"\" start_time = time . perf_counter () logger . info ( f \"Preparing corpus for { phenopacket_dir } \" ) output_dir . joinpath ( \"phenopackets\" ) . mkdir ( exist_ok = True , parents = True ) logger . info ( f \" Created output directory: { output_dir . joinpath ( 'phenopackets' ) } \" ) identifier_map = create_gene_identifier_map () for phenopacket_path in all_files ( phenopacket_dir ): phenopacket_util = PhenopacketUtil ( phenopacket_reader ( phenopacket_path )) if not phenopacket_util . observed_phenotypic_features (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to no observed phenotypic features.\" ) continue if variant_analysis : if phenopacket_util . check_incomplete_variant_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing variant fields.\" ) continue elif phenopacket_util . check_variant_alleles (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to identical \" \"reference and alternate allele fields.\" ) if gene_analysis : if phenopacket_util . check_incomplete_gene_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing gene fields.\" ) continue if disease_analysis : if phenopacket_util . check_incomplete_disease_record (): logger . warning ( f \"Removed { phenopacket_path . name } from the corpus due to missing disease fields.\" ) continue logger . info ( f \" { phenopacket_path . name } OK!\" ) if hg19_template_vcf or hg38_template_vcf : output_dir . joinpath ( \"vcf\" ) . mkdir ( exist_ok = True ) logger . info ( f \" Created output directory: { output_dir . joinpath ( 'vcf' ) } \" ) logger . info ( f \"Spiking VCF for { phenopacket_path } .\" ) create_spiked_vcf ( output_dir . joinpath ( \"vcf\" ), phenopacket_path , hg19_template_vcf , hg38_template_vcf , hg19_vcf_dir , hg38_vcf_dir , ) if gene_identifier : logger . info ( f \"Updating gene identifiers to { gene_identifier } for { phenopacket_dir } \" ) create_updated_phenopacket ( gene_identifier , phenopacket_path , output_dir . joinpath ( \"phenopackets\" ), identifier_map , ) else : # if not updating phenopacket gene identifiers then copy phenopacket as is to output directory ( shutil . copy ( phenopacket_path , output_dir . joinpath ( f \"phenopackets/ { phenopacket_path . name } \" ) ) if phenopacket_path != output_dir . joinpath ( f \"phenopackets/ { phenopacket_path . name } \" ) else None ) logger . info ( f \"Finished preparing corpus for { phenopacket_dir } . \" f \"Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"prepare_corpus"},{"location":"api/pheval/prepare/update_phenopacket/","text":"create_updated_phenopacket ( gene_identifier , phenopacket_path , output_dir , identifier_map = None ) Update the gene context within the interpretations for a Phenopacket and writes the updated Phenopacket. Parameters: Name Type Description Default gene_identifier str Identifier used to update the gene context. required phenopacket_path Path The path to the input Phenopacket file. required output_dir Path The directory where the updated Phenopacket will be written. required identifier_map DataFrame The gene identifier map used for updating. None Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def create_updated_phenopacket ( gene_identifier : str , phenopacket_path : Path , output_dir : Path , identifier_map : pl . DataFrame = None , ) -> None : \"\"\" Update the gene context within the interpretations for a Phenopacket and writes the updated Phenopacket. Args: gene_identifier (str): Identifier used to update the gene context. phenopacket_path (Path): The path to the input Phenopacket file. output_dir (Path): The directory where the updated Phenopacket will be written. identifier_map (pl.DataFrame): The gene identifier map used for updating. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" identifier_map = create_gene_identifier_map () if identifier_map is None else identifier_map updated_phenopacket = update_outdated_gene_context ( phenopacket_path , gene_identifier , identifier_map ) write_phenopacket ( updated_phenopacket , output_dir . joinpath ( phenopacket_path . name )) create_updated_phenopackets ( gene_identifier , phenopacket_dir , output_dir ) Update the gene context within the interpretations for a directory of Phenopackets and writes the updated Phenopackets. Parameters: Name Type Description Default gene_identifier str Identifier used to update the gene context. required phenopacket_dir Path The path to the input Phenopacket directory. required output_dir Path The directory where the updated Phenopackets will be written. required Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def create_updated_phenopackets ( gene_identifier : str , phenopacket_dir : Path , output_dir : Path ) -> None : \"\"\" Update the gene context within the interpretations for a directory of Phenopackets and writes the updated Phenopackets. Args: gene_identifier (str): Identifier used to update the gene context. phenopacket_dir (Path): The path to the input Phenopacket directory. output_dir (Path): The directory where the updated Phenopackets will be written. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" identifier_map = create_gene_identifier_map () for phenopacket_path in all_files ( phenopacket_dir ): logger . info ( f \"Updating gene context for: { phenopacket_path . name } \" ) updated_phenopacket = update_outdated_gene_context ( phenopacket_path , gene_identifier , identifier_map ) write_phenopacket ( updated_phenopacket , output_dir . joinpath ( phenopacket_path . name )) update_outdated_gene_context ( phenopacket_path , gene_identifier , identifier_map ) Update the gene context of the Phenopacket. Parameters: Name Type Description Default phenopacket_path Path The path to the Phenopacket file. required gene_identifier str Identifier to update the gene context. required identifier_map DataFrame The gene identifier map used for updating. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The updated Phenopacket or Family. Notes: This function updates the gene context within the Phenopacket or Family instance. The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def update_outdated_gene_context ( phenopacket_path : Path , gene_identifier : str , identifier_map : pl . DataFrame ) -> Union [ Phenopacket , Family ]: \"\"\" Update the gene context of the Phenopacket. Args: phenopacket_path (Path): The path to the Phenopacket file. gene_identifier (str): Identifier to update the gene context. identifier_map (pl.DataFrame): The gene identifier map used for updating. Returns: Union[Phenopacket, Family]: The updated Phenopacket or Family. Notes: This function updates the gene context within the Phenopacket or Family instance. The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) interpretations = PhenopacketUtil ( phenopacket ) . interpretations () updated_interpretations = GeneIdentifierUpdater ( identifier_map = identifier_map , gene_identifier = gene_identifier ) . update_genomic_interpretations_gene_identifier ( interpretations , phenopacket_path ) return PhenopacketRebuilder ( phenopacket ) . update_interpretations ( updated_interpretations ) update_phenopackets ( gene_identifier , phenopacket_path , phenopacket_dir , output_dir ) Update the gene identifiers in either a single phenopacket or a directory of phenopackets. Parameters: Name Type Description Default gene_identifier str The gene identifier to be updated. required phenopacket_path Path The path to a single Phenopacket file. required phenopacket_dir Path The directory containing multiple Phenopacket files. required output_dir Path The output directory to save the updated Phenopacket files. required Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update_phenopackets ( gene_identifier : str , phenopacket_path : Path , phenopacket_dir : Path , output_dir : Path ) -> None : \"\"\" Update the gene identifiers in either a single phenopacket or a directory of phenopackets. Args: gene_identifier (str): The gene identifier to be updated. phenopacket_path (Path): The path to a single Phenopacket file. phenopacket_dir (Path): The directory containing multiple Phenopacket files. output_dir (Path): The output directory to save the updated Phenopacket files. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" start_time = time . perf_counter () logger . info ( \"Updating phenopackets.\" ) output_dir . mkdir ( exist_ok = True ) logger . info ( f \"Created directory { output_dir } .\" ) logger . info ( f \"Gene identifier set to: { gene_identifier } .\" ) if phenopacket_path is not None : logger . info ( f \"Updating { phenopacket_path } .\" ) create_updated_phenopacket ( gene_identifier , phenopacket_path , output_dir ) elif phenopacket_dir is not None : logger . info ( f \"Updating { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) create_updated_phenopackets ( gene_identifier , phenopacket_dir , output_dir ) logger . info ( f \"Updating finished! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"Update phenopacket"},{"location":"api/pheval/prepare/update_phenopacket/#src.pheval.prepare.update_phenopacket.create_updated_phenopacket","text":"Update the gene context within the interpretations for a Phenopacket and writes the updated Phenopacket. Parameters: Name Type Description Default gene_identifier str Identifier used to update the gene context. required phenopacket_path Path The path to the input Phenopacket file. required output_dir Path The directory where the updated Phenopacket will be written. required identifier_map DataFrame The gene identifier map used for updating. None Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def create_updated_phenopacket ( gene_identifier : str , phenopacket_path : Path , output_dir : Path , identifier_map : pl . DataFrame = None , ) -> None : \"\"\" Update the gene context within the interpretations for a Phenopacket and writes the updated Phenopacket. Args: gene_identifier (str): Identifier used to update the gene context. phenopacket_path (Path): The path to the input Phenopacket file. output_dir (Path): The directory where the updated Phenopacket will be written. identifier_map (pl.DataFrame): The gene identifier map used for updating. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" identifier_map = create_gene_identifier_map () if identifier_map is None else identifier_map updated_phenopacket = update_outdated_gene_context ( phenopacket_path , gene_identifier , identifier_map ) write_phenopacket ( updated_phenopacket , output_dir . joinpath ( phenopacket_path . name ))","title":"create_updated_phenopacket"},{"location":"api/pheval/prepare/update_phenopacket/#src.pheval.prepare.update_phenopacket.create_updated_phenopackets","text":"Update the gene context within the interpretations for a directory of Phenopackets and writes the updated Phenopackets. Parameters: Name Type Description Default gene_identifier str Identifier used to update the gene context. required phenopacket_dir Path The path to the input Phenopacket directory. required output_dir Path The directory where the updated Phenopackets will be written. required Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def create_updated_phenopackets ( gene_identifier : str , phenopacket_dir : Path , output_dir : Path ) -> None : \"\"\" Update the gene context within the interpretations for a directory of Phenopackets and writes the updated Phenopackets. Args: gene_identifier (str): Identifier used to update the gene context. phenopacket_dir (Path): The path to the input Phenopacket directory. output_dir (Path): The directory where the updated Phenopackets will be written. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" identifier_map = create_gene_identifier_map () for phenopacket_path in all_files ( phenopacket_dir ): logger . info ( f \"Updating gene context for: { phenopacket_path . name } \" ) updated_phenopacket = update_outdated_gene_context ( phenopacket_path , gene_identifier , identifier_map ) write_phenopacket ( updated_phenopacket , output_dir . joinpath ( phenopacket_path . name ))","title":"create_updated_phenopackets"},{"location":"api/pheval/prepare/update_phenopacket/#src.pheval.prepare.update_phenopacket.update_outdated_gene_context","text":"Update the gene context of the Phenopacket. Parameters: Name Type Description Default phenopacket_path Path The path to the Phenopacket file. required gene_identifier str Identifier to update the gene context. required identifier_map DataFrame The gene identifier map used for updating. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The updated Phenopacket or Family. Notes: This function updates the gene context within the Phenopacket or Family instance. The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def update_outdated_gene_context ( phenopacket_path : Path , gene_identifier : str , identifier_map : pl . DataFrame ) -> Union [ Phenopacket , Family ]: \"\"\" Update the gene context of the Phenopacket. Args: phenopacket_path (Path): The path to the Phenopacket file. gene_identifier (str): Identifier to update the gene context. identifier_map (pl.DataFrame): The gene identifier map used for updating. Returns: Union[Phenopacket, Family]: The updated Phenopacket or Family. Notes: This function updates the gene context within the Phenopacket or Family instance. The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" phenopacket = phenopacket_reader ( phenopacket_path ) interpretations = PhenopacketUtil ( phenopacket ) . interpretations () updated_interpretations = GeneIdentifierUpdater ( identifier_map = identifier_map , gene_identifier = gene_identifier ) . update_genomic_interpretations_gene_identifier ( interpretations , phenopacket_path ) return PhenopacketRebuilder ( phenopacket ) . update_interpretations ( updated_interpretations )","title":"update_outdated_gene_context"},{"location":"api/pheval/prepare/update_phenopacket/#src.pheval.prepare.update_phenopacket.update_phenopackets","text":"Update the gene identifiers in either a single phenopacket or a directory of phenopackets. Parameters: Name Type Description Default gene_identifier str The gene identifier to be updated. required phenopacket_path Path The path to a single Phenopacket file. required phenopacket_dir Path The directory containing multiple Phenopacket files. required output_dir Path The output directory to save the updated Phenopacket files. required Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. Source code in src/pheval/prepare/update_phenopacket.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update_phenopackets ( gene_identifier : str , phenopacket_path : Path , phenopacket_dir : Path , output_dir : Path ) -> None : \"\"\" Update the gene identifiers in either a single phenopacket or a directory of phenopackets. Args: gene_identifier (str): The gene identifier to be updated. phenopacket_path (Path): The path to a single Phenopacket file. phenopacket_dir (Path): The directory containing multiple Phenopacket files. output_dir (Path): The output directory to save the updated Phenopacket files. Notes: The gene_identifier parameter should be chosen from ensembl_id, hgnc_id, or entrez_id to update to the current gene identifier in the Phenopacket. We recommend using the ENSEMBL namespace to describe the gene identifiers. \"\"\" start_time = time . perf_counter () logger . info ( \"Updating phenopackets.\" ) output_dir . mkdir ( exist_ok = True ) logger . info ( f \"Created directory { output_dir } .\" ) logger . info ( f \"Gene identifier set to: { gene_identifier } .\" ) if phenopacket_path is not None : logger . info ( f \"Updating { phenopacket_path } .\" ) create_updated_phenopacket ( gene_identifier , phenopacket_path , output_dir ) elif phenopacket_dir is not None : logger . info ( f \"Updating { len ( all_files ( phenopacket_dir )) } phenopackets in { phenopacket_dir } .\" ) create_updated_phenopackets ( gene_identifier , phenopacket_dir , output_dir ) logger . info ( f \"Updating finished! Total time: { time . perf_counter () - start_time : .2f } seconds.\" )","title":"update_phenopackets"},{"location":"api/pheval/runners/runner/","text":"Runners Module DefaultPhEvalRunner dataclass Bases: PhEvalRunner DefaultPhEvalRunner Parameters: Name Type Description Default PhEvalRunner PhEvalRunner Abstract PhEvalRunnerClass required Source code in src/pheval/runners/runner.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class DefaultPhEvalRunner ( PhEvalRunner ): \"\"\"DefaultPhEvalRunner Args: PhEvalRunner (PhEvalRunner): Abstract PhEvalRunnerClass \"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): print ( \"preparing\" ) def run ( self ): print ( \"running\" ) def post_process ( self ): print ( \"post processing\" ) PhEvalRunner dataclass Bases: ABC PhEvalRunner Class Source code in src/pheval/runners/runner.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @dataclass class PhEvalRunner ( ABC ): \"\"\"PhEvalRunner Class\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str directory_path = None input_dir_config = None _meta_data = None __raw_results_dir = \"raw_results/\" __pheval_gene_results_dir = \"pheval_gene_results/\" __pheval_variant_results_dir = \"pheval_variant_results/\" __pheval_disease_results_dir = \"pheval_disease_results/\" __tool_input_commands_dir = \"tool_input_commands/\" __run_meta_data_file = \"results.yml\" def __post_init__ ( self ): self . input_dir_config = parse_input_dir_config ( self . input_dir ) def _get_tool ( self ): return self . input_dir_config . tool def _get_variant_analysis ( self ): return self . input_dir_config . variant_analysis def _get_gene_analysis ( self ): return self . input_dir_config . gene_analysis def _get_disease_analysis ( self ): return self . input_dir_config . disease_analysis @property def tool_input_commands_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __tool_input_commands_dir ) @tool_input_commands_dir . setter def tool_input_commands_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def raw_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __raw_results_dir ) @raw_results_dir . setter def raw_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_gene_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_gene_results_dir ) @pheval_gene_results_dir . setter def pheval_gene_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_variant_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_variant_results_dir ) @pheval_variant_results_dir . setter def pheval_variant_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_disease_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_disease_results_dir ) @pheval_disease_results_dir . setter def pheval_disease_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True ) @property def meta_data ( self ): self . _meta_data = BasicOutputRunMetaData ( tool = self . input_dir_config . tool , tool_version = self . version , config = f \" { Path ( self . input_dir ) . parent . name } / { Path ( self . input_dir ) . name } \" , run_timestamp = datetime . now () . timestamp (), corpus = f \" { Path ( self . testdata_dir ) . parent . name } / { Path ( self . testdata_dir ) . name } \" , mondo_download_date = get_resource_timestamp ( \"mondo.sssom.tsv\" ), hgnc_download_date = get_resource_timestamp ( \"hgnc_complete_set.txt\" ), ) return self . _meta_data @meta_data . setter def meta_data ( self , meta_data ): self . _meta_data = meta_data @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\" @abstractmethod def run ( self ): \"\"\"run\"\"\" @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\" def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data build_output_directory_structure () build output directory structure Source code in src/pheval/runners/runner.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True ) construct_meta_data () Construct run output meta data Source code in src/pheval/runners/runner.py 135 136 137 def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data post_process () abstractmethod post_process Source code in src/pheval/runners/runner.py 131 132 133 @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\" prepare () abstractmethod prepare Source code in src/pheval/runners/runner.py 123 124 125 @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\" run () abstractmethod run Source code in src/pheval/runners/runner.py 127 128 129 @abstractmethod def run ( self ): \"\"\"run\"\"\"","title":"Runner"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.DefaultPhEvalRunner","text":"Bases: PhEvalRunner DefaultPhEvalRunner Parameters: Name Type Description Default PhEvalRunner PhEvalRunner Abstract PhEvalRunnerClass required Source code in src/pheval/runners/runner.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class DefaultPhEvalRunner ( PhEvalRunner ): \"\"\"DefaultPhEvalRunner Args: PhEvalRunner (PhEvalRunner): Abstract PhEvalRunnerClass \"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str def prepare ( self ): print ( \"preparing\" ) def run ( self ): print ( \"running\" ) def post_process ( self ): print ( \"post processing\" )","title":"DefaultPhEvalRunner"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner","text":"Bases: ABC PhEvalRunner Class Source code in src/pheval/runners/runner.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @dataclass class PhEvalRunner ( ABC ): \"\"\"PhEvalRunner Class\"\"\" input_dir : Path testdata_dir : Path tmp_dir : Path output_dir : Path config_file : Path version : str directory_path = None input_dir_config = None _meta_data = None __raw_results_dir = \"raw_results/\" __pheval_gene_results_dir = \"pheval_gene_results/\" __pheval_variant_results_dir = \"pheval_variant_results/\" __pheval_disease_results_dir = \"pheval_disease_results/\" __tool_input_commands_dir = \"tool_input_commands/\" __run_meta_data_file = \"results.yml\" def __post_init__ ( self ): self . input_dir_config = parse_input_dir_config ( self . input_dir ) def _get_tool ( self ): return self . input_dir_config . tool def _get_variant_analysis ( self ): return self . input_dir_config . variant_analysis def _get_gene_analysis ( self ): return self . input_dir_config . gene_analysis def _get_disease_analysis ( self ): return self . input_dir_config . disease_analysis @property def tool_input_commands_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __tool_input_commands_dir ) @tool_input_commands_dir . setter def tool_input_commands_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def raw_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __raw_results_dir ) @raw_results_dir . setter def raw_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_gene_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_gene_results_dir ) @pheval_gene_results_dir . setter def pheval_gene_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_variant_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_variant_results_dir ) @pheval_variant_results_dir . setter def pheval_variant_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) @property def pheval_disease_results_dir ( self ): return Path ( self . output_dir ) . joinpath ( self . __pheval_disease_results_dir ) @pheval_disease_results_dir . setter def pheval_disease_results_dir ( self , directory_path ): self . directory_path = Path ( directory_path ) def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True ) @property def meta_data ( self ): self . _meta_data = BasicOutputRunMetaData ( tool = self . input_dir_config . tool , tool_version = self . version , config = f \" { Path ( self . input_dir ) . parent . name } / { Path ( self . input_dir ) . name } \" , run_timestamp = datetime . now () . timestamp (), corpus = f \" { Path ( self . testdata_dir ) . parent . name } / { Path ( self . testdata_dir ) . name } \" , mondo_download_date = get_resource_timestamp ( \"mondo.sssom.tsv\" ), hgnc_download_date = get_resource_timestamp ( \"hgnc_complete_set.txt\" ), ) return self . _meta_data @meta_data . setter def meta_data ( self , meta_data ): self . _meta_data = meta_data @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\" @abstractmethod def run ( self ): \"\"\"run\"\"\" @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\" def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data","title":"PhEvalRunner"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner.build_output_directory_structure","text":"build output directory structure Source code in src/pheval/runners/runner.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def build_output_directory_structure ( self ): \"\"\"build output directory structure\"\"\" logger . info ( f \"Building output directory structure for { self . input_dir_config . tool } \" f \"version { self . input_dir_config . tool_version } \" ) self . tool_input_commands_dir . mkdir ( exist_ok = True ) self . raw_results_dir . mkdir ( exist_ok = True ) if self . _get_variant_analysis (): self . pheval_variant_results_dir . mkdir ( exist_ok = True ) if self . _get_gene_analysis (): self . pheval_gene_results_dir . mkdir ( exist_ok = True ) if self . _get_disease_analysis (): self . pheval_disease_results_dir . mkdir ( exist_ok = True )","title":"build_output_directory_structure"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner.construct_meta_data","text":"Construct run output meta data Source code in src/pheval/runners/runner.py 135 136 137 def construct_meta_data ( self ): \"\"\"Construct run output meta data\"\"\" return self . meta_data","title":"construct_meta_data"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner.post_process","text":"post_process Source code in src/pheval/runners/runner.py 131 132 133 @abstractmethod def post_process ( self ): \"\"\"post_process\"\"\"","title":"post_process"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner.prepare","text":"prepare Source code in src/pheval/runners/runner.py 123 124 125 @abstractmethod def prepare ( self ) -> str : \"\"\"prepare\"\"\"","title":"prepare"},{"location":"api/pheval/runners/runner/#src.pheval.runners.runner.PhEvalRunner.run","text":"run Source code in src/pheval/runners/runner.py 127 128 129 @abstractmethod def run ( self ): \"\"\"run\"\"\"","title":"run"},{"location":"api/pheval/utils/exomiser/","text":"semsim_to_exomiserdb ( input_path , object_prefix , subject_prefix , db_path ) ingests semsim file into exomiser phenotypic database Parameters: Name Type Description Default input_path Path semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv required object_prefix str object prefix. e.g. MP required subject_prefix str subject prefix e.g HP required db_path Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) required Source code in src/pheval/utils/exomiser.py 6 7 8 9 10 11 12 13 14 15 16 def semsim_to_exomiserdb ( input_path : Path , object_prefix : str , subject_prefix : str , db_path : Path ): \"\"\"ingests semsim file into exomiser phenotypic database Args: input_path (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) \"\"\" exomiserdb = ExomiserDB ( db_path ) exomiserdb . import_from_semsim_file ( input_path , object_prefix , subject_prefix )","title":"Exomiser"},{"location":"api/pheval/utils/exomiser/#src.pheval.utils.exomiser.semsim_to_exomiserdb","text":"ingests semsim file into exomiser phenotypic database Parameters: Name Type Description Default input_path Path semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv required object_prefix str object prefix. e.g. MP required subject_prefix str subject prefix e.g HP required db_path Path Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) required Source code in src/pheval/utils/exomiser.py 6 7 8 9 10 11 12 13 14 15 16 def semsim_to_exomiserdb ( input_path : Path , object_prefix : str , subject_prefix : str , db_path : Path ): \"\"\"ingests semsim file into exomiser phenotypic database Args: input_path (Path): semsim input file. e.g phenio-plus-hp-mp.0.semsimian.tsv object_prefix (str): object prefix. e.g. MP subject_prefix (str): subject prefix e.g HP db_path (Path): Exomiser Phenotypic Database Folder Path. (e.g. /exomiser_folder/2209_phenotype/2209_phenotype/) \"\"\" exomiserdb = ExomiserDB ( db_path ) exomiserdb . import_from_semsim_file ( input_path , object_prefix , subject_prefix )","title":"semsim_to_exomiserdb"},{"location":"api/pheval/utils/file_utils/","text":"all_files ( directory ) Obtains all files from a given directory. Parameters: Name Type Description Default directory Path The directory path. required Returns: Type Description list [ Path ] list[Path]: A list of Path objects representing all files in the directory. Source code in src/pheval/utils/file_utils.py 31 32 33 34 35 36 37 38 39 40 41 42 43 def all_files ( directory : Path ) -> list [ Path ]: \"\"\" Obtains all files from a given directory. Args: directory (Path): The directory path. Returns: list[Path]: A list of Path objects representing all files in the directory. \"\"\" files = [ file_path for file_path in directory . iterdir ()] files . sort () return files ensure_columns_exists ( cols , dataframes , err_message = '' ) Ensures the columns exist in dataframes passed as argument (e.g) \" ensure_columns_exists( cols=['column_a', 'column_b, 'column_c'], err_message=\"Custom error message if any column doesn't exist in any dataframe passed as argument\", dataframes=[data_frame1, data_frame2], ) \" Source code in src/pheval/utils/file_utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def ensure_columns_exists ( cols : list , dataframes : List [ pd . DataFrame ], err_message : str = \"\" ): \"\"\"Ensures the columns exist in dataframes passed as argument (e.g) \" ensure_columns_exists( cols=['column_a', 'column_b, 'column_c'], err_message=\"Custom error message if any column doesn't exist in any dataframe passed as argument\", dataframes=[data_frame1, data_frame2], ) \" \"\"\" flat_cols = list ( itertools . chain ( cols )) if not dataframes or not flat_cols : return if err_message : err_msg = f \"\"\"columns: { \", \" . join ( flat_cols [: - 1 ]) } and { flat_cols [ - 1 ] } { err_message } \"\"\" else : err_msg = f \"\"\"columns: { \", \" . join ( flat_cols [: - 1 ]) } and { flat_cols [ - 1 ] } \\ - must be present in both left and right files\"\"\" for dataframe in dataframes : if not all ( x in dataframe . columns for x in flat_cols ): raise ValueError ( err_msg ) ensure_file_exists ( * files ) Ensures the existence of files passed as parameter Raises: FileNotFoundError: If any file passed as a parameter doesn't exist a FileNotFound Exception will be raised Source code in src/pheval/utils/file_utils.py 73 74 75 76 77 78 79 80 def ensure_file_exists ( * files : str ): \"\"\"Ensures the existence of files passed as parameter Raises: FileNotFoundError: If any file passed as a parameter doesn't exist a FileNotFound Exception will be raised \"\"\" for file in files : if not path . isfile ( file ): raise FileNotFoundError ( f \"File { file } not found\" ) files_with_suffix ( directory , suffix ) Obtains all files ending in a specified suffix from a given directory. Parameters: Name Type Description Default directory Path The directory path. required suffix str The specified suffix to filter files. required Returns: Type Description list [ Path ] list[Path]: A list of Path objects representing files with the specified suffix. Source code in src/pheval/utils/file_utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def files_with_suffix ( directory : Path , suffix : str ) -> list [ Path ]: \"\"\" Obtains all files ending in a specified suffix from a given directory. Args: directory (Path): The directory path. suffix (str): The specified suffix to filter files. Returns: list[Path]: A list of Path objects representing files with the specified suffix. \"\"\" files = [ file_path for file_path in directory . iterdir () if file_path . suffix == suffix ] files . sort () return files is_gzipped ( file_path ) Confirms whether a file is gzipped. Parameters: Name Type Description Default file_path Path The path to the file. required Returns: Name Type Description bool bool True if the file is gzipped, False otherwise. Source code in src/pheval/utils/file_utils.py 46 47 48 49 50 51 52 53 54 55 56 def is_gzipped ( file_path : Path ) -> bool : \"\"\" Confirms whether a file is gzipped. Args: file_path (Path): The path to the file. Returns: bool: True if the file is gzipped, False otherwise. \"\"\" return file_path . name . endswith ( \".gz\" ) normalise_file_name ( file_path ) Normalises the file name by removing diacritical marks (accents) from Unicode characters. Parameters: Name Type Description Default file_path Path The path to the file. required Returns: Name Type Description str str The normalised file name without diacritical marks. Source code in src/pheval/utils/file_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 def normalise_file_name ( file_path : Path ) -> str : \"\"\" Normalises the file name by removing diacritical marks (accents) from Unicode characters. Args: file_path (Path): The path to the file. Returns: str: The normalised file name without diacritical marks. \"\"\" normalised_file_name = unicodedata . normalize ( \"NFD\" , str ( file_path )) return re . sub ( \"[ \\u0300 - \\u036f ]\" , \"\" , normalised_file_name ) write_metadata ( output_dir , meta_data ) Write the metadata for a run to a YAML file. Parameters: Name Type Description Default output_dir Path The directory where the metadata file will be saved. required meta_data BasicOutputRunMetaData The metadata to be written. required Source code in src/pheval/utils/file_utils.py 108 109 110 111 112 113 114 115 116 117 118 def write_metadata ( output_dir : Path , meta_data : BasicOutputRunMetaData ) -> None : \"\"\" Write the metadata for a run to a YAML file. Args: output_dir (Path): The directory where the metadata file will be saved. meta_data (BasicOutputRunMetaData): The metadata to be written. \"\"\" with open ( Path ( output_dir ) . joinpath ( \"results.yml\" ), \"w\" ) as metadata_file : yaml . dump ( to_dict ( meta_data ), metadata_file , sort_keys = False , default_style = \"\" ) metadata_file . close ()","title":"File utils"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.all_files","text":"Obtains all files from a given directory. Parameters: Name Type Description Default directory Path The directory path. required Returns: Type Description list [ Path ] list[Path]: A list of Path objects representing all files in the directory. Source code in src/pheval/utils/file_utils.py 31 32 33 34 35 36 37 38 39 40 41 42 43 def all_files ( directory : Path ) -> list [ Path ]: \"\"\" Obtains all files from a given directory. Args: directory (Path): The directory path. Returns: list[Path]: A list of Path objects representing all files in the directory. \"\"\" files = [ file_path for file_path in directory . iterdir ()] files . sort () return files","title":"all_files"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.ensure_columns_exists","text":"Ensures the columns exist in dataframes passed as argument (e.g) \" ensure_columns_exists( cols=['column_a', 'column_b, 'column_c'], err_message=\"Custom error message if any column doesn't exist in any dataframe passed as argument\", dataframes=[data_frame1, data_frame2], ) \" Source code in src/pheval/utils/file_utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def ensure_columns_exists ( cols : list , dataframes : List [ pd . DataFrame ], err_message : str = \"\" ): \"\"\"Ensures the columns exist in dataframes passed as argument (e.g) \" ensure_columns_exists( cols=['column_a', 'column_b, 'column_c'], err_message=\"Custom error message if any column doesn't exist in any dataframe passed as argument\", dataframes=[data_frame1, data_frame2], ) \" \"\"\" flat_cols = list ( itertools . chain ( cols )) if not dataframes or not flat_cols : return if err_message : err_msg = f \"\"\"columns: { \", \" . join ( flat_cols [: - 1 ]) } and { flat_cols [ - 1 ] } { err_message } \"\"\" else : err_msg = f \"\"\"columns: { \", \" . join ( flat_cols [: - 1 ]) } and { flat_cols [ - 1 ] } \\ - must be present in both left and right files\"\"\" for dataframe in dataframes : if not all ( x in dataframe . columns for x in flat_cols ): raise ValueError ( err_msg )","title":"ensure_columns_exists"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.ensure_file_exists","text":"Ensures the existence of files passed as parameter Raises: FileNotFoundError: If any file passed as a parameter doesn't exist a FileNotFound Exception will be raised Source code in src/pheval/utils/file_utils.py 73 74 75 76 77 78 79 80 def ensure_file_exists ( * files : str ): \"\"\"Ensures the existence of files passed as parameter Raises: FileNotFoundError: If any file passed as a parameter doesn't exist a FileNotFound Exception will be raised \"\"\" for file in files : if not path . isfile ( file ): raise FileNotFoundError ( f \"File { file } not found\" )","title":"ensure_file_exists"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.files_with_suffix","text":"Obtains all files ending in a specified suffix from a given directory. Parameters: Name Type Description Default directory Path The directory path. required suffix str The specified suffix to filter files. required Returns: Type Description list [ Path ] list[Path]: A list of Path objects representing files with the specified suffix. Source code in src/pheval/utils/file_utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def files_with_suffix ( directory : Path , suffix : str ) -> list [ Path ]: \"\"\" Obtains all files ending in a specified suffix from a given directory. Args: directory (Path): The directory path. suffix (str): The specified suffix to filter files. Returns: list[Path]: A list of Path objects representing files with the specified suffix. \"\"\" files = [ file_path for file_path in directory . iterdir () if file_path . suffix == suffix ] files . sort () return files","title":"files_with_suffix"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.is_gzipped","text":"Confirms whether a file is gzipped. Parameters: Name Type Description Default file_path Path The path to the file. required Returns: Name Type Description bool bool True if the file is gzipped, False otherwise. Source code in src/pheval/utils/file_utils.py 46 47 48 49 50 51 52 53 54 55 56 def is_gzipped ( file_path : Path ) -> bool : \"\"\" Confirms whether a file is gzipped. Args: file_path (Path): The path to the file. Returns: bool: True if the file is gzipped, False otherwise. \"\"\" return file_path . name . endswith ( \".gz\" )","title":"is_gzipped"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.normalise_file_name","text":"Normalises the file name by removing diacritical marks (accents) from Unicode characters. Parameters: Name Type Description Default file_path Path The path to the file. required Returns: Name Type Description str str The normalised file name without diacritical marks. Source code in src/pheval/utils/file_utils.py 59 60 61 62 63 64 65 66 67 68 69 70 def normalise_file_name ( file_path : Path ) -> str : \"\"\" Normalises the file name by removing diacritical marks (accents) from Unicode characters. Args: file_path (Path): The path to the file. Returns: str: The normalised file name without diacritical marks. \"\"\" normalised_file_name = unicodedata . normalize ( \"NFD\" , str ( file_path )) return re . sub ( \"[ \\u0300 - \\u036f ]\" , \"\" , normalised_file_name )","title":"normalise_file_name"},{"location":"api/pheval/utils/file_utils/#src.pheval.utils.file_utils.write_metadata","text":"Write the metadata for a run to a YAML file. Parameters: Name Type Description Default output_dir Path The directory where the metadata file will be saved. required meta_data BasicOutputRunMetaData The metadata to be written. required Source code in src/pheval/utils/file_utils.py 108 109 110 111 112 113 114 115 116 117 118 def write_metadata ( output_dir : Path , meta_data : BasicOutputRunMetaData ) -> None : \"\"\" Write the metadata for a run to a YAML file. Args: output_dir (Path): The directory where the metadata file will be saved. meta_data (BasicOutputRunMetaData): The metadata to be written. \"\"\" with open ( Path ( output_dir ) . joinpath ( \"results.yml\" ), \"w\" ) as metadata_file : yaml . dump ( to_dict ( meta_data ), metadata_file , sort_keys = False , default_style = \"\" ) metadata_file . close ()","title":"write_metadata"},{"location":"api/pheval/utils/logger/","text":"print_ascii_banner () Prints ASCII banner only once when the script starts. Source code in src/pheval/utils/logger.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def print_ascii_banner (): \"\"\"Prints ASCII banner only once when the script starts.\"\"\" if not getattr ( logging , \"_ascii_printed\" , False ): logging . _ascii_printed = True pheval_banner = \"\"\" Welcome to: \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d A framework for the empirical evaluation of phenotype-driven prioritisation tools. \"\"\" print ( pheval_banner )","title":"Logger"},{"location":"api/pheval/utils/logger/#src.pheval.utils.logger.print_ascii_banner","text":"Prints ASCII banner only once when the script starts. Source code in src/pheval/utils/logger.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def print_ascii_banner (): \"\"\"Prints ASCII banner only once when the script starts.\"\"\" if not getattr ( logging , \"_ascii_printed\" , False ): logging . _ascii_printed = True pheval_banner = \"\"\" Welcome to: \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d A framework for the empirical evaluation of phenotype-driven prioritisation tools. \"\"\" print ( pheval_banner )","title":"print_ascii_banner"},{"location":"api/pheval/utils/phenopacket_utils/","text":"GeneIdentifierUpdater Class for updating gene identifiers within genomic interpretations. Source code in src/pheval/utils/phenopacket_utils.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 class GeneIdentifierUpdater : \"\"\"Class for updating gene identifiers within genomic interpretations.\"\"\" def __init__ ( self , gene_identifier : str , identifier_map : pl . DataFrame = None , ): \"\"\" Initialise the GeneIdentifierUpdater. Args: gene_identifier (str): The gene identifier to update to. identifier_map (dict): A polars dataframe mapping gene identifiers (default: None). \"\"\" self . gene_identifier = gene_identifier self . identifier_map = identifier_map def find_identifier ( self , gene_symbol : str ) -> str : \"\"\" Find the specified gene identifier for a gene symbol. Args: gene_symbol (str): The gene symbol to find the identifier for. Returns: str: The identified gene identifier. \"\"\" matches = self . identifier_map . filter ( ( pl . col ( \"gene_symbol\" ) == gene_symbol ) & ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) ) if matches . height > 0 : return matches [ \"identifier\" ][ 0 ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) & ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return prev_symbol_matches [ \"identifier\" ][ 0 ] logger . warn ( f \"Could not find { self . gene_identifier } for { gene_symbol } .\" ) return None def obtain_gene_symbol_from_identifier ( self , query_gene_identifier : str ) -> str : \"\"\" Obtain gene symbol from a gene identifier. Args: query_gene_identifier (str): The gene identifier. Returns: str: The gene symbol corresponding to the identifier. \"\"\" return self . identifier_map . filter ( pl . col ( \"identifier\" ) == query_gene_identifier )[ \"gene_symbol\" ][ 0 ] def _find_alternate_ids ( self , gene_symbol : str ) -> List [ str ]: \"\"\" Find the alternate IDs for a gene symbol. Args: gene_symbol (str): The gene symbol to find alternate IDs for. Returns: List[str]: List of alternate IDs for the gene symbol. \"\"\" matches = self . identifier_map . filter (( pl . col ( \"gene_symbol\" ) == gene_symbol )) if matches . height > 0 : return [ f \" { row [ 'prefix' ] }{ row [ 'identifier' ] } \" for row in matches . rows ( named = True )] + [ f \"symbol: { gene_symbol } \" ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return [ f \" { row [ 'prefix' ] }{ row [ 'identifier' ] } \" for row in prev_symbol_matches . rows ( named = True ) ] + [ f \"symbol: { gene_symbol } \" ] return None def update_genomic_interpretations_gene_identifier ( self , interpretations : List [ Interpretation ], phenopacket_path : Path ) -> List [ Interpretation ]: \"\"\" Update the genomic interpretations of a Phenopacket. Args: interpretations (List[Interpretation]): List of Interpretation objects. phenopacket_path (Path): The Path to the Phenopacket. Returns: List[Interpretation]: Updated list of Interpretation objects. \"\"\" updated_interpretations = copy ( list ( interpretations )) for updated_interpretation in updated_interpretations : for g in updated_interpretation . diagnosis . genomic_interpretations : updated_gene_identifier = self . find_identifier ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) logger . info ( f \"Updating gene identifier in { phenopacket_path . name } from \" f \" { g . variant_interpretation . variation_descriptor . gene_context . value_id } \" f \" to { updated_gene_identifier } \" ) g . variant_interpretation . variation_descriptor . gene_context . value_id = ( updated_gene_identifier ) del g . variant_interpretation . variation_descriptor . gene_context . alternate_ids [:] g . variant_interpretation . variation_descriptor . gene_context . alternate_ids . extend ( self . _find_alternate_ids ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) ) return updated_interpretations __init__ ( gene_identifier , identifier_map = None ) Initialise the GeneIdentifierUpdater. Parameters: Name Type Description Default gene_identifier str The gene identifier to update to. required identifier_map dict A polars dataframe mapping gene identifiers (default: None). None Source code in src/pheval/utils/phenopacket_utils.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def __init__ ( self , gene_identifier : str , identifier_map : pl . DataFrame = None , ): \"\"\" Initialise the GeneIdentifierUpdater. Args: gene_identifier (str): The gene identifier to update to. identifier_map (dict): A polars dataframe mapping gene identifiers (default: None). \"\"\" self . gene_identifier = gene_identifier self . identifier_map = identifier_map find_identifier ( gene_symbol ) Find the specified gene identifier for a gene symbol. Parameters: Name Type Description Default gene_symbol str The gene symbol to find the identifier for. required Returns: Name Type Description str str The identified gene identifier. Source code in src/pheval/utils/phenopacket_utils.py 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 def find_identifier ( self , gene_symbol : str ) -> str : \"\"\" Find the specified gene identifier for a gene symbol. Args: gene_symbol (str): The gene symbol to find the identifier for. Returns: str: The identified gene identifier. \"\"\" matches = self . identifier_map . filter ( ( pl . col ( \"gene_symbol\" ) == gene_symbol ) & ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) ) if matches . height > 0 : return matches [ \"identifier\" ][ 0 ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) & ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return prev_symbol_matches [ \"identifier\" ][ 0 ] logger . warn ( f \"Could not find { self . gene_identifier } for { gene_symbol } .\" ) return None obtain_gene_symbol_from_identifier ( query_gene_identifier ) Obtain gene symbol from a gene identifier. Parameters: Name Type Description Default query_gene_identifier str The gene identifier. required Returns: Name Type Description str str The gene symbol corresponding to the identifier. Source code in src/pheval/utils/phenopacket_utils.py 685 686 687 688 689 690 691 692 693 694 695 696 697 def obtain_gene_symbol_from_identifier ( self , query_gene_identifier : str ) -> str : \"\"\" Obtain gene symbol from a gene identifier. Args: query_gene_identifier (str): The gene identifier. Returns: str: The gene symbol corresponding to the identifier. \"\"\" return self . identifier_map . filter ( pl . col ( \"identifier\" ) == query_gene_identifier )[ \"gene_symbol\" ][ 0 ] update_genomic_interpretations_gene_identifier ( interpretations , phenopacket_path ) Update the genomic interpretations of a Phenopacket. Parameters: Name Type Description Default interpretations List [ Interpretation ] List of Interpretation objects. required phenopacket_path Path The Path to the Phenopacket. required Returns: Type Description List [ Interpretation ] List[Interpretation]: Updated list of Interpretation objects. Source code in src/pheval/utils/phenopacket_utils.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 def update_genomic_interpretations_gene_identifier ( self , interpretations : List [ Interpretation ], phenopacket_path : Path ) -> List [ Interpretation ]: \"\"\" Update the genomic interpretations of a Phenopacket. Args: interpretations (List[Interpretation]): List of Interpretation objects. phenopacket_path (Path): The Path to the Phenopacket. Returns: List[Interpretation]: Updated list of Interpretation objects. \"\"\" updated_interpretations = copy ( list ( interpretations )) for updated_interpretation in updated_interpretations : for g in updated_interpretation . diagnosis . genomic_interpretations : updated_gene_identifier = self . find_identifier ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) logger . info ( f \"Updating gene identifier in { phenopacket_path . name } from \" f \" { g . variant_interpretation . variation_descriptor . gene_context . value_id } \" f \" to { updated_gene_identifier } \" ) g . variant_interpretation . variation_descriptor . gene_context . value_id = ( updated_gene_identifier ) del g . variant_interpretation . variation_descriptor . gene_context . alternate_ids [:] g . variant_interpretation . variation_descriptor . gene_context . alternate_ids . extend ( self . _find_alternate_ids ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) ) return updated_interpretations GenomicVariant dataclass Represents a genomic variant. Parameters: Name Type Description Default chrom str The chromosome position of the variant recommended to be provided in the following format. required pos int Position of the variant following VCF convention. required ref str Reference allele following VCF convention. required alt str Alternate allele following VCF convention. required Source code in src/pheval/utils/phenopacket_utils.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @dataclass class GenomicVariant : \"\"\" Represents a genomic variant. Args: chrom (str): The chromosome position of the variant recommended to be provided in the following format. This includes numerical designations from 1 to 22 representing autosomal chromosomes, as well as the sex chromosomes X and Y, and the mitochondrial chromosome MT. pos (int): Position of the variant following VCF convention. ref (str): Reference allele following VCF convention. alt (str): Alternate allele following VCF convention. \"\"\" chrom : str pos : int ref : str alt : str IncompatibleGenomeAssemblyError Bases: Exception Exception raised for incompatible genome assembly. Source code in src/pheval/utils/phenopacket_utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class IncompatibleGenomeAssemblyError ( Exception ): \"\"\"Exception raised for incompatible genome assembly.\"\"\" def __init__ ( self , assembly , phenopacket , message = \"Incompatible Genome Assembly\" ): \"\"\" Initialise IncompatibleGenomeAssemblyError. Attributes: assembly (str): Incompatible genome assembly encountered. phenopacket (Path): Path to the Phenopacket associated with the error. message (str, optional): Custom error message (default is \"Incompatible Genome Assembly\"). \"\"\" self . assembly : str = assembly self . phenopacket : Path = phenopacket self . message : str = message super () . __init__ ( self . message ) def __str__ ( self ): return f \" { self . message } -> { self . assembly } in { self . phenopacket } \" __init__ ( assembly , phenopacket , message = 'Incompatible Genome Assembly' ) Initialise IncompatibleGenomeAssemblyError. Attributes: Name Type Description assembly str Incompatible genome assembly encountered. phenopacket Path Path to the Phenopacket associated with the error. message str Custom error message (default is \"Incompatible Genome Assembly\"). Source code in src/pheval/utils/phenopacket_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , assembly , phenopacket , message = \"Incompatible Genome Assembly\" ): \"\"\" Initialise IncompatibleGenomeAssemblyError. Attributes: assembly (str): Incompatible genome assembly encountered. phenopacket (Path): Path to the Phenopacket associated with the error. message (str, optional): Custom error message (default is \"Incompatible Genome Assembly\"). \"\"\" self . assembly : str = assembly self . phenopacket : Path = phenopacket self . message : str = message super () . __init__ ( self . message ) PhenopacketRebuilder Class for rebuilding a Phenopacket Source code in src/pheval/utils/phenopacket_utils.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 class PhenopacketRebuilder : \"\"\"Class for rebuilding a Phenopacket\"\"\" def __init__ ( self , phenopacket : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Attributes: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket = phenopacket def update_interpretations ( self , interpretations : [ Interpretation ] ) -> Union [ Phenopacket , Family ]: \"\"\" Add the updated interpretations to a Phenopacket or Family. Args: interpretations (List[Interpretation]): The updated interpretations to be added. Returns: Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . interpretations [:] phenopacket . proband . interpretations . extend ( interpretations ) else : del phenopacket . interpretations [:] phenopacket . interpretations . extend ( interpretations ) return phenopacket def add_randomised_hpo ( self , randomised_hpo : [ PhenotypicFeature ]) -> Union [ Phenopacket , Family ]: \"\"\" Add randomised phenotypic profiles to a Phenopacket or Family. Args: randomised_hpo: The randomised phenotypic profiles to be added. Returns: Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . phenotypic_features [:] phenopacket . proband . phenotypic_features . extend ( randomised_hpo ) else : del phenopacket . phenotypic_features [:] phenopacket . phenotypic_features . extend ( randomised_hpo ) return phenopacket def add_spiked_vcf_path ( self , spiked_vcf_file_data : File ) -> Union [ Phenopacket , Family ]: \"\"\" Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. \"\"\" logger . info ( f \"Adding spiked VCF path { spiked_vcf_file_data . uri } to phenopacket.\" ) phenopacket = copy ( self . phenopacket ) phenopacket_files = [ file for file in phenopacket . files if file . file_attributes [ \"fileFormat\" ] != \"vcf\" ] phenopacket_files . append ( spiked_vcf_file_data ) del phenopacket . files [:] phenopacket . files . extend ( phenopacket_files ) return phenopacket __init__ ( phenopacket ) Initialise PhenopacketUtil Attributes: Name Type Description phenopacket Union [ Phenopacket , Family ] Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 540 541 542 543 544 545 546 def __init__ ( self , phenopacket : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Attributes: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket = phenopacket add_randomised_hpo ( randomised_hpo ) Add randomised phenotypic profiles to a Phenopacket or Family. Parameters: Name Type Description Default randomised_hpo [ PhenotypicFeature ] The randomised phenotypic profiles to be added. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. Source code in src/pheval/utils/phenopacket_utils.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 def add_randomised_hpo ( self , randomised_hpo : [ PhenotypicFeature ]) -> Union [ Phenopacket , Family ]: \"\"\" Add randomised phenotypic profiles to a Phenopacket or Family. Args: randomised_hpo: The randomised phenotypic profiles to be added. Returns: Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . phenotypic_features [:] phenopacket . proband . phenotypic_features . extend ( randomised_hpo ) else : del phenopacket . phenotypic_features [:] phenopacket . phenotypic_features . extend ( randomised_hpo ) return phenopacket add_spiked_vcf_path ( spiked_vcf_file_data ) Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. Source code in src/pheval/utils/phenopacket_utils.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def add_spiked_vcf_path ( self , spiked_vcf_file_data : File ) -> Union [ Phenopacket , Family ]: \"\"\" Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. \"\"\" logger . info ( f \"Adding spiked VCF path { spiked_vcf_file_data . uri } to phenopacket.\" ) phenopacket = copy ( self . phenopacket ) phenopacket_files = [ file for file in phenopacket . files if file . file_attributes [ \"fileFormat\" ] != \"vcf\" ] phenopacket_files . append ( spiked_vcf_file_data ) del phenopacket . files [:] phenopacket . files . extend ( phenopacket_files ) return phenopacket update_interpretations ( interpretations ) Add the updated interpretations to a Phenopacket or Family. Parameters: Name Type Description Default interpretations List [ Interpretation ] The updated interpretations to be added. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. Source code in src/pheval/utils/phenopacket_utils.py 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def update_interpretations ( self , interpretations : [ Interpretation ] ) -> Union [ Phenopacket , Family ]: \"\"\" Add the updated interpretations to a Phenopacket or Family. Args: interpretations (List[Interpretation]): The updated interpretations to be added. Returns: Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . interpretations [:] phenopacket . proband . interpretations . extend ( interpretations ) else : del phenopacket . interpretations [:] phenopacket . interpretations . extend ( interpretations ) return phenopacket PhenopacketUtil Class for retrieving data from a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 class PhenopacketUtil : \"\"\"Class for retrieving data from a Phenopacket or Family object\"\"\" def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases def _diagnosis_from_interpretations ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the interpretations object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] interpretation = self . interpretations () for i in interpretation : ( diagnoses . append ( ProbandDisease ( disease_name = i . diagnosis . disease . label , disease_identifier = i . diagnosis . disease . id , ) ) if i . diagnosis . disease . label != \"\" and i . diagnosis . disease . id != \"\" else None ) return diagnoses def _diagnosis_from_disease ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the diseases object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] for disease in self . diseases (): diagnoses . append ( ProbandDisease ( disease_name = disease . term . label , disease_identifier = disease . term . id ) ) return diagnoses def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ())) def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data @staticmethod def _extract_diagnosed_gene ( genomic_interpretation : GenomicInterpretation , ) -> ProbandCausativeGene : \"\"\" Retrieve the disease causing genes from the variant descriptor field if not empty, otherwise, retrieves from the gene descriptor from a phenopacket. Args: genomic_interpretation (GenomicInterpretation): A genomic interpretation from a Phenopacket Returns: ProbandCausativeGene: The disease causing gene \"\"\" if genomic_interpretation . variant_interpretation . ByteSize () != 0 : return ProbandCausativeGene ( genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . symbol , genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . value_id , ) else : return ProbandCausativeGene ( gene_symbol = genomic_interpretation . gene . symbol , gene_identifier = genomic_interpretation . gene . value_id , ) def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False __init__ ( phenopacket_contents ) Initialise PhenopacketUtil Parameters: Name Type Description Default phenopacket_contents Union [ Phenopacket , Family ] Phenopacket or Family object required Source code in src/pheval/utils/phenopacket_utils.py 209 210 211 212 213 214 215 def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents causative_variants () Retrieve a list of causative variants listed in a Phenopacket Returns: Type Description List [ ProbandCausativeVariant ] List[ProbandCausativeVariant]: List of proband causative variants Source code in src/pheval/utils/phenopacket_utils.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants check_incomplete_disease_record () Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: Name Type Description bool bool True if any disease record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 522 523 524 525 526 527 528 529 530 531 532 533 534 def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False check_incomplete_gene_record () Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: Name Type Description bool bool True if any gene record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False check_incomplete_variant_record () Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: Name Type Description bool bool True if any variant record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False check_variant_alleles () Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: Name Type Description bool bool True if the reference and alternate alleles are identical, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 493 494 495 496 497 498 499 500 501 502 503 504 def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False diagnosed_genes () Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes Source code in src/pheval/utils/phenopacket_utils.py 433 434 435 436 437 438 439 440 441 442 443 444 445 def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes diagnosed_variants () Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants Source code in src/pheval/utils/phenopacket_utils.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants diagnoses () Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: Type Description List [ ProbandDisease ] List[ProbandDisease]: List of diagnosed diseases Source code in src/pheval/utils/phenopacket_utils.py 318 319 320 321 322 323 324 325 def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ())) diseases () Retrieve a list of Diseases associated with the proband Returns: Type Description List [ Disease ] List[Disease]: List of diseases Source code in src/pheval/utils/phenopacket_utils.py 270 271 272 273 274 275 276 277 278 279 280 def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases files () Retrieve a list of files associated with a phenopacket Returns: Type Description List [ File ] List[File]: List of files associated with a phenopacket Source code in src/pheval/utils/phenopacket_utils.py 367 368 369 370 371 372 373 374 def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files interpretations () Retrieve a list of interpretations from a Phenopacket Returns: Type Description List [ Interpretation ] List[Interpretation]: List of interpretations Source code in src/pheval/utils/phenopacket_utils.py 327 328 329 330 331 332 333 334 335 336 337 def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations negated_phenotypic_features () Retrieve a list of all negated HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of negated HPO terms Source code in src/pheval/utils/phenopacket_utils.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features observed_phenotypic_features () Retrieve a list of all observed HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of observed HPO terms Source code in src/pheval/utils/phenopacket_utils.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features phenotypic_features () Retrieve a list of all HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of HPO terms Source code in src/pheval/utils/phenopacket_utils.py 229 230 231 232 233 234 235 236 237 238 239 def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features sample_id () Retrieve the sample ID from a Phenopacket or proband of a Family Returns: Name Type Description str str Sample ID Source code in src/pheval/utils/phenopacket_utils.py 217 218 219 220 221 222 223 224 225 226 227 def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id vcf_file_data ( phenopacket_path , vcf_dir ) Retrieve the genome assembly and VCF file name from a phenopacket. Parameters: Name Type Description Default phenopacket_path Path The path to the phenopacket file. required vcf_dir Path The directory path where the VCF file is stored. required Returns: Name Type Description File File The VCF file with updated URI pointing to the specified directory. Raises: Type Description IncorrectFileFormatError If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError If the genome assembly of the VCF file is not compatible. Note This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. Source code in src/pheval/utils/phenopacket_utils.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data ProbandCausativeGene dataclass Represents a causative gene associated with a proband Parameters: Name Type Description Default gene_symbol str Symbol representing the gene required gene_identifier str The ENSEMBL gene identifier for the result entry required Notes: While we recommend providing the gene identifier in the ENSEMBL namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. Source code in src/pheval/utils/phenopacket_utils.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @dataclass class ProbandCausativeGene : \"\"\" Represents a causative gene associated with a proband Args: gene_symbol (str): Symbol representing the gene gene_identifier (str): The ENSEMBL gene identifier for the result entry Notes: While we recommend providing the gene identifier in the ENSEMBL namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. \"\"\" gene_symbol : str gene_identifier : str ProbandCausativeVariant dataclass Represents a causative variant associated with a proband Parameters: Name Type Description Default proband_id str ID of the proband required assembly str Genome assembly required variant GenomicVariant Genomic variant associated with the proband required genotype str Genotype information for the variant required info str Additional information about the variant (default is an empty string) '' Source code in src/pheval/utils/phenopacket_utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ProbandCausativeVariant : \"\"\" Represents a causative variant associated with a proband Args: proband_id (str): ID of the proband assembly (str): Genome assembly variant (GenomicVariant): Genomic variant associated with the proband genotype (str): Genotype information for the variant info (str, optional): Additional information about the variant (default is an empty string) \"\"\" proband_id : str assembly : str variant : GenomicVariant genotype : str info : str = \"\" ProbandDisease dataclass Represents a disease associated with a proband Parameters: Name Type Description Default disease_name str Name of the disease required disease_identifier str Identifier for the disease result entry in the OMIM namespace required Notes While we recommend providing the disease identifier in the OMIM namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. Source code in src/pheval/utils/phenopacket_utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 @dataclass ( frozen = True , eq = True ) class ProbandDisease : \"\"\" Represents a disease associated with a proband Args: disease_name (str): Name of the disease disease_identifier (str): Identifier for the disease result entry in the OMIM namespace Notes: While we recommend providing the disease identifier in the OMIM namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. \"\"\" disease_name : str disease_identifier : str create_gene_identifier_map () Create a mapping of gene identifiers to gene symbols using HGNC data. Returns: Type Description DataFrame pl.DataFrame: A mapping of gene identifiers to gene symbols. Source code in src/pheval/utils/phenopacket_utils.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def create_gene_identifier_map () -> pl . DataFrame : \"\"\" Create a mapping of gene identifiers to gene symbols using HGNC data. Returns: pl.DataFrame: A mapping of gene identifiers to gene symbols. \"\"\" logger . info ( \"Creating gene identifier map.\" ) hgnc_df = parse_hgnc_data () return hgnc_df . melt ( id_vars = [ \"gene_symbol\" , \"prev_symbols\" ], value_vars = [ \"ensembl_id\" , \"hgnc_id\" , \"entrez_id\" , \"refseq_accession\" ], variable_name = \"identifier_type\" , value_name = \"identifier\" , ) . with_columns ( pl . col ( \"identifier_type\" ) . replace ( { \"ensembl_id\" : \"ensembl:\" , \"hgnc_id\" : \"\" , \"entrez_id\" : \"ncbigene:\" , \"refseq_accession\" : \"\" , }, default = \"\" , ) . alias ( \"prefix\" ) ) create_json_message ( phenopacket ) Create a JSON message for writing to a file. Args: - phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family object to convert to JSON. Returns: - str: A JSON-formatted string representation of the Phenopacket or Family object. Source code in src/pheval/utils/phenopacket_utils.py 609 610 611 612 613 614 615 616 617 618 619 def create_json_message ( phenopacket : Union [ Phenopacket , Family ]) -> str : \"\"\" Create a JSON message for writing to a file. Args: - phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family object to convert to JSON. Returns: - str: A JSON-formatted string representation of the Phenopacket or Family object. \"\"\" return MessageToJson ( phenopacket ) parse_hgnc_data () Read HGNC data from a file and return it as a Polars DataFrame. Returns: Type Description DataFrame pl.DataFrame: DataFrame containing the HGNC data. Source code in src/pheval/utils/phenopacket_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def parse_hgnc_data () -> pl . DataFrame : \"\"\" Read HGNC data from a file and return it as a Polars DataFrame. Returns: pl.DataFrame: DataFrame containing the HGNC data. \"\"\" return ( pl . read_csv ( os . path . dirname ( __file__ ) . replace ( \"utils\" , \"resources/hgnc_complete_set.txt\" ), separator = \" \\t \" , infer_schema = 10000000000 , dtypes = { \"omim_id\" : pl . Utf8 }, ) . select ( [ pl . col ( \"hgnc_id\" ) . alias ( \"hgnc_id\" ), pl . col ( \"symbol\" ) . alias ( \"gene_symbol\" ), pl . col ( \"ensembl_gene_id\" ) . alias ( \"ensembl_id\" ), pl . col ( \"entrez_id\" ) . alias ( \"entrez_id\" ), pl . col ( \"refseq_accession\" ) . alias ( \"refseq_accession\" ), pl . col ( \"prev_symbol\" ) . alias ( \"previous_symbol_raw\" ), ] ) . with_columns ( pl . col ( \"previous_symbol_raw\" ) . str . split ( \"|\" ) . list . eval ( pl . element () . str . strip_chars ( '\"' )) . alias ( \"prev_symbols\" ) ) ) phenopacket_reader ( file ) Read a Phenopacket file and returns its contents as a Phenopacket or Family object Parameters: Name Type Description Default file Path Path to the Phenopacket file required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: Contents of the Phenopacket file as a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def phenopacket_reader ( file : Path ) -> Union [ Phenopacket , Family ]: \"\"\" Read a Phenopacket file and returns its contents as a Phenopacket or Family object Args: file (Path): Path to the Phenopacket file Returns: Union[Phenopacket, Family]: Contents of the Phenopacket file as a Phenopacket or Family object \"\"\" logger . info ( f \"Parsing Phenopacket: { file . name } \" ) file = open ( file , \"r\" ) phenopacket = json . load ( file ) file . close () if \"proband\" in phenopacket : return Parse ( json . dumps ( phenopacket ), Family ()) else : return Parse ( json . dumps ( phenopacket ), Phenopacket ()) write_phenopacket ( phenopacket , output_file ) Write a Phenopacket or Family object to a file in JSON format. Parameters: Name Type Description Default phenopacket Phenopacket or Family The Phenopacket or Family object to be written. required output_file Path The Path object representing the file to write the Phenopacket data. required Returns: Type Description None None Source code in src/pheval/utils/phenopacket_utils.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def write_phenopacket ( phenopacket : Union [ Phenopacket , Family ], output_file : Path ) -> None : \"\"\" Write a Phenopacket or Family object to a file in JSON format. Args: phenopacket (Phenopacket or Family): The Phenopacket or Family object to be written. output_file (Path): The Path object representing the file to write the Phenopacket data. Returns: None \"\"\" logger . info ( f \"Writing Phenopacket to { output_file } .\" ) phenopacket_json = create_json_message ( phenopacket ) with open ( output_file , \"w\" ) as outfile : outfile . write ( phenopacket_json ) outfile . close ()","title":"Phenopacket utils"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GeneIdentifierUpdater","text":"Class for updating gene identifiers within genomic interpretations. Source code in src/pheval/utils/phenopacket_utils.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 class GeneIdentifierUpdater : \"\"\"Class for updating gene identifiers within genomic interpretations.\"\"\" def __init__ ( self , gene_identifier : str , identifier_map : pl . DataFrame = None , ): \"\"\" Initialise the GeneIdentifierUpdater. Args: gene_identifier (str): The gene identifier to update to. identifier_map (dict): A polars dataframe mapping gene identifiers (default: None). \"\"\" self . gene_identifier = gene_identifier self . identifier_map = identifier_map def find_identifier ( self , gene_symbol : str ) -> str : \"\"\" Find the specified gene identifier for a gene symbol. Args: gene_symbol (str): The gene symbol to find the identifier for. Returns: str: The identified gene identifier. \"\"\" matches = self . identifier_map . filter ( ( pl . col ( \"gene_symbol\" ) == gene_symbol ) & ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) ) if matches . height > 0 : return matches [ \"identifier\" ][ 0 ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) & ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return prev_symbol_matches [ \"identifier\" ][ 0 ] logger . warn ( f \"Could not find { self . gene_identifier } for { gene_symbol } .\" ) return None def obtain_gene_symbol_from_identifier ( self , query_gene_identifier : str ) -> str : \"\"\" Obtain gene symbol from a gene identifier. Args: query_gene_identifier (str): The gene identifier. Returns: str: The gene symbol corresponding to the identifier. \"\"\" return self . identifier_map . filter ( pl . col ( \"identifier\" ) == query_gene_identifier )[ \"gene_symbol\" ][ 0 ] def _find_alternate_ids ( self , gene_symbol : str ) -> List [ str ]: \"\"\" Find the alternate IDs for a gene symbol. Args: gene_symbol (str): The gene symbol to find alternate IDs for. Returns: List[str]: List of alternate IDs for the gene symbol. \"\"\" matches = self . identifier_map . filter (( pl . col ( \"gene_symbol\" ) == gene_symbol )) if matches . height > 0 : return [ f \" { row [ 'prefix' ] }{ row [ 'identifier' ] } \" for row in matches . rows ( named = True )] + [ f \"symbol: { gene_symbol } \" ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return [ f \" { row [ 'prefix' ] }{ row [ 'identifier' ] } \" for row in prev_symbol_matches . rows ( named = True ) ] + [ f \"symbol: { gene_symbol } \" ] return None def update_genomic_interpretations_gene_identifier ( self , interpretations : List [ Interpretation ], phenopacket_path : Path ) -> List [ Interpretation ]: \"\"\" Update the genomic interpretations of a Phenopacket. Args: interpretations (List[Interpretation]): List of Interpretation objects. phenopacket_path (Path): The Path to the Phenopacket. Returns: List[Interpretation]: Updated list of Interpretation objects. \"\"\" updated_interpretations = copy ( list ( interpretations )) for updated_interpretation in updated_interpretations : for g in updated_interpretation . diagnosis . genomic_interpretations : updated_gene_identifier = self . find_identifier ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) logger . info ( f \"Updating gene identifier in { phenopacket_path . name } from \" f \" { g . variant_interpretation . variation_descriptor . gene_context . value_id } \" f \" to { updated_gene_identifier } \" ) g . variant_interpretation . variation_descriptor . gene_context . value_id = ( updated_gene_identifier ) del g . variant_interpretation . variation_descriptor . gene_context . alternate_ids [:] g . variant_interpretation . variation_descriptor . gene_context . alternate_ids . extend ( self . _find_alternate_ids ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) ) return updated_interpretations","title":"GeneIdentifierUpdater"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GeneIdentifierUpdater.__init__","text":"Initialise the GeneIdentifierUpdater. Parameters: Name Type Description Default gene_identifier str The gene identifier to update to. required identifier_map dict A polars dataframe mapping gene identifiers (default: None). None Source code in src/pheval/utils/phenopacket_utils.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def __init__ ( self , gene_identifier : str , identifier_map : pl . DataFrame = None , ): \"\"\" Initialise the GeneIdentifierUpdater. Args: gene_identifier (str): The gene identifier to update to. identifier_map (dict): A polars dataframe mapping gene identifiers (default: None). \"\"\" self . gene_identifier = gene_identifier self . identifier_map = identifier_map","title":"__init__"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GeneIdentifierUpdater.find_identifier","text":"Find the specified gene identifier for a gene symbol. Parameters: Name Type Description Default gene_symbol str The gene symbol to find the identifier for. required Returns: Name Type Description str str The identified gene identifier. Source code in src/pheval/utils/phenopacket_utils.py 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 def find_identifier ( self , gene_symbol : str ) -> str : \"\"\" Find the specified gene identifier for a gene symbol. Args: gene_symbol (str): The gene symbol to find the identifier for. Returns: str: The identified gene identifier. \"\"\" matches = self . identifier_map . filter ( ( pl . col ( \"gene_symbol\" ) == gene_symbol ) & ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) ) if matches . height > 0 : return matches [ \"identifier\" ][ 0 ] prev_symbol_matches = self . identifier_map . filter ( ( pl . col ( \"identifier_type\" ) == self . gene_identifier ) & ( pl . col ( \"prev_symbols\" ) . list . contains ( gene_symbol )) ) if prev_symbol_matches . height > 0 : return prev_symbol_matches [ \"identifier\" ][ 0 ] logger . warn ( f \"Could not find { self . gene_identifier } for { gene_symbol } .\" ) return None","title":"find_identifier"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GeneIdentifierUpdater.obtain_gene_symbol_from_identifier","text":"Obtain gene symbol from a gene identifier. Parameters: Name Type Description Default query_gene_identifier str The gene identifier. required Returns: Name Type Description str str The gene symbol corresponding to the identifier. Source code in src/pheval/utils/phenopacket_utils.py 685 686 687 688 689 690 691 692 693 694 695 696 697 def obtain_gene_symbol_from_identifier ( self , query_gene_identifier : str ) -> str : \"\"\" Obtain gene symbol from a gene identifier. Args: query_gene_identifier (str): The gene identifier. Returns: str: The gene symbol corresponding to the identifier. \"\"\" return self . identifier_map . filter ( pl . col ( \"identifier\" ) == query_gene_identifier )[ \"gene_symbol\" ][ 0 ]","title":"obtain_gene_symbol_from_identifier"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GeneIdentifierUpdater.update_genomic_interpretations_gene_identifier","text":"Update the genomic interpretations of a Phenopacket. Parameters: Name Type Description Default interpretations List [ Interpretation ] List of Interpretation objects. required phenopacket_path Path The Path to the Phenopacket. required Returns: Type Description List [ Interpretation ] List[Interpretation]: Updated list of Interpretation objects. Source code in src/pheval/utils/phenopacket_utils.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 def update_genomic_interpretations_gene_identifier ( self , interpretations : List [ Interpretation ], phenopacket_path : Path ) -> List [ Interpretation ]: \"\"\" Update the genomic interpretations of a Phenopacket. Args: interpretations (List[Interpretation]): List of Interpretation objects. phenopacket_path (Path): The Path to the Phenopacket. Returns: List[Interpretation]: Updated list of Interpretation objects. \"\"\" updated_interpretations = copy ( list ( interpretations )) for updated_interpretation in updated_interpretations : for g in updated_interpretation . diagnosis . genomic_interpretations : updated_gene_identifier = self . find_identifier ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) logger . info ( f \"Updating gene identifier in { phenopacket_path . name } from \" f \" { g . variant_interpretation . variation_descriptor . gene_context . value_id } \" f \" to { updated_gene_identifier } \" ) g . variant_interpretation . variation_descriptor . gene_context . value_id = ( updated_gene_identifier ) del g . variant_interpretation . variation_descriptor . gene_context . alternate_ids [:] g . variant_interpretation . variation_descriptor . gene_context . alternate_ids . extend ( self . _find_alternate_ids ( g . variant_interpretation . variation_descriptor . gene_context . symbol ) ) return updated_interpretations","title":"update_genomic_interpretations_gene_identifier"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.GenomicVariant","text":"Represents a genomic variant. Parameters: Name Type Description Default chrom str The chromosome position of the variant recommended to be provided in the following format. required pos int Position of the variant following VCF convention. required ref str Reference allele following VCF convention. required alt str Alternate allele following VCF convention. required Source code in src/pheval/utils/phenopacket_utils.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @dataclass class GenomicVariant : \"\"\" Represents a genomic variant. Args: chrom (str): The chromosome position of the variant recommended to be provided in the following format. This includes numerical designations from 1 to 22 representing autosomal chromosomes, as well as the sex chromosomes X and Y, and the mitochondrial chromosome MT. pos (int): Position of the variant following VCF convention. ref (str): Reference allele following VCF convention. alt (str): Alternate allele following VCF convention. \"\"\" chrom : str pos : int ref : str alt : str","title":"GenomicVariant"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.IncompatibleGenomeAssemblyError","text":"Bases: Exception Exception raised for incompatible genome assembly. Source code in src/pheval/utils/phenopacket_utils.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class IncompatibleGenomeAssemblyError ( Exception ): \"\"\"Exception raised for incompatible genome assembly.\"\"\" def __init__ ( self , assembly , phenopacket , message = \"Incompatible Genome Assembly\" ): \"\"\" Initialise IncompatibleGenomeAssemblyError. Attributes: assembly (str): Incompatible genome assembly encountered. phenopacket (Path): Path to the Phenopacket associated with the error. message (str, optional): Custom error message (default is \"Incompatible Genome Assembly\"). \"\"\" self . assembly : str = assembly self . phenopacket : Path = phenopacket self . message : str = message super () . __init__ ( self . message ) def __str__ ( self ): return f \" { self . message } -> { self . assembly } in { self . phenopacket } \"","title":"IncompatibleGenomeAssemblyError"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.IncompatibleGenomeAssemblyError.__init__","text":"Initialise IncompatibleGenomeAssemblyError. Attributes: Name Type Description assembly str Incompatible genome assembly encountered. phenopacket Path Path to the Phenopacket associated with the error. message str Custom error message (default is \"Incompatible Genome Assembly\"). Source code in src/pheval/utils/phenopacket_utils.py 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , assembly , phenopacket , message = \"Incompatible Genome Assembly\" ): \"\"\" Initialise IncompatibleGenomeAssemblyError. Attributes: assembly (str): Incompatible genome assembly encountered. phenopacket (Path): Path to the Phenopacket associated with the error. message (str, optional): Custom error message (default is \"Incompatible Genome Assembly\"). \"\"\" self . assembly : str = assembly self . phenopacket : Path = phenopacket self . message : str = message super () . __init__ ( self . message )","title":"__init__"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketRebuilder","text":"Class for rebuilding a Phenopacket Source code in src/pheval/utils/phenopacket_utils.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 class PhenopacketRebuilder : \"\"\"Class for rebuilding a Phenopacket\"\"\" def __init__ ( self , phenopacket : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Attributes: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket = phenopacket def update_interpretations ( self , interpretations : [ Interpretation ] ) -> Union [ Phenopacket , Family ]: \"\"\" Add the updated interpretations to a Phenopacket or Family. Args: interpretations (List[Interpretation]): The updated interpretations to be added. Returns: Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . interpretations [:] phenopacket . proband . interpretations . extend ( interpretations ) else : del phenopacket . interpretations [:] phenopacket . interpretations . extend ( interpretations ) return phenopacket def add_randomised_hpo ( self , randomised_hpo : [ PhenotypicFeature ]) -> Union [ Phenopacket , Family ]: \"\"\" Add randomised phenotypic profiles to a Phenopacket or Family. Args: randomised_hpo: The randomised phenotypic profiles to be added. Returns: Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . phenotypic_features [:] phenopacket . proband . phenotypic_features . extend ( randomised_hpo ) else : del phenopacket . phenotypic_features [:] phenopacket . phenotypic_features . extend ( randomised_hpo ) return phenopacket def add_spiked_vcf_path ( self , spiked_vcf_file_data : File ) -> Union [ Phenopacket , Family ]: \"\"\" Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. \"\"\" logger . info ( f \"Adding spiked VCF path { spiked_vcf_file_data . uri } to phenopacket.\" ) phenopacket = copy ( self . phenopacket ) phenopacket_files = [ file for file in phenopacket . files if file . file_attributes [ \"fileFormat\" ] != \"vcf\" ] phenopacket_files . append ( spiked_vcf_file_data ) del phenopacket . files [:] phenopacket . files . extend ( phenopacket_files ) return phenopacket","title":"PhenopacketRebuilder"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketRebuilder.__init__","text":"Initialise PhenopacketUtil Attributes: Name Type Description phenopacket Union [ Phenopacket , Family ] Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 540 541 542 543 544 545 546 def __init__ ( self , phenopacket : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Attributes: phenopacket (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket = phenopacket","title":"__init__"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketRebuilder.add_randomised_hpo","text":"Add randomised phenotypic profiles to a Phenopacket or Family. Parameters: Name Type Description Default randomised_hpo [ PhenotypicFeature ] The randomised phenotypic profiles to be added. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. Source code in src/pheval/utils/phenopacket_utils.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 def add_randomised_hpo ( self , randomised_hpo : [ PhenotypicFeature ]) -> Union [ Phenopacket , Family ]: \"\"\" Add randomised phenotypic profiles to a Phenopacket or Family. Args: randomised_hpo: The randomised phenotypic profiles to be added. Returns: Union[Phenopacket, Family] The Phenopacket or Family object with added randomised profiles. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . phenotypic_features [:] phenopacket . proband . phenotypic_features . extend ( randomised_hpo ) else : del phenopacket . phenotypic_features [:] phenopacket . phenotypic_features . extend ( randomised_hpo ) return phenopacket","title":"add_randomised_hpo"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketRebuilder.add_spiked_vcf_path","text":"Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. Source code in src/pheval/utils/phenopacket_utils.py 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def add_spiked_vcf_path ( self , spiked_vcf_file_data : File ) -> Union [ Phenopacket , Family ]: \"\"\" Add a spiked VCF path to a Phenopacket or Family. Args: - spiked_vcf_file_data (File): The VCF file data to be added. Returns: - Phenopacket or Family: The Phenopacket or Family object with the added spiked VCF path. \"\"\" logger . info ( f \"Adding spiked VCF path { spiked_vcf_file_data . uri } to phenopacket.\" ) phenopacket = copy ( self . phenopacket ) phenopacket_files = [ file for file in phenopacket . files if file . file_attributes [ \"fileFormat\" ] != \"vcf\" ] phenopacket_files . append ( spiked_vcf_file_data ) del phenopacket . files [:] phenopacket . files . extend ( phenopacket_files ) return phenopacket","title":"add_spiked_vcf_path"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketRebuilder.update_interpretations","text":"Add the updated interpretations to a Phenopacket or Family. Parameters: Name Type Description Default interpretations List [ Interpretation ] The updated interpretations to be added. required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. Source code in src/pheval/utils/phenopacket_utils.py 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def update_interpretations ( self , interpretations : [ Interpretation ] ) -> Union [ Phenopacket , Family ]: \"\"\" Add the updated interpretations to a Phenopacket or Family. Args: interpretations (List[Interpretation]): The updated interpretations to be added. Returns: Union[Phenopacket, Family]: The Phenopacket or Family object with updated interpretations. \"\"\" phenopacket = copy ( self . phenopacket ) if hasattr ( phenopacket , \"proband\" ): del phenopacket . proband . interpretations [:] phenopacket . proband . interpretations . extend ( interpretations ) else : del phenopacket . interpretations [:] phenopacket . interpretations . extend ( interpretations ) return phenopacket","title":"update_interpretations"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil","text":"Class for retrieving data from a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 class PhenopacketUtil : \"\"\"Class for retrieving data from a Phenopacket or Family object\"\"\" def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases def _diagnosis_from_interpretations ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the interpretations object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] interpretation = self . interpretations () for i in interpretation : ( diagnoses . append ( ProbandDisease ( disease_name = i . diagnosis . disease . label , disease_identifier = i . diagnosis . disease . id , ) ) if i . diagnosis . disease . label != \"\" and i . diagnosis . disease . id != \"\" else None ) return diagnoses def _diagnosis_from_disease ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a list of disease diagnoses associated with the proband from the diseases object Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" diagnoses = [] for disease in self . diseases (): diagnoses . append ( ProbandDisease ( disease_name = disease . term . label , disease_identifier = disease . term . id ) ) return diagnoses def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ())) def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data @staticmethod def _extract_diagnosed_gene ( genomic_interpretation : GenomicInterpretation , ) -> ProbandCausativeGene : \"\"\" Retrieve the disease causing genes from the variant descriptor field if not empty, otherwise, retrieves from the gene descriptor from a phenopacket. Args: genomic_interpretation (GenomicInterpretation): A genomic interpretation from a Phenopacket Returns: ProbandCausativeGene: The disease causing gene \"\"\" if genomic_interpretation . variant_interpretation . ByteSize () != 0 : return ProbandCausativeGene ( genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . symbol , genomic_interpretation . variant_interpretation . variation_descriptor . gene_context . value_id , ) else : return ProbandCausativeGene ( gene_symbol = genomic_interpretation . gene . symbol , gene_identifier = genomic_interpretation . gene . value_id , ) def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False","title":"PhenopacketUtil"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.__init__","text":"Initialise PhenopacketUtil Parameters: Name Type Description Default phenopacket_contents Union [ Phenopacket , Family ] Phenopacket or Family object required Source code in src/pheval/utils/phenopacket_utils.py 209 210 211 212 213 214 215 def __init__ ( self , phenopacket_contents : Union [ Phenopacket , Family ]): \"\"\"Initialise PhenopacketUtil Args: phenopacket_contents (Union[Phenopacket, Family]): Phenopacket or Family object \"\"\" self . phenopacket_contents = phenopacket_contents","title":"__init__"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.causative_variants","text":"Retrieve a list of causative variants listed in a Phenopacket Returns: Type Description List [ ProbandCausativeVariant ] List[ProbandCausativeVariant]: List of proband causative variants Source code in src/pheval/utils/phenopacket_utils.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def causative_variants ( self ) -> List [ ProbandCausativeVariant ]: \"\"\" Retrieve a list of causative variants listed in a Phenopacket Returns: List[ProbandCausativeVariant]: List of proband causative variants \"\"\" all_variants = [] interpretation = self . interpretations () for i in interpretation : for g in i . diagnosis . genomic_interpretations : vcf_record = g . variant_interpretation . variation_descriptor . vcf_record genotype = g . variant_interpretation . variation_descriptor . allelic_state variant_data = ProbandCausativeVariant ( self . phenopacket_contents . subject . id , vcf_record . genome_assembly , GenomicVariant ( vcf_record . chrom , vcf_record . pos , vcf_record . ref , vcf_record . alt , ), genotype . label , vcf_record . info , ) all_variants . append ( variant_data ) return all_variants","title":"causative_variants"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.check_incomplete_disease_record","text":"Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: Name Type Description bool bool True if any disease record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 522 523 524 525 526 527 528 529 530 531 532 533 534 def check_incomplete_disease_record ( self ) -> bool : \"\"\" Check if any disease record in the phenopacket has incomplete information. This method iterates through the diagnosed disease records and checks if any of them have missing or incomplete information such as empty disease name, or disease identifier. Returns: bool: True if any disease record is incomplete, False otherwise. \"\"\" if len ( self . diagnoses ()) == 0 : return True return False","title":"check_incomplete_disease_record"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.check_incomplete_gene_record","text":"Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: Name Type Description bool bool True if any gene record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def check_incomplete_gene_record ( self ) -> bool : \"\"\" Check if any gene record in the phenopacket has incomplete information. This method iterates through the diagnosed gene records and checks if any of them have missing or incomplete information such as gene name, or gene identifier. Returns: bool: True if any gene record is incomplete, False otherwise. \"\"\" genes = self . diagnosed_genes () for gene in genes : if gene . gene_symbol == \"\" or gene . gene_identifier == \"\" : return True return False","title":"check_incomplete_gene_record"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.check_incomplete_variant_record","text":"Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: Name Type Description bool bool True if any variant record is incomplete, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def check_incomplete_variant_record ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has incomplete information. This method iterates through the diagnosed variant records and checks if any of them have missing or incomplete information such as empty chromosome, position, reference, or alternate allele. Returns: bool: True if any variant record is incomplete, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if ( variant . chrom == \"\" or variant . pos == 0 or variant . pos == \"\" or variant . ref == \"\" or variant . alt == \"\" ): return True return False","title":"check_incomplete_variant_record"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.check_variant_alleles","text":"Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: Name Type Description bool bool True if the reference and alternate alleles are identical, False otherwise. Source code in src/pheval/utils/phenopacket_utils.py 493 494 495 496 497 498 499 500 501 502 503 504 def check_variant_alleles ( self ) -> bool : \"\"\" Check if any variant record in the phenopacket has identical reference and alternate alleles. Returns: bool: True if the reference and alternate alleles are identical, False otherwise. \"\"\" variants = self . diagnosed_variants () for variant in variants : if variant . ref == variant . alt : return True return False","title":"check_variant_alleles"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.diagnosed_genes","text":"Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes Source code in src/pheval/utils/phenopacket_utils.py 433 434 435 436 437 438 439 440 441 442 443 444 445 def diagnosed_genes ( self ) -> List [ ProbandCausativeGene ]: \"\"\" Retrieve the disease causing genes from a phenopacket. Returns: List[ProbandCausativeGene]: List of causative genes \"\"\" pheno_interpretation = self . interpretations () genes = [] for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : genes . append ( self . _extract_diagnosed_gene ( g )) genes = list ({ gene . gene_symbol : gene for gene in genes } . values ()) return genes","title":"diagnosed_genes"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.diagnosed_variants","text":"Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants Source code in src/pheval/utils/phenopacket_utils.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def diagnosed_variants ( self ) -> List [ GenomicVariant ]: \"\"\" Retrieve a list of all known causative variants from a phenopacket. Returns: List[GenomicVariant]: List of causative variants \"\"\" variants = [] pheno_interpretation = self . interpretations () for i in pheno_interpretation : for g in i . diagnosis . genomic_interpretations : variant = GenomicVariant ( chrom = str ( g . variant_interpretation . variation_descriptor . vcf_record . chrom . replace ( \"chr\" , \"\" ) ), pos = int ( g . variant_interpretation . variation_descriptor . vcf_record . pos ), ref = g . variant_interpretation . variation_descriptor . vcf_record . ref , alt = g . variant_interpretation . variation_descriptor . vcf_record . alt , ) variants . append ( variant ) return variants","title":"diagnosed_variants"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.diagnoses","text":"Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: Type Description List [ ProbandDisease ] List[ProbandDisease]: List of diagnosed diseases Source code in src/pheval/utils/phenopacket_utils.py 318 319 320 321 322 323 324 325 def diagnoses ( self ) -> List [ ProbandDisease ]: \"\"\" Retrieve a unique list of disease diagnoses associated with the proband from a Phenopacket Returns: List[ProbandDisease]: List of diagnosed diseases \"\"\" return list ( set ( self . _diagnosis_from_interpretations () + self . _diagnosis_from_disease ()))","title":"diagnoses"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.diseases","text":"Retrieve a list of Diseases associated with the proband Returns: Type Description List [ Disease ] List[Disease]: List of diseases Source code in src/pheval/utils/phenopacket_utils.py 270 271 272 273 274 275 276 277 278 279 280 def diseases ( self ) -> List [ Disease ]: \"\"\" Retrieve a list of Diseases associated with the proband Returns: List[Disease]: List of diseases \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . diseases else : return self . phenopacket_contents . diseases","title":"diseases"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.files","text":"Retrieve a list of files associated with a phenopacket Returns: Type Description List [ File ] List[File]: List of files associated with a phenopacket Source code in src/pheval/utils/phenopacket_utils.py 367 368 369 370 371 372 373 374 def files ( self ) -> List [ File ]: \"\"\" Retrieve a list of files associated with a phenopacket Returns: List[File]: List of files associated with a phenopacket \"\"\" return self . phenopacket_contents . files","title":"files"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.interpretations","text":"Retrieve a list of interpretations from a Phenopacket Returns: Type Description List [ Interpretation ] List[Interpretation]: List of interpretations Source code in src/pheval/utils/phenopacket_utils.py 327 328 329 330 331 332 333 334 335 336 337 def interpretations ( self ) -> List [ Interpretation ]: \"\"\" Retrieve a list of interpretations from a Phenopacket Returns: List[Interpretation]: List of interpretations \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . interpretations else : return self . phenopacket_contents . interpretations","title":"interpretations"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.negated_phenotypic_features","text":"Retrieve a list of all negated HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of negated HPO terms Source code in src/pheval/utils/phenopacket_utils.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def negated_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all negated HPO terms Returns: List[PhenotypicFeature]: List of negated HPO terms \"\"\" negated_phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : negated_phenotypic_features . append ( p ) return negated_phenotypic_features","title":"negated_phenotypic_features"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.observed_phenotypic_features","text":"Retrieve a list of all observed HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of observed HPO terms Source code in src/pheval/utils/phenopacket_utils.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def observed_phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all observed HPO terms Returns: List[PhenotypicFeature]: List of observed HPO terms \"\"\" phenotypic_features = [] all_phenotypic_features = self . phenotypic_features () for p in all_phenotypic_features : if p . excluded : continue phenotypic_features . append ( p ) return phenotypic_features","title":"observed_phenotypic_features"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.phenotypic_features","text":"Retrieve a list of all HPO terms Returns: Type Description List [ PhenotypicFeature ] List[PhenotypicFeature]: List of HPO terms Source code in src/pheval/utils/phenopacket_utils.py 229 230 231 232 233 234 235 236 237 238 239 def phenotypic_features ( self ) -> List [ PhenotypicFeature ]: \"\"\" Retrieve a list of all HPO terms Returns: List[PhenotypicFeature]: List of HPO terms \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . phenotypic_features else : return self . phenopacket_contents . phenotypic_features","title":"phenotypic_features"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.sample_id","text":"Retrieve the sample ID from a Phenopacket or proband of a Family Returns: Name Type Description str str Sample ID Source code in src/pheval/utils/phenopacket_utils.py 217 218 219 220 221 222 223 224 225 226 227 def sample_id ( self ) -> str : \"\"\" Retrieve the sample ID from a Phenopacket or proband of a Family Returns: str: Sample ID \"\"\" if hasattr ( self . phenopacket_contents , \"proband\" ): return self . phenopacket_contents . proband . subject . id else : return self . phenopacket_contents . subject . id","title":"sample_id"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.PhenopacketUtil.vcf_file_data","text":"Retrieve the genome assembly and VCF file name from a phenopacket. Parameters: Name Type Description Default phenopacket_path Path The path to the phenopacket file. required vcf_dir Path The directory path where the VCF file is stored. required Returns: Name Type Description File File The VCF file with updated URI pointing to the specified directory. Raises: Type Description IncorrectFileFormatError If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError If the genome assembly of the VCF file is not compatible. Note This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. Source code in src/pheval/utils/phenopacket_utils.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def vcf_file_data ( self , phenopacket_path : Path , vcf_dir : Path ) -> File : \"\"\" Retrieve the genome assembly and VCF file name from a phenopacket. Args: phenopacket_path (Path): The path to the phenopacket file. vcf_dir (Path): The directory path where the VCF file is stored. Returns: File: The VCF file with updated URI pointing to the specified directory. Raises: IncorrectFileFormatError: If the provided file is not in .vcf or .vcf.gz format. IncompatibleGenomeAssemblyError: If the genome assembly of the VCF file is not compatible. Note: This function searches for a VCF file within the provided list of files, validates its format, and checks if the genome assembly is compatible. If the conditions are met, it updates the URI of the VCF file to the specified directory and returns the modified file object. \"\"\" compatible_genome_assembly = [ \"GRCh37\" , \"hg19\" , \"GRCh38\" , \"hg38\" ] vcf_data = [ file for file in self . files () if file . file_attributes [ \"fileFormat\" ] == \"vcf\" ][ 0 ] if not Path ( vcf_data . uri ) . name . endswith ( \".vcf\" ) and not Path ( vcf_data . uri ) . name . endswith ( \".vcf.gz\" ): raise IncorrectFileFormatError ( Path ( vcf_data . uri ), \".vcf or .vcf.gz file\" ) if vcf_data . file_attributes [ \"genomeAssembly\" ] not in compatible_genome_assembly : raise IncompatibleGenomeAssemblyError ( vcf_data . file_attributes [ \"genomeAssembly\" ], phenopacket_path ) vcf_data . uri = str ( vcf_dir . joinpath ( Path ( vcf_data . uri ) . name )) return vcf_data","title":"vcf_file_data"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.ProbandCausativeGene","text":"Represents a causative gene associated with a proband Parameters: Name Type Description Default gene_symbol str Symbol representing the gene required gene_identifier str The ENSEMBL gene identifier for the result entry required Notes: While we recommend providing the gene identifier in the ENSEMBL namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. Source code in src/pheval/utils/phenopacket_utils.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @dataclass class ProbandCausativeGene : \"\"\" Represents a causative gene associated with a proband Args: gene_symbol (str): Symbol representing the gene gene_identifier (str): The ENSEMBL gene identifier for the result entry Notes: While we recommend providing the gene identifier in the ENSEMBL namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. \"\"\" gene_symbol : str gene_identifier : str","title":"ProbandCausativeGene"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.ProbandCausativeVariant","text":"Represents a causative variant associated with a proband Parameters: Name Type Description Default proband_id str ID of the proband required assembly str Genome assembly required variant GenomicVariant Genomic variant associated with the proband required genotype str Genotype information for the variant required info str Additional information about the variant (default is an empty string) '' Source code in src/pheval/utils/phenopacket_utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ProbandCausativeVariant : \"\"\" Represents a causative variant associated with a proband Args: proband_id (str): ID of the proband assembly (str): Genome assembly variant (GenomicVariant): Genomic variant associated with the proband genotype (str): Genotype information for the variant info (str, optional): Additional information about the variant (default is an empty string) \"\"\" proband_id : str assembly : str variant : GenomicVariant genotype : str info : str = \"\"","title":"ProbandCausativeVariant"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.ProbandDisease","text":"Represents a disease associated with a proband Parameters: Name Type Description Default disease_name str Name of the disease required disease_identifier str Identifier for the disease result entry in the OMIM namespace required Notes While we recommend providing the disease identifier in the OMIM namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. Source code in src/pheval/utils/phenopacket_utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 @dataclass ( frozen = True , eq = True ) class ProbandDisease : \"\"\" Represents a disease associated with a proband Args: disease_name (str): Name of the disease disease_identifier (str): Identifier for the disease result entry in the OMIM namespace Notes: While we recommend providing the disease identifier in the OMIM namespace, any matching format used in Phenopacket interpretations and result output is acceptable for result matching purposes in the analysis. \"\"\" disease_name : str disease_identifier : str","title":"ProbandDisease"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.create_gene_identifier_map","text":"Create a mapping of gene identifiers to gene symbols using HGNC data. Returns: Type Description DataFrame pl.DataFrame: A mapping of gene identifiers to gene symbols. Source code in src/pheval/utils/phenopacket_utils.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def create_gene_identifier_map () -> pl . DataFrame : \"\"\" Create a mapping of gene identifiers to gene symbols using HGNC data. Returns: pl.DataFrame: A mapping of gene identifiers to gene symbols. \"\"\" logger . info ( \"Creating gene identifier map.\" ) hgnc_df = parse_hgnc_data () return hgnc_df . melt ( id_vars = [ \"gene_symbol\" , \"prev_symbols\" ], value_vars = [ \"ensembl_id\" , \"hgnc_id\" , \"entrez_id\" , \"refseq_accession\" ], variable_name = \"identifier_type\" , value_name = \"identifier\" , ) . with_columns ( pl . col ( \"identifier_type\" ) . replace ( { \"ensembl_id\" : \"ensembl:\" , \"hgnc_id\" : \"\" , \"entrez_id\" : \"ncbigene:\" , \"refseq_accession\" : \"\" , }, default = \"\" , ) . alias ( \"prefix\" ) )","title":"create_gene_identifier_map"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.create_json_message","text":"Create a JSON message for writing to a file. Args: - phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family object to convert to JSON. Returns: - str: A JSON-formatted string representation of the Phenopacket or Family object. Source code in src/pheval/utils/phenopacket_utils.py 609 610 611 612 613 614 615 616 617 618 619 def create_json_message ( phenopacket : Union [ Phenopacket , Family ]) -> str : \"\"\" Create a JSON message for writing to a file. Args: - phenopacket (Union[Phenopacket, Family]): The Phenopacket or Family object to convert to JSON. Returns: - str: A JSON-formatted string representation of the Phenopacket or Family object. \"\"\" return MessageToJson ( phenopacket )","title":"create_json_message"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.parse_hgnc_data","text":"Read HGNC data from a file and return it as a Polars DataFrame. Returns: Type Description DataFrame pl.DataFrame: DataFrame containing the HGNC data. Source code in src/pheval/utils/phenopacket_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def parse_hgnc_data () -> pl . DataFrame : \"\"\" Read HGNC data from a file and return it as a Polars DataFrame. Returns: pl.DataFrame: DataFrame containing the HGNC data. \"\"\" return ( pl . read_csv ( os . path . dirname ( __file__ ) . replace ( \"utils\" , \"resources/hgnc_complete_set.txt\" ), separator = \" \\t \" , infer_schema = 10000000000 , dtypes = { \"omim_id\" : pl . Utf8 }, ) . select ( [ pl . col ( \"hgnc_id\" ) . alias ( \"hgnc_id\" ), pl . col ( \"symbol\" ) . alias ( \"gene_symbol\" ), pl . col ( \"ensembl_gene_id\" ) . alias ( \"ensembl_id\" ), pl . col ( \"entrez_id\" ) . alias ( \"entrez_id\" ), pl . col ( \"refseq_accession\" ) . alias ( \"refseq_accession\" ), pl . col ( \"prev_symbol\" ) . alias ( \"previous_symbol_raw\" ), ] ) . with_columns ( pl . col ( \"previous_symbol_raw\" ) . str . split ( \"|\" ) . list . eval ( pl . element () . str . strip_chars ( '\"' )) . alias ( \"prev_symbols\" ) ) )","title":"parse_hgnc_data"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.phenopacket_reader","text":"Read a Phenopacket file and returns its contents as a Phenopacket or Family object Parameters: Name Type Description Default file Path Path to the Phenopacket file required Returns: Type Description Union [ Phenopacket , Family ] Union[Phenopacket, Family]: Contents of the Phenopacket file as a Phenopacket or Family object Source code in src/pheval/utils/phenopacket_utils.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def phenopacket_reader ( file : Path ) -> Union [ Phenopacket , Family ]: \"\"\" Read a Phenopacket file and returns its contents as a Phenopacket or Family object Args: file (Path): Path to the Phenopacket file Returns: Union[Phenopacket, Family]: Contents of the Phenopacket file as a Phenopacket or Family object \"\"\" logger . info ( f \"Parsing Phenopacket: { file . name } \" ) file = open ( file , \"r\" ) phenopacket = json . load ( file ) file . close () if \"proband\" in phenopacket : return Parse ( json . dumps ( phenopacket ), Family ()) else : return Parse ( json . dumps ( phenopacket ), Phenopacket ())","title":"phenopacket_reader"},{"location":"api/pheval/utils/phenopacket_utils/#src.pheval.utils.phenopacket_utils.write_phenopacket","text":"Write a Phenopacket or Family object to a file in JSON format. Parameters: Name Type Description Default phenopacket Phenopacket or Family The Phenopacket or Family object to be written. required output_file Path The Path object representing the file to write the Phenopacket data. required Returns: Type Description None None Source code in src/pheval/utils/phenopacket_utils.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def write_phenopacket ( phenopacket : Union [ Phenopacket , Family ], output_file : Path ) -> None : \"\"\" Write a Phenopacket or Family object to a file in JSON format. Args: phenopacket (Phenopacket or Family): The Phenopacket or Family object to be written. output_file (Path): The Path object representing the file to write the Phenopacket data. Returns: None \"\"\" logger . info ( f \"Writing Phenopacket to { output_file } .\" ) phenopacket_json = create_json_message ( phenopacket ) with open ( output_file , \"w\" ) as outfile : outfile . write ( phenopacket_json ) outfile . close ()","title":"write_phenopacket"},{"location":"api/pheval/utils/semsim_utils/","text":"Contains all pheval utility methods diff_semsim ( semsim_left , semsim_right , score_column , absolute_diff ) Calculates score difference between two semantic similarity profiles Parameters: Name Type Description Default semsim_left DataFrame first semantic similarity dataframe required semsim_right DataFrame second semantic similarity dataframe required score_column str Score column that will be computed (e.g. jaccard_similarity) required absolute_diff bool Whether the difference is absolute (True) or percentage (False). required Returns: Type Description DataFrame pd.DataFrame: A dataframe with terms and its scores differences Source code in src/pheval/utils/semsim_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def diff_semsim ( semsim_left : pd . DataFrame , semsim_right : pd . DataFrame , score_column : str , absolute_diff : bool ) -> pd . DataFrame : \"\"\"Calculates score difference between two semantic similarity profiles Args: semsim_left (pd.DataFrame): first semantic similarity dataframe semsim_right (pd.DataFrame): second semantic similarity dataframe score_column (str): Score column that will be computed (e.g. jaccard_similarity) absolute_diff (bool, optional): Whether the difference is absolute (True) or percentage (False). Defaults to True. Returns: pd.DataFrame: A dataframe with terms and its scores differences \"\"\" df = pd . merge ( semsim_left , semsim_right , on = [ \"subject_id\" , \"object_id\" ], how = \"outer\" ) if absolute_diff : df [ \"diff\" ] = df [ f \" { score_column } _x\" ] - df [ f \" { score_column } _y\" ] return df [[ \"subject_id\" , \"object_id\" , \"diff\" ]] df [ \"diff\" ] = df . apply ( lambda row : get_percentage_diff ( row [ f \" { score_column } _x\" ], row [ f \" { score_column } _y\" ]), axis = 1 ) return df [[ \"subject_id\" , \"object_id\" , f \" { score_column } _x\" , f \" { score_column } _y\" , \"diff\" ]] filter_non_0_score ( data , col ) Removes rows that have value equal to 0 based on the given column passed by col parameter Parameters: Name Type Description Default data DataFrame Dirty dataframe required col str Column to be filtered required Returns: Type Description DataFrame pd.DataFrame: Filtered dataframe Source code in src/pheval/utils/semsim_utils.py 14 15 16 17 18 19 20 21 22 23 24 def filter_non_0_score ( data : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\"Removes rows that have value equal to 0 based on the given column passed by col parameter Args: data (pd.DataFrame): Dirty dataframe col (str): Column to be filtered Returns: pd.DataFrame: Filtered dataframe \"\"\" return data [ data [ col ] != 0 ] get_percentage_diff ( current_number , previous_number ) Gets the percentage difference between two numbers Parameters: Name Type Description Default current_number float second number in comparison required previous_number float first number in comparison required Returns: Name Type Description float float percentage difference between two numbers Source code in src/pheval/utils/semsim_utils.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def get_percentage_diff ( current_number : float , previous_number : float ) -> float : \"\"\"Gets the percentage difference between two numbers Args: current_number (float): second number in comparison previous_number (float): first number in comparison Returns: float: percentage difference between two numbers \"\"\" try : if current_number == previous_number : return \" {:.2%} \" . format ( 0 ) if current_number > previous_number : number = ( 1 - (( current_number / previous_number ))) * 100 else : number = ( 100 - (( previous_number / current_number ) * 100 )) * - 1 return \" {:.2%} \" . format ( number / 100 ) except ZeroDivisionError : return None parse_semsim ( df , cols ) Parses semantic similarity profiles converting the score column as a numeric value and dropping the null ones Parameters: Name Type Description Default df DataFrame semantic similarity profile dataframe required cols list list of columns that will be selected on semsim data required Returns: Type Description DataFrame pd.Dataframe: parsed semantic similarity dataframe Source code in src/pheval/utils/semsim_utils.py 27 28 29 30 31 32 33 34 35 36 37 38 39 def parse_semsim ( df : pd . DataFrame , cols : list ) -> pd . DataFrame : \"\"\"Parses semantic similarity profiles converting the score column as a numeric value and dropping the null ones Args: df (pd.DataFrame): semantic similarity profile dataframe cols (list): list of columns that will be selected on semsim data Returns: pd.Dataframe: parsed semantic similarity dataframe \"\"\" df [ cols [ - 1 ]] = pd . to_numeric ( df [ cols [ - 1 ]], errors = \"coerce\" ) df . replace ( \"None\" , numpy . nan ) . dropna ( subset = cols [ - 1 ], inplace = True ) return df percentage_diff ( semsim_left , semsim_right , score_column , output ) Compares two semantic similarity profiles Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required output Path Output path for the difference tsv file required Source code in src/pheval/utils/semsim_utils.py 67 68 69 70 71 72 73 74 75 76 77 def percentage_diff ( semsim_left : Path , semsim_right : Path , score_column : str , output : Path ): \"\"\"Compares two semantic similarity profiles Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) output (Path): Output path for the difference tsv file \"\"\" clean_df = semsim_analysis ( semsim_left , semsim_right , score_column , absolute_diff = False ) clean_df . sort_values ( by = \"diff\" , ascending = False ) . to_csv ( output , sep = \" \\t \" , index = False ) semsim_analysis ( semsim_left , semsim_right , score_column , absolute_diff = True ) semsim_analysis Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required absolute_diff bool Whether the difference is absolute (True) or percentage (False). True Returns: Type Description DataFrame [pd.DataFrame]: DataFrame with the differences between two semantic similarity profiles Source code in src/pheval/utils/semsim_utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def semsim_analysis ( semsim_left : Path , semsim_right : Path , score_column : str , absolute_diff = True ) -> pd . DataFrame : \"\"\"semsim_analysis Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) absolute_diff (bool, optional): Whether the difference is absolute (True) or percentage (False). Defaults to True. Returns: [pd.DataFrame]: DataFrame with the differences between two semantic similarity profiles \"\"\" validate_semsim_file_comparison ( semsim_left , semsim_right ) cols = [ \"subject_id\" , \"object_id\" , score_column ] semsim_left = pd . read_csv ( semsim_left , sep = \" \\t \" ) semsim_right = pd . read_csv ( semsim_right , sep = \" \\t \" ) file_utils . ensure_columns_exists ( cols = cols , err_message = \"must exist in semsim dataframes\" , dataframes = [ semsim_left , semsim_right ], ) semsim_left = parse_semsim ( semsim_left , cols ) semsim_right = parse_semsim ( semsim_right , cols ) diff_df = diff_semsim ( semsim_left , semsim_right , score_column , absolute_diff ) return filter_non_0_score ( diff_df , \"diff\" ) semsim_heatmap_plot ( semsim_left , semsim_right , score_column ) Plots semantic similarity profiles heatmap Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required Source code in src/pheval/utils/semsim_utils.py 80 81 82 83 84 85 86 87 88 89 90 91 def semsim_heatmap_plot ( semsim_left : Path , semsim_right : Path , score_column : str ): \"\"\"Plots semantic similarity profiles heatmap Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) \"\"\" clean_df = semsim_analysis ( semsim_left , semsim_right , score_column ) df = clean_df . pivot ( index = \"subject_id\" , columns = \"object_id\" , values = \"diff\" ) fig = px . imshow ( df , text_auto = True ) fig . show () validate_semsim_file_comparison ( semsim_left , semsim_right ) Checks if files exist and whether they're different Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile Raises: Exception: FileNotFoundException Source code in src/pheval/utils/semsim_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 def validate_semsim_file_comparison ( semsim_left : Path , semsim_right : Path ): \"\"\"Checks if files exist and whether they're different Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile Raises: Exception: FileNotFoundException \"\"\" if semsim_left == semsim_right : errmsg = \"Semantic similarity profiles are equal. Make sure you have selected different files to analyze\" raise Exception ( errmsg ) file_utils . ensure_file_exists ( semsim_left , semsim_right )","title":"Semsim utils"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.diff_semsim","text":"Calculates score difference between two semantic similarity profiles Parameters: Name Type Description Default semsim_left DataFrame first semantic similarity dataframe required semsim_right DataFrame second semantic similarity dataframe required score_column str Score column that will be computed (e.g. jaccard_similarity) required absolute_diff bool Whether the difference is absolute (True) or percentage (False). required Returns: Type Description DataFrame pd.DataFrame: A dataframe with terms and its scores differences Source code in src/pheval/utils/semsim_utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def diff_semsim ( semsim_left : pd . DataFrame , semsim_right : pd . DataFrame , score_column : str , absolute_diff : bool ) -> pd . DataFrame : \"\"\"Calculates score difference between two semantic similarity profiles Args: semsim_left (pd.DataFrame): first semantic similarity dataframe semsim_right (pd.DataFrame): second semantic similarity dataframe score_column (str): Score column that will be computed (e.g. jaccard_similarity) absolute_diff (bool, optional): Whether the difference is absolute (True) or percentage (False). Defaults to True. Returns: pd.DataFrame: A dataframe with terms and its scores differences \"\"\" df = pd . merge ( semsim_left , semsim_right , on = [ \"subject_id\" , \"object_id\" ], how = \"outer\" ) if absolute_diff : df [ \"diff\" ] = df [ f \" { score_column } _x\" ] - df [ f \" { score_column } _y\" ] return df [[ \"subject_id\" , \"object_id\" , \"diff\" ]] df [ \"diff\" ] = df . apply ( lambda row : get_percentage_diff ( row [ f \" { score_column } _x\" ], row [ f \" { score_column } _y\" ]), axis = 1 ) return df [[ \"subject_id\" , \"object_id\" , f \" { score_column } _x\" , f \" { score_column } _y\" , \"diff\" ]]","title":"diff_semsim"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.filter_non_0_score","text":"Removes rows that have value equal to 0 based on the given column passed by col parameter Parameters: Name Type Description Default data DataFrame Dirty dataframe required col str Column to be filtered required Returns: Type Description DataFrame pd.DataFrame: Filtered dataframe Source code in src/pheval/utils/semsim_utils.py 14 15 16 17 18 19 20 21 22 23 24 def filter_non_0_score ( data : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\"Removes rows that have value equal to 0 based on the given column passed by col parameter Args: data (pd.DataFrame): Dirty dataframe col (str): Column to be filtered Returns: pd.DataFrame: Filtered dataframe \"\"\" return data [ data [ col ] != 0 ]","title":"filter_non_0_score"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.get_percentage_diff","text":"Gets the percentage difference between two numbers Parameters: Name Type Description Default current_number float second number in comparison required previous_number float first number in comparison required Returns: Name Type Description float float percentage difference between two numbers Source code in src/pheval/utils/semsim_utils.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def get_percentage_diff ( current_number : float , previous_number : float ) -> float : \"\"\"Gets the percentage difference between two numbers Args: current_number (float): second number in comparison previous_number (float): first number in comparison Returns: float: percentage difference between two numbers \"\"\" try : if current_number == previous_number : return \" {:.2%} \" . format ( 0 ) if current_number > previous_number : number = ( 1 - (( current_number / previous_number ))) * 100 else : number = ( 100 - (( previous_number / current_number ) * 100 )) * - 1 return \" {:.2%} \" . format ( number / 100 ) except ZeroDivisionError : return None","title":"get_percentage_diff"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.parse_semsim","text":"Parses semantic similarity profiles converting the score column as a numeric value and dropping the null ones Parameters: Name Type Description Default df DataFrame semantic similarity profile dataframe required cols list list of columns that will be selected on semsim data required Returns: Type Description DataFrame pd.Dataframe: parsed semantic similarity dataframe Source code in src/pheval/utils/semsim_utils.py 27 28 29 30 31 32 33 34 35 36 37 38 39 def parse_semsim ( df : pd . DataFrame , cols : list ) -> pd . DataFrame : \"\"\"Parses semantic similarity profiles converting the score column as a numeric value and dropping the null ones Args: df (pd.DataFrame): semantic similarity profile dataframe cols (list): list of columns that will be selected on semsim data Returns: pd.Dataframe: parsed semantic similarity dataframe \"\"\" df [ cols [ - 1 ]] = pd . to_numeric ( df [ cols [ - 1 ]], errors = \"coerce\" ) df . replace ( \"None\" , numpy . nan ) . dropna ( subset = cols [ - 1 ], inplace = True ) return df","title":"parse_semsim"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.percentage_diff","text":"Compares two semantic similarity profiles Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required output Path Output path for the difference tsv file required Source code in src/pheval/utils/semsim_utils.py 67 68 69 70 71 72 73 74 75 76 77 def percentage_diff ( semsim_left : Path , semsim_right : Path , score_column : str , output : Path ): \"\"\"Compares two semantic similarity profiles Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) output (Path): Output path for the difference tsv file \"\"\" clean_df = semsim_analysis ( semsim_left , semsim_right , score_column , absolute_diff = False ) clean_df . sort_values ( by = \"diff\" , ascending = False ) . to_csv ( output , sep = \" \\t \" , index = False )","title":"percentage_diff"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.semsim_analysis","text":"semsim_analysis Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required absolute_diff bool Whether the difference is absolute (True) or percentage (False). True Returns: Type Description DataFrame [pd.DataFrame]: DataFrame with the differences between two semantic similarity profiles Source code in src/pheval/utils/semsim_utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def semsim_analysis ( semsim_left : Path , semsim_right : Path , score_column : str , absolute_diff = True ) -> pd . DataFrame : \"\"\"semsim_analysis Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) absolute_diff (bool, optional): Whether the difference is absolute (True) or percentage (False). Defaults to True. Returns: [pd.DataFrame]: DataFrame with the differences between two semantic similarity profiles \"\"\" validate_semsim_file_comparison ( semsim_left , semsim_right ) cols = [ \"subject_id\" , \"object_id\" , score_column ] semsim_left = pd . read_csv ( semsim_left , sep = \" \\t \" ) semsim_right = pd . read_csv ( semsim_right , sep = \" \\t \" ) file_utils . ensure_columns_exists ( cols = cols , err_message = \"must exist in semsim dataframes\" , dataframes = [ semsim_left , semsim_right ], ) semsim_left = parse_semsim ( semsim_left , cols ) semsim_right = parse_semsim ( semsim_right , cols ) diff_df = diff_semsim ( semsim_left , semsim_right , score_column , absolute_diff ) return filter_non_0_score ( diff_df , \"diff\" )","title":"semsim_analysis"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.semsim_heatmap_plot","text":"Plots semantic similarity profiles heatmap Parameters: Name Type Description Default semsim_left Path File path of the first semantic similarity profile required semsim_right Path File path of the second semantic similarity profile required score_column str Score column that will be computed (e.g. jaccard_similarity) required Source code in src/pheval/utils/semsim_utils.py 80 81 82 83 84 85 86 87 88 89 90 91 def semsim_heatmap_plot ( semsim_left : Path , semsim_right : Path , score_column : str ): \"\"\"Plots semantic similarity profiles heatmap Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile score_column (str): Score column that will be computed (e.g. jaccard_similarity) \"\"\" clean_df = semsim_analysis ( semsim_left , semsim_right , score_column ) df = clean_df . pivot ( index = \"subject_id\" , columns = \"object_id\" , values = \"diff\" ) fig = px . imshow ( df , text_auto = True ) fig . show ()","title":"semsim_heatmap_plot"},{"location":"api/pheval/utils/semsim_utils/#src.pheval.utils.semsim_utils.validate_semsim_file_comparison","text":"Checks if files exist and whether they're different Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile Raises: Exception: FileNotFoundException Source code in src/pheval/utils/semsim_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 def validate_semsim_file_comparison ( semsim_left : Path , semsim_right : Path ): \"\"\"Checks if files exist and whether they're different Args: semsim_left (Path): File path of the first semantic similarity profile semsim_right (Path): File path of the second semantic similarity profile Raises: Exception: FileNotFoundException \"\"\" if semsim_left == semsim_right : errmsg = \"Semantic similarity profiles are equal. Make sure you have selected different files to analyze\" raise Exception ( errmsg ) file_utils . ensure_file_exists ( semsim_left , semsim_right )","title":"validate_semsim_file_comparison"},{"location":"api/pheval/utils/utils/","text":"Contains all pheval utility methods download_hgnc_data () Download latest HGNC complete set file. Source code in src/pheval/utils/utils.py 127 128 129 def download_hgnc_data () -> None : \"\"\"Download latest HGNC complete set file.\"\"\" _download_file ( HGNC_URL , RESOURCES_DIR / \"hgnc_complete_set.txt\" ) download_mondo_mapping () Download latest MONDO SSSOM mapping file. Source code in src/pheval/utils/utils.py 122 123 124 def download_mondo_mapping () -> None : \"\"\"Download latest MONDO SSSOM mapping file.\"\"\" _download_file ( MONDO_URL , RESOURCES_DIR / \"mondo.sssom.tsv\" ) get_resource_timestamp ( file_name ) Return the ISO timestamp when the resource file was last updated. Args: file_name (str): The file name. Source code in src/pheval/utils/utils.py 132 133 134 135 136 137 138 139 140 141 def get_resource_timestamp ( file_name : str ) -> str | None : \"\"\" Return the ISO timestamp when the resource file was last updated. Args: file_name (str): The file name. \"\"\" if METADATA_PATH . exists (): with open ( METADATA_PATH , \"r\" ) as f : return json . load ( f ) . get ( file_name ) return None rand ( df , min_num , max_num , scramble_factor ) Numeric scrambling Args: df (pd.DataFrame): dataframe records min_num (int): min value from this records max_num (int): max value from this records scramble_factor (float): scramble factor scalar Returns: float: randomized number Source code in src/pheval/utils/utils.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def rand ( df : pd . DataFrame , min_num : int , max_num : int , scramble_factor : float ) -> float : \"\"\" Numeric scrambling Args: df (pd.DataFrame): dataframe records min_num (int): min value from this records max_num (int): max value from this records scramble_factor (float): scramble factor scalar Returns: float: randomized number \"\"\" try : return df + ( random . uniform ( min_num , max_num ) * scramble_factor ) except TypeError as err : logger . error ( df , exc_info = err ) return df semsim_scramble ( input , output , columns_to_be_scrambled , scramble_factor = 0.5 ) Scrambles semantic similarity profile with a magnitude between 0 and 1 (scramble_factor: 0 means no scrambling and 1 means complete randomisation). It then randomises the above scores with a degree of the scramble_factor and returns a scrambles pandas dataframe. Args: input (Path): scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): columns that will be scrambled in semsim file (e.g. jaccard_similarity). output (Path) Returns: pd.Dataframe: scrambled dataframe Source code in src/pheval/utils/utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def semsim_scramble ( input : Path , output : Path , columns_to_be_scrambled : List [ str ], scramble_factor : float = 0.5 , ) -> pd . DataFrame : \"\"\" Scrambles semantic similarity profile with a magnitude between 0 and 1 (scramble_factor: 0 means no scrambling and 1 means complete randomisation). It then randomises the above scores with a degree of the scramble_factor and returns a scrambles pandas dataframe. Args: input (Path): scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): columns that will be scrambled in semsim file (e.g. jaccard_similarity). output (Path) Returns: pd.Dataframe: scrambled dataframe \"\"\" semsim = pd . read_csv ( input , sep = \" \\t \" ) dataframe = semsim_scramble_df ( semsim , columns_to_be_scrambled , scramble_factor ) dataframe . to_csv ( output , sep = \" \\t \" , index = False ) semsim_scramble_df ( dataframe , columns_to_be_scrambled , scramble_factor ) scramble_semsim_df Args: dataframe (pd.DataFrame): dataframe that contains semsim profile scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): Returns: pd.Dataframe: scrambled dataframe Source code in src/pheval/utils/utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def semsim_scramble_df ( dataframe : pd . DataFrame , columns_to_be_scrambled : List [ str ], scramble_factor : float , ) -> pd . DataFrame : \"\"\"scramble_semsim_df Args: dataframe (pd.DataFrame): dataframe that contains semsim profile scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): Returns: pd.Dataframe: scrambled dataframe \"\"\" for col in columns_to_be_scrambled : min_num = dataframe [ col ] . min () max_num = dataframe [ col ] . max () dataframe [ col ] = dataframe [ col ] . apply ( rand , args = ( min_num , max_num , scramble_factor )) return dataframe","title":"Utils"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.download_hgnc_data","text":"Download latest HGNC complete set file. Source code in src/pheval/utils/utils.py 127 128 129 def download_hgnc_data () -> None : \"\"\"Download latest HGNC complete set file.\"\"\" _download_file ( HGNC_URL , RESOURCES_DIR / \"hgnc_complete_set.txt\" )","title":"download_hgnc_data"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.download_mondo_mapping","text":"Download latest MONDO SSSOM mapping file. Source code in src/pheval/utils/utils.py 122 123 124 def download_mondo_mapping () -> None : \"\"\"Download latest MONDO SSSOM mapping file.\"\"\" _download_file ( MONDO_URL , RESOURCES_DIR / \"mondo.sssom.tsv\" )","title":"download_mondo_mapping"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.get_resource_timestamp","text":"Return the ISO timestamp when the resource file was last updated. Args: file_name (str): The file name. Source code in src/pheval/utils/utils.py 132 133 134 135 136 137 138 139 140 141 def get_resource_timestamp ( file_name : str ) -> str | None : \"\"\" Return the ISO timestamp when the resource file was last updated. Args: file_name (str): The file name. \"\"\" if METADATA_PATH . exists (): with open ( METADATA_PATH , \"r\" ) as f : return json . load ( f ) . get ( file_name ) return None","title":"get_resource_timestamp"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.rand","text":"Numeric scrambling Args: df (pd.DataFrame): dataframe records min_num (int): min value from this records max_num (int): max value from this records scramble_factor (float): scramble factor scalar Returns: float: randomized number Source code in src/pheval/utils/utils.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def rand ( df : pd . DataFrame , min_num : int , max_num : int , scramble_factor : float ) -> float : \"\"\" Numeric scrambling Args: df (pd.DataFrame): dataframe records min_num (int): min value from this records max_num (int): max value from this records scramble_factor (float): scramble factor scalar Returns: float: randomized number \"\"\" try : return df + ( random . uniform ( min_num , max_num ) * scramble_factor ) except TypeError as err : logger . error ( df , exc_info = err ) return df","title":"rand"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.semsim_scramble","text":"Scrambles semantic similarity profile with a magnitude between 0 and 1 (scramble_factor: 0 means no scrambling and 1 means complete randomisation). It then randomises the above scores with a degree of the scramble_factor and returns a scrambles pandas dataframe. Args: input (Path): scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): columns that will be scrambled in semsim file (e.g. jaccard_similarity). output (Path) Returns: pd.Dataframe: scrambled dataframe Source code in src/pheval/utils/utils.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def semsim_scramble ( input : Path , output : Path , columns_to_be_scrambled : List [ str ], scramble_factor : float = 0.5 , ) -> pd . DataFrame : \"\"\" Scrambles semantic similarity profile with a magnitude between 0 and 1 (scramble_factor: 0 means no scrambling and 1 means complete randomisation). It then randomises the above scores with a degree of the scramble_factor and returns a scrambles pandas dataframe. Args: input (Path): scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): columns that will be scrambled in semsim file (e.g. jaccard_similarity). output (Path) Returns: pd.Dataframe: scrambled dataframe \"\"\" semsim = pd . read_csv ( input , sep = \" \\t \" ) dataframe = semsim_scramble_df ( semsim , columns_to_be_scrambled , scramble_factor ) dataframe . to_csv ( output , sep = \" \\t \" , index = False )","title":"semsim_scramble"},{"location":"api/pheval/utils/utils/#src.pheval.utils.utils.semsim_scramble_df","text":"scramble_semsim_df Args: dataframe (pd.DataFrame): dataframe that contains semsim profile scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): Returns: pd.Dataframe: scrambled dataframe Source code in src/pheval/utils/utils.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def semsim_scramble_df ( dataframe : pd . DataFrame , columns_to_be_scrambled : List [ str ], scramble_factor : float , ) -> pd . DataFrame : \"\"\"scramble_semsim_df Args: dataframe (pd.DataFrame): dataframe that contains semsim profile scramble_factor (float) scalar scramble factor columns_to_be_scrambled (List[str]): Returns: pd.Dataframe: scrambled dataframe \"\"\" for col in columns_to_be_scrambled : min_num = dataframe [ col ] . min () max_num = dataframe [ col ] . max () dataframe [ col ] = dataframe [ col ] . apply ( rand , args = ( min_num , max_num , scramble_factor )) return dataframe","title":"semsim_scramble_df"}]}